<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Coder On The Road"><title>统计学习方法——决策树 | 流光</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.1.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.2/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/2.2.3/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">统计学习方法——决策树</h1><a id="logo" href="/.">流光</a><p class="description">他跑啊跑啊，只为追上那个曾经被寄予厚望的自己</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/guestbook/"><i class="fa fa-comments"> 留言</i></a><a href="/resource/share"><i class="fa fa-download"> 资源</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-4-5"><div class="content_container"><div class="post"><h1 class="post-title">统计学习方法——决策树</h1><div class="post-meta">Sep 23, 2016<span> | </span><span class="category"><a href="/categories/MachineLearning/">MachineLearning</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/09/23/2016-09-23-statical-learning-5.html" href="/2016/09/23/2016-09-23-statical-learning-5.html#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#决策树模型与学习"><span class="toc-number">1.</span> <span class="toc-text">决策树模型与学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树模型"><span class="toc-number">1.1.</span> <span class="toc-text">决策树模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树与if-then规则"><span class="toc-number">1.2.</span> <span class="toc-text">决策树与if-then规则</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树与条件概率分布"><span class="toc-number">1.3.</span> <span class="toc-text">决策树与条件概率分布</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#决策树学习"><span class="toc-number">1.4.</span> <span class="toc-text">决策树学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#特征选择"><span class="toc-number">2.</span> <span class="toc-text">特征选择</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#特征选择-1"><span class="toc-number">2.1.</span> <span class="toc-text">特征选择</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#信息增益"><span class="toc-number">2.2.</span> <span class="toc-text">信息增益</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#信息增益的算法"><span class="toc-number">2.3.</span> <span class="toc-text">信息增益的算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#信息增益比"><span class="toc-number">2.4.</span> <span class="toc-text">信息增益比</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#决策树的生成"><span class="toc-number">3.</span> <span class="toc-text">决策树的生成</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#ID3算法"><span class="toc-number">3.1.</span> <span class="toc-text">ID3算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#算法描述"><span class="toc-number">3.1.1.</span> <span class="toc-text">算法描述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Python实现"><span class="toc-number">3.1.2.</span> <span class="toc-text">Python实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#可视化"><span class="toc-number">3.1.3.</span> <span class="toc-text">可视化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#C4-5生成算法"><span class="toc-number">3.2.</span> <span class="toc-text">C4.5生成算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#算法描述-1"><span class="toc-number">3.2.1.</span> <span class="toc-text">算法描述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#python实现"><span class="toc-number">3.2.2.</span> <span class="toc-text">python实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#决策树的剪枝"><span class="toc-number">4.</span> <span class="toc-text">决策树的剪枝</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#树的剪枝算法"><span class="toc-number">4.1.</span> <span class="toc-text">树的剪枝算法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CART算法"><span class="toc-number">5.</span> <span class="toc-text">CART算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#CART生成"><span class="toc-number">5.1.</span> <span class="toc-text">CART生成</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#回归树"><span class="toc-number">5.1.1.</span> <span class="toc-text">回归树</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最小二乘回归树生成算法"><span class="toc-number">5.1.2.</span> <span class="toc-text">最小二乘回归树生成算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#分类树"><span class="toc-number">5.1.3.</span> <span class="toc-text">分类树</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CART剪枝"><span class="toc-number">5.2.</span> <span class="toc-text">CART剪枝</span></a></li></ol></li></ol></div></div><div class="post-content"><blockquote>
<p><strong>本节内容：</strong>决策树(decision tree)是一种基本的分类与回归方法。决策树模型呈树形结构，其主要优点是模型具有可读性，分类速度快，学习时，利用训练数据根据损失函数最小化的原则建立决策树模型，分类速度快。决策树的学习通常包括3个步骤：特征选择、决策树的生成和决策树的修建。本节对应于统计学习方法第五章的内容，主要学习ID3、C4.5和CART算法。</p>
</blockquote>
<a id="more"></a>
<h1 id="决策树模型与学习"><a href="#决策树模型与学习" class="headerlink" title="决策树模型与学习"></a>决策树模型与学习</h1><h2 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h2><p>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类。</p>
<p>分类的时候，从根节点开始，当前节点设为根节点，当前节点必定是一种特征，根据实例的该特征的取值，向下移动，直到到达叶节点，将实例分到叶节点对应的类中。</p>
<h2 id="决策树与if-then规则"><a href="#决策树与if-then规则" class="headerlink" title="决策树与if-then规则"></a>决策树与if-then规则</h2><p>决策树的属性结构其实对应着一个规则集合：由决策树的根节点到叶节点的每条路径构成的规则组成；路径上的内部特征对应着if条件，叶节点对应着then结论。决策树和规则集合是等效的，都具有一个重要的性质：互斥且完备。也就是说任何实例都被且仅被一条路径或规则覆盖。</p>
<h2 id="决策树与条件概率分布"><a href="#决策树与条件概率分布" class="headerlink" title="决策树与条件概率分布"></a>决策树与条件概率分布</h2><p>决策树还是给定特征条件下类的条件概率分布的一种退化表示（非等效，个人理解）。该条件分布定义在特征空间的划分上，特征空间被花费为互不相交的单元，每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的每条路径对应于划分中的一个单元。给定实例的特征X，一定落入某个划分，决策树选取该划分里最大概率的类作为结果输出。如图：</p>
<p><img src="/resource/blog/2016-08/决策树与条件概率密度.jpg" alt="决策树与条件概率密度"></p>
<p>关于b图，我是这么理解的，将a图的基础上增加一个条件概率的维度P，代表在当前特征X的情况下，分类为+的后验概率。图中的方块有些地方完全没有，比如x2轴上[a2,1]这个区间，说明只要X落在这里，Y就一定是-的，同理对于[0,a1]和[0,a2]围起来的一定是+的。有些地方只有一半，比如x1轴上[a1,1]这个区间，说明决策树认为X落在这里，Y只有一半概率是+的，根据选择条件概率大的类别的原则，就认为Y是-的（因为不满足P(+)&gt;0.5)。</p>
<h2 id="决策树学习"><a href="#决策树学习" class="headerlink" title="决策树学习"></a>决策树学习</h2><p>决策树学习算法包含特征选择、决策树的生成与剪枝过程。决策树的学习算法一般是递归地选择最优特征，并用最优特征对数据集进行分割。开始时，构建根节点，选择最优特征，该特征有几种值就分割为几个子集，每个子集分别递归调用此方法，返回节点，返回的节点就是上一层的子节点。直到数据集为空，或者数据集只有一维特征为止。</p>
<p>基本骨架的Python实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    返回出现次数最多的分类名称</span><br><span class="line">    :param classList: 类列表</span><br><span class="line">    :return: 出现次数最多的类名称</span><br><span class="line">    """</span></span><br><span class="line">    classCount = &#123;&#125;  <span class="comment"># 这是一个字典</span></span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys(): classCount[vote] = <span class="number">0</span></span><br><span class="line">        classCount[vote] += <span class="number">1</span></span><br><span class="line">    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataSet, labels, chooseBestFeatureToSplitFunc=chooseBestFeatureToSplitByID3)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    创建决策树</span><br><span class="line">    :param dataSet:数据集</span><br><span class="line">    :param labels:数据集每一维的名称</span><br><span class="line">    :return:决策树</span><br><span class="line">    """</span></span><br><span class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  <span class="comment"># 类别列表</span></span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]  <span class="comment"># 当类别完全相同则停止继续划分</span></span><br><span class="line">    <span class="keyword">if</span> len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>:  <span class="comment"># 当只有一个特征的时候，遍历完所有实例返回出现次数最多的类别</span></span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    bestFeat = chooseBestFeatureToSplitFunc(dataSet)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    myTree = &#123;bestFeatLabel: &#123;&#125;&#125;</span><br><span class="line">    <span class="keyword">del</span> (labels[bestFeat])</span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]</span><br><span class="line">    uniqueVals = set(featValues)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subLabels = labels[:]  <span class="comment"># 复制操作</span></span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></figure>
<p>由于决策树表示条件概率分布，所以高度不同的决策树对应不同复杂度的概率模型。最优决策树的生成是个NP问题，能实现的生成算法都是局部最优的，剪枝则是既定决策树下的全局最优。</p>
<h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><h2 id="特征选择-1"><a href="#特征选择-1" class="headerlink" title="特征选择"></a>特征选择</h2><p>样本通常有很多维特征，希望选择具有分类能力的特征。比如下表：</p>
<p><img src="/resource/blog/2016-08/贷款申请.jpg" alt="贷款申请"></p>
<p>可以用Python建立数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    创建数据集</span><br><span class="line">    :return:数据集和每个维度的名称</span><br><span class="line">    """</span></span><br><span class="line">    dataSet = [[<span class="string">u'青年'</span>, <span class="string">u'否'</span>, <span class="string">u'否'</span>, <span class="string">u'一般'</span>, <span class="string">u'拒绝'</span>],</span><br><span class="line">               [<span class="string">u'青年'</span>, <span class="string">u'否'</span>, <span class="string">u'否'</span>, <span class="string">u'好'</span>, <span class="string">u'拒绝'</span>],</span><br><span class="line">               [<span class="string">u'青年'</span>, <span class="string">u'是'</span>, <span class="string">u'否'</span>, <span class="string">u'好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'青年'</span>, <span class="string">u'是'</span>, <span class="string">u'是'</span>, <span class="string">u'一般'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'青年'</span>, <span class="string">u'否'</span>, <span class="string">u'否'</span>, <span class="string">u'一般'</span>, <span class="string">u'拒绝'</span>],</span><br><span class="line">               [<span class="string">u'中年'</span>, <span class="string">u'否'</span>, <span class="string">u'否'</span>, <span class="string">u'一般'</span>, <span class="string">u'拒绝'</span>],</span><br><span class="line">               [<span class="string">u'中年'</span>, <span class="string">u'否'</span>, <span class="string">u'否'</span>, <span class="string">u'好'</span>, <span class="string">u'拒绝'</span>],</span><br><span class="line">               [<span class="string">u'中年'</span>, <span class="string">u'是'</span>, <span class="string">u'是'</span>, <span class="string">u'好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'中年'</span>, <span class="string">u'否'</span>, <span class="string">u'是'</span>, <span class="string">u'非常好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'中年'</span>, <span class="string">u'否'</span>, <span class="string">u'是'</span>, <span class="string">u'非常好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'老年'</span>, <span class="string">u'否'</span>, <span class="string">u'是'</span>, <span class="string">u'非常好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'老年'</span>, <span class="string">u'否'</span>, <span class="string">u'是'</span>, <span class="string">u'好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'老年'</span>, <span class="string">u'是'</span>, <span class="string">u'否'</span>, <span class="string">u'好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'老年'</span>, <span class="string">u'是'</span>, <span class="string">u'否'</span>, <span class="string">u'非常好'</span>, <span class="string">u'同意'</span>],</span><br><span class="line">               [<span class="string">u'老年'</span>, <span class="string">u'否'</span>, <span class="string">u'否'</span>, <span class="string">u'一般'</span>, <span class="string">u'拒绝'</span>],</span><br><span class="line">               ]</span><br><span class="line">    labels = [<span class="string">u'年龄'</span>, <span class="string">u'有工作'</span>, <span class="string">u'有房子'</span>, <span class="string">u'信贷情况'</span>]</span><br><span class="line">    <span class="comment"># 返回数据集和每个维度的名称</span></span><br><span class="line">    <span class="keyword">return</span> dataSet, labels</span><br></pre></td></tr></table></figure>
<p>也可以根据特征分割数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataSet, axis, value)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    按照给定特征划分数据集</span><br><span class="line">    :param dataSet: 待划分的数据集</span><br><span class="line">    :param axis: 划分数据集的特征的维度</span><br><span class="line">    :param value: 特征的值</span><br><span class="line">    :return: 符合该特征的所有实例（并且自动移除掉这维特征）</span><br><span class="line">    """</span></span><br><span class="line">    retDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:</span><br><span class="line">            reducedFeatVec = featVec[:axis]  <span class="comment"># 删掉这一维特征</span></span><br><span class="line">            reducedFeatVec.extend(featVec[axis + <span class="number">1</span>:])</span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> retDataSet</span><br></pre></td></tr></table></figure>
<h2 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h2><p>对于一个可能有n种取值的随机变量：$P(X=x_i)=p_i$,其熵为：$H(X)=-\sum_{i=1}^np_i\log p_i$ ,另外定义0log0=0,当对数的底为2时，熵的单位是bit，为自然对数时，单位是nat。</p>
<p>用Python实现信息熵（香农熵）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    计算训练数据集中的Y随机变量的香农熵</span><br><span class="line">    :param dataSet:</span><br><span class="line">    :return:</span><br><span class="line">    """</span></span><br><span class="line">    numEntries = len(dataSet)  <span class="comment"># 实例的个数</span></span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:  <span class="comment"># 遍历每个实例，统计标签的频次</span></span><br><span class="line">        currentLabel = featVec[<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys(): labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:</span><br><span class="line">        prob = float(labelCounts[key]) / numEntries</span><br><span class="line">        shannonEnt -= prob * log(prob, <span class="number">2</span>)  <span class="comment"># log base 2</span></span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></figure>
<p>由定义知，X的熵与X的值无关，只与分布有关，所以也可以将X的熵记作H(p),即：</p>
<p>$$H(p)=-\sum_{i=1}^np_i\log p_i$$</p>
<p>熵其实就是X的不确定性，从定义可以验证$0 \leq H(p) \leq \log n$</p>
<p>设随机变量(X,Y)，其联合分布为：</p>
<p>$$P(X=x_i,Y=y_i)=p_{ij},i=1,2,\cdots,n;j=1,2,\cdots,m$$</p>
<p>条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性，定义为在X给定的条件下，Y的概率分布对X的数学期望：</p>
<p>$$H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i),p_i=P(X=x_i),i=1,2,\cdots,n$$</p>
<p>当上述定义式中的概率由数据估计（比如上一章提到的极大似然估计）得到时，所对应的熵和条件熵分别称为经验熵和经验条件熵。</p>
<p>Python实现条件熵的计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcConditionalEntropy</span><span class="params">(dataSet, i, featList, uniqueVals)</span>:</span></span><br><span class="line">    <span class="string">'''</span><br><span class="line">    计算X_i给定的条件下，Y的条件熵</span><br><span class="line">    :param dataSet:数据集</span><br><span class="line">    :param i:维度i</span><br><span class="line">    :param featList: 数据集特征列表</span><br><span class="line">    :param uniqueVals: 数据集特征集合</span><br><span class="line">    :return:条件熵</span><br><span class="line">    '''</span></span><br><span class="line">    ce = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">        prob = len(subDataSet) / float(len(dataSet))  <span class="comment"># 极大似然估计概率</span></span><br><span class="line">        ce += prob * calcShannonEnt(subDataSet)  <span class="comment"># ∑pH(Y|X=xi) 条件熵的计算</span></span><br><span class="line">    <span class="keyword">return</span> ce</span><br></pre></td></tr></table></figure>
<p>有了上述知识，就可以一句话说明什么叫信息增益了：信息增益表示得知特征X的信息而使类Y的信息的熵减少的程度。形式化的定义如下：</p>
<blockquote>
<p>特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即$g(D|A)=H(D)-H(D|A)$,这个差又称为互信息，决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p>
</blockquote>
<p>用Python计算信息增益：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcInformationGain</span><span class="params">(dataSet, baseEntropy, i)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    计算信息增益</span><br><span class="line">    :param dataSet:数据集</span><br><span class="line">    :param baseEntropy:数据集中Y的信息熵</span><br><span class="line">    :param i: 特征维度i</span><br><span class="line">    :return: 特征i对数据集的信息增益g(dataSet|X_i)</span><br><span class="line">    """</span></span><br><span class="line">    featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  <span class="comment"># 第i维特征列表</span></span><br><span class="line">    uniqueVals = set(featList)  <span class="comment"># 转换成集合</span></span><br><span class="line">    newEntropy = calcConditionalEntropy(dataSet, i, featList, uniqueVals)</span><br><span class="line">    infoGain = baseEntropy - newEntropy  <span class="comment"># 信息增益，就是熵的减少，也就是不确定性的减少</span></span><br><span class="line">    <span class="keyword">return</span> infoGain</span><br></pre></td></tr></table></figure>
<p>回到最初的问题，如何判断一个特征的分类能力呢？信息增益大的特征具有更强的分类能力。只要计算出各个特征的信息增益，找出最大的那一个就行。</p>
<h2 id="信息增益的算法"><a href="#信息增益的算法" class="headerlink" title="信息增益的算法"></a>信息增益的算法</h2><p>输入：训练数据集D和特征A；</p>
<p>输出：特征A对训练数据集D的信息增益g(D,A);</p>
<p>(1) 计算数据集D的经验熵H(D)</p>
<p>$$H(D)=-\sum_{k=1}^K\frac{\vert C_k \vert}{\vert D \vert}\log_2\frac{\vert C_k \vert}{\vert D \vert}$$</p>
<p>(2) 计算特征A对数据集D的经验条件熵H(D|A)</p>
<p>$$H(D \vert A)=\sum_{i=1}^n \frac{\vert D_i \vert}{\vert D \vert} H(D_i) = -\sum_{i=1}^n \frac{\vert D_i \vert}{\vert D \vert} \sum_{k=1}^K \frac{\vert D_{ik} \vert}{\vert D_i \vert} \log_2 \frac{\vert D_{ik} \vert}{\vert D_i \vert}$$</p>
<p>(3) 计算信息增益</p>
<p>$$g(D,A)=H(D)-H(D|A)$$</p>
<h2 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h2><p>信息增益算法有个缺点，信息增益的值是相对于训练数据集而言的，当H(D)大的时候，信息增益值往往会偏大，这样对H(D)小的特征不公平。改进的方法是信息增益比：</p>
<blockquote>
<p>特征增益比：特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益g(D,A)与训练数据集D的经验熵H(D)之比：$$g_R(D,A)=\frac{g(D,A)}{H(D)}$$</p>
</blockquote>
<p>Python代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcInformationGainRate</span><span class="params">(dataSet, baseEntropy, i)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    计算信息增益比</span><br><span class="line">    :param dataSet:数据集</span><br><span class="line">    :param baseEntropy:数据集中Y的信息熵</span><br><span class="line">    :param i: 特征维度i</span><br><span class="line">    :return: 特征i对数据集的信息增益g(dataSet|X_i)</span><br><span class="line">    """</span></span><br><span class="line">    <span class="keyword">return</span> calcInformationGain(dataSet, baseEntropy, i) / baseEntropy</span><br></pre></td></tr></table></figure>
<h1 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h1><h2 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h2><h3 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h3><p>从根节点开始，计算所有可能的特征的信息增益，选择信息增益最大的特征作为当前节点的特征，由特征的不同取值建立空白子节点，对空白子节点递归调用此方法，直到所有特征的信息增益小于阀值或者没有特征可选为止。</p>
<h3 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h3><p>ID3特征选择算法的Python实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplitByID3</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    选择最好的数据集划分方式</span><br><span class="line">    :param dataSet:</span><br><span class="line">    :return:</span><br><span class="line">    """</span></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span>  <span class="comment"># 最后一列是分类</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    bestInfoGain = <span class="number">0.0</span></span><br><span class="line">    bestFeature = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):  <span class="comment"># 遍历所有维度特征</span></span><br><span class="line">        infoGain = calcInformationGain(dataSet, baseEntropy, i)</span><br><span class="line">        <span class="keyword">if</span> (infoGain &gt; bestInfoGain):  <span class="comment"># 选择最大的信息增益</span></span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature  <span class="comment"># 返回最佳特征对应的维度</span></span><br></pre></td></tr></table></figure>
<p>完整调用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># Filename: testTree.py</span></span><br><span class="line"><span class="comment"># Author：hankcs</span></span><br><span class="line"><span class="comment"># Date: 2014-04-19 下午9:19</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###########中文支持################</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> tree <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">mpl.rcParams[<span class="string">'font.sans-serif'</span>] = [<span class="string">'SimHei'</span>]  <span class="comment"># 指定默认字体</span></span><br><span class="line">mpl.rcParams[<span class="string">'axes.unicode_minus'</span>] = <span class="keyword">False</span>  <span class="comment"># 解决保存图像时负号'-'显示为方块的问题</span></span><br><span class="line"><span class="comment">##################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试决策树的构建</span></span><br><span class="line">myDat, labels = createDataSet()</span><br><span class="line">myTree = createTree(myDat, labels)</span><br><span class="line"><span class="comment"># 绘制决策树</span></span><br><span class="line"><span class="keyword">import</span> treePlotter</span><br><span class="line">treePlotter.createPlot(myTree)</span><br></pre></td></tr></table></figure>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># Filename: treePlotter.py</span></span><br><span class="line"><span class="comment"># Author：hankcs</span></span><br><span class="line"><span class="comment"># Date: 2015/2/9 21:24</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义文本框和箭头格式</span></span><br><span class="line">decisionNode = dict(boxstyle=<span class="string">"round4"</span>, color=<span class="string">'#3366FF'</span>)  <span class="comment">#定义判断结点形态</span></span><br><span class="line">leafNode = dict(boxstyle=<span class="string">"circle"</span>, color=<span class="string">'#FF6633'</span>)  <span class="comment">#定义叶结点形态</span></span><br><span class="line">arrow_args = dict(arrowstyle=<span class="string">"&lt;-"</span>, color=<span class="string">'g'</span>)  <span class="comment">#定义箭头</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制带箭头的注释</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotNode</span><span class="params">(nodeTxt, centerPt, parentPt, nodeType)</span>:</span></span><br><span class="line">    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">                            xytext=centerPt, textcoords=<span class="string">'axes fraction'</span>,</span><br><span class="line">                            va=<span class="string">"center"</span>, ha=<span class="string">"center"</span>, bbox=nodeType, arrowprops=arrow_args)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算叶结点数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNumLeafs</span><span class="params">(myTree)</span>:</span></span><br><span class="line">    numLeafs = <span class="number">0</span></span><br><span class="line">    firstStr = myTree.keys()[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__ == <span class="string">'dict'</span>:</span><br><span class="line">            numLeafs += getNumLeafs(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            numLeafs += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> numLeafs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算树的层数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTreeDepth</span><span class="params">(myTree)</span>:</span></span><br><span class="line">    maxDepth = <span class="number">0</span></span><br><span class="line">    firstStr = myTree.keys()[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__ == <span class="string">'dict'</span>:</span><br><span class="line">            thisDepth = <span class="number">1</span> + getTreeDepth(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            thisDepth = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> thisDepth &gt; maxDepth:</span><br><span class="line">            maxDepth = thisDepth</span><br><span class="line">    <span class="keyword">return</span> maxDepth</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#在父子结点间填充文本信息</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotMidText</span><span class="params">(cntrPt, parentPt, txtString)</span>:</span></span><br><span class="line">    xMid = (parentPt[<span class="number">0</span>] - cntrPt[<span class="number">0</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">0</span>]</span><br><span class="line">    yMid = (parentPt[<span class="number">1</span>] - cntrPt[<span class="number">1</span>]) / <span class="number">2.0</span> + cntrPt[<span class="number">1</span>]</span><br><span class="line">    createPlot.ax1.text(xMid, yMid, txtString, va=<span class="string">"center"</span>, ha=<span class="string">"center"</span>, rotation=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotTree</span><span class="params">(myTree, parentPt, nodeTxt)</span>:</span></span><br><span class="line">    numLeafs = getNumLeafs(myTree)</span><br><span class="line">    depth = getTreeDepth(myTree)</span><br><span class="line">    firstStr = myTree.keys()[<span class="number">0</span>]</span><br><span class="line">    cntrPt = (plotTree.xOff + (<span class="number">1.0</span> + float(numLeafs)) / <span class="number">2.0</span> / plotTree.totalW, plotTree.yOff)</span><br><span class="line">    plotMidText(cntrPt, parentPt, nodeTxt)  <span class="comment">#在父子结点间填充文本信息</span></span><br><span class="line">    plotNode(firstStr, cntrPt, parentPt, decisionNode)  <span class="comment">#绘制带箭头的注释</span></span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    plotTree.yOff = plotTree.yOff - <span class="number">1.0</span> / plotTree.totalD</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__ == <span class="string">'dict'</span>:</span><br><span class="line">            plotTree(secondDict[key], cntrPt, str(key))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            plotTree.xOff = plotTree.xOff + <span class="number">1.0</span> / plotTree.totalW</span><br><span class="line">            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)</span><br><span class="line">            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))</span><br><span class="line">    plotTree.yOff = plotTree.yOff + <span class="number">1.0</span> / plotTree.totalD</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createPlot</span><span class="params">(inTree)</span>:</span></span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">    fig.clf()</span><br><span class="line">    axprops = dict(xticks=[], yticks=[])</span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="keyword">False</span>, **axprops)</span><br><span class="line">    plotTree.totalW = float(getNumLeafs(inTree))</span><br><span class="line">    plotTree.totalD = float(getTreeDepth(inTree))</span><br><span class="line">    plotTree.xOff = <span class="number">-0.5</span> / plotTree.totalW;</span><br><span class="line">    plotTree.yOff = <span class="number">1.0</span>;</span><br><span class="line">    plotTree(inTree, (<span class="number">0.5</span>, <span class="number">1.0</span>), <span class="string">''</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<h2 id="C4-5生成算法"><a href="#C4-5生成算法" class="headerlink" title="C4.5生成算法"></a>C4.5生成算法</h2><h3 id="算法描述-1"><a href="#算法描述-1" class="headerlink" title="算法描述"></a>算法描述</h3><p>输入： 训练数据集D，特征集A，阈值$\epsilon$</p>
<p>输出：决策树T</p>
<p>(1) 如果D中所有实例属于同一类$C_k$,则置T为单节点树，并将$C_k$作为该节点的类，返回T</p>
<p>(2) 如果$A=\emptyset$,则置T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类，返回T</p>
<p>(3) 否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征$A_g$</p>
<p>(4) 如果$A_g$的信息增益比小于阈值$\epsilon$，则置T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类，返回T</p>
<p>(5) 否则，对$A_g$的每一个可能值$a_i$,依$A_g=a_i$将D分割为子集若干非空$D_i$,将$D_i$中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树T，返回T</p>
<p>(6) 对节点i，以$D_i$作为训练集，以$A-\{A_g\}$为特征集，递归调用(1)~(5)步，得到子树$T_i$,返回$T_i$</p>
<h3 id="python实现"><a href="#python实现" class="headerlink" title="python实现"></a>python实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplitByC45</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    <span class="string">"""</span><br><span class="line">    选择最好的数据集划分方式</span><br><span class="line">    :param dataSet:</span><br><span class="line">    :return:</span><br><span class="line">    """</span></span><br><span class="line">    numFeatures = len(dataSet[<span class="number">0</span>]) - <span class="number">1</span>  <span class="comment"># 最后一列是分类</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)</span><br><span class="line">    bestInfoGainRate = <span class="number">0.0</span></span><br><span class="line">    bestFeature = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):  <span class="comment"># 遍历所有维度特征</span></span><br><span class="line">        infoGainRate = calcInformationGainRate(dataSet, baseEntropy, i)</span><br><span class="line">        <span class="keyword">if</span> (infoGainRate &gt; bestInfoGainRate):  <span class="comment"># 选择最大的信息增益</span></span><br><span class="line">            bestInfoGainRate = infoGainRate</span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature  <span class="comment"># 返回最佳特征对应的维度</span></span><br></pre></td></tr></table></figure>
<p>调用方法只需加个参数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">myTree = createTree(myDat, labels, chooseBestFeatureToSplitByC45)</span><br></pre></td></tr></table></figure>
<h1 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h1><p>决策树很容易发生过拟合，过拟合的原因在于学习的时候过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法就是简化已生成的决策树，也就是剪枝。</p>
<p>决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。</p>
<p>设决策树T的叶节点有|T|个，t是某个叶节点，t有$N_t$个样本点，其中归入k类的样本点有$N_{tk}$个，$H_t(T)$为叶节点t上的经验熵，α≥0为参数，则损失函数可以定义为：</p>
<p>$$C_{\alpha}(T)=\sum_{t=1}^{\vert T \vert} N_tH_t(T) + \alpha \vert T \vert$$</p>
<p>其中经验熵Ht(T)为：</p>
<p>$$H_t(T)=- \sum_k \frac{N_{ik}}{N_t} \log \frac{N_{tk}}{N_t}$$</p>
<p>表示叶节点t所代表的类别的不确定性。损失函数对它求和表示所有被导向该叶节点的样本点所带来的不确定的和的和。</p>
<p>在损失函数中，将右边第一项记作：</p>
<p>$$C(T)=\sum_{t=1}^{\vert T \vert}N_tH_t(T)=-\sum_{t=1}^{\vert T \vert}\sum_{k=1}^K N_{tk} \log \frac{N_{tk}}{N_t}$$</p>
<p>则损失函数可以简单记作：</p>
<p>$$C_{\alpha}(T)=C(T) + \alpha \vert T \vert$$</p>
<p>C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数α≥0控制两者之间的影响，α越大，模型越简单，α=0表示不考虑复杂度。</p>
<p>剪枝，就是当α确定时，选择损失函数最小的模型。子树越大C(T)越小，但是α|T|越大，损失函数反映的是两者的平衡。</p>
<p>决策树的生成过程只考虑了信息增益或信息增益比，只考虑更好地拟合训练数据，而剪枝过程则考虑了减小复杂度。前者是局部学习，后者是整体学习。</p>
<h2 id="树的剪枝算法"><a href="#树的剪枝算法" class="headerlink" title="树的剪枝算法"></a>树的剪枝算法</h2><p>从每个叶节点往上走，走了后如果损失函数减小了，则减掉叶节点，将父节点作为叶节点。如图：</p>
<p><img src="/resource/blog/2016-08/决策树剪枝.jpg" alt="树的剪枝算法"></p>
<p>说是这么说，实际上如果叶节点有多个，那么父节点变成叶节点后，新叶节点到底应该选择原来的叶节点中的哪一种类别呢？大概又是多数表决吧，原著并没有深入展开。</p>
<h1 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h1><p>分类与回归树（CART）模型同样由特征选取、树的生成和剪枝组成，既可以用于分类也可以用于回归。CART假设决策树是二叉树，内部节点特征的取值为是和否，对应一个实例的特征是否是这样的。决策树递归地二分每个特征，将输入空间划分为有限个单元。</p>
<h2 id="CART生成"><a href="#CART生成" class="headerlink" title="CART生成"></a>CART生成</h2><p>决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼系数最小化准则，进行特征选择，生成二叉树。</p>
<h3 id="回归树"><a href="#回归树" class="headerlink" title="回归树"></a>回归树</h3><p>回归树与分类树在数据集上的不同就是数据集的输出部分不是类别，而是连续变量。</p>
<p>假设输入空间已经被分为M个单元输入空间单元$R_1,R_2,\cdots,R_M$，分别对应输出值$c_m$，于是回归树模型可以表示为：</p>
<p>$$f(x)=\sum_{m=1}^Mc_mI(x \in R_m)$$</p>
<p>回归树的预测误差：</p>
<p>$$\sum_{x_x \in R_m}(y_i - f(x_i))^2$$</p>
<p>那么输出值就是使上面误差最小的值，也就是均值：</p>
<p>$$\hat c_m = ave(y_i \vert x_i \in R_m)$$</p>
<p>难点在于怎么划分，一种启发式的方法（其实就是暴力搜索吧）：</p>
<p>遍历所有输入变量，选择第j个变量和它的值s作为切分变量和切分点，将空间分为两个区域：</p>
<p>$$R_1(j,s)=\{x \vert x^{(j)} \leq s\} 和R_2(j,s)=\{x \vert x^{(j)} &gt; s\}$$</p>
<p>然后计算两个区域的平方误差，求和，极小化这个和，具体的，就是：</p>
<p>$$\min_{j,s} \left [ \min_{c_1} \sum_{x_i \in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)}(y_i-c_2)^2 \right ]$$</p>
<p>当j最优化的时候，就可以将切分点最优化：</p>
<p>$$\hat c_1 = ave()y_i | x_i \in R_1(j,s)) 和 \hat c_2 = ave()y_i | x_i \in R_2(j,s))$$</p>
<p>递归调用此过程，这种回归树通常称为最小二乘回归树。</p>
<h3 id="最小二乘回归树生成算法"><a href="#最小二乘回归树生成算法" class="headerlink" title="最小二乘回归树生成算法"></a>最小二乘回归树生成算法</h3><p>输入：训练数据集D</p>
<p>输出：回归树f(x)</p>
<p>在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：</p>
<p>(1) 选择最优切分变量j与切分点s，求解：</p>
<p>$$\min_{j,s} \left [ \min_{c_1} \sum_{x_i \in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)}(y_i-c_2)^2 \right ]$$</p>
<p>遍历变量j，对固定的切分变量j扫描切分点s,选择使上式达到最小值的对(j,s)</p>
<p>(2) 用选定的对(j,s)划分区域并决定相应的输出值：</p>
<p>$$R_1(j,s)=\{x \vert x^{(j)} \leq s\} 和R_2(j,s)=\{x \vert x^{(j)} &gt; s\}$$</p>
<p>$$\hat c_m = \frac{1}{N_m}\sum_{x_i \in R_m(j,s)} y_i, x \in R_m,m=1,2 $$</p>
<p>(3) 继续对两个子区域调用步骤(1)和(2)，直到满足停止条件</p>
<p>(4) 将输入空间划分为M个区域R_1,R_2,\cdots,R_M,生成决策树：</p>
<p>$$f(x)=\sum_{m=1}^M \hat c_m I(x \in R_m)$$</p>
<h3 id="分类树"><a href="#分类树" class="headerlink" title="分类树"></a>分类树</h3><p>与回归树算法流程类似，只不过选择的是最优切分特征和最优切分点，并采用基尼指数衡量。基尼指数定义：</p>
<p>$$Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$$</p>
<p>对于给定数据集D，其基尼指数是：</p>
<p>$$Gini(D)=1-\sum_{k=1}^K \left ( \frac{\vert C_k \vert}{\vert D \vert} \right) ^2$$</p>
<p>Ck是属于第k类的样本子集，K是类的个数。Gini(D)反应的是D的不确定性（与熵类似），分区的目标就是降低不确定性。</p>
<p>D根据特征A是否取某一个可能值a而分为D1和D2两部分：</p>
<p>$$D_1=\{(x,y) \in D \vert A(x) = a\}, D_2 = D - D_1$$</p>
<p>则在特征A的条件下，D的基尼指数是：</p>
<p>$$Gini(D,A)=\frac{D_1}{D}Gini(D_1) + \frac{D_2}{D}Gini(D_2)$$</p>
<p>有了上述知识储备，可以给出CART生成算法的伪码：</p>
<p>设节点的当前数据集为D，对D中每一个特征A，对齐每个值a根据D中样本点是否满足A==a分为两部分，计算基尼指数。对所有基尼指数选择最小的，对应的特征和切分点作为最优特征和最优切分点，生成两个子节点，将对应的两个分区分配过去，然后对两个子节点递归。</p>
<h2 id="CART剪枝"><a href="#CART剪枝" class="headerlink" title="CART剪枝"></a>CART剪枝</h2><p>在上面介绍的损失函数中，当α固定时，一定存在使得损失函数最小的子树，记为复杂度=Tα，α偏大Tα就偏小。设对α递增的序列，对应的最优子树序列为Tn，子树序列第一棵包含第二棵，依次类推。</p>
<p>从T0开始剪枝，对它内部的任意节点t，只有t这一个节点的子树的损失函数是：</p>
<p>$$C_{\alpha}=C(t)+\alpha$$</p>
<p>以t为根节点的子树的损失函数是：</p>
<p>$$C_{\alpha}(T_t)=C(T_t)+\alpha \vert T \vert$$</p>
<p>当α充分小，肯定有:</p>
<p>$$C_{\alpha}(T_t)&lt;C_{\alpha}(t)$$</p>
<p>这个不等式的意思是复杂模型在复杂度影响力小的情况下损失函数更小。</p>
<p>当α增大到某一点，这个不等式的符号会反过来。</p>
<p>只要$\alpha = \frac{C(t)-C(T_t)}{\vert T_t \vert -1}$,损失函数值就相同，但是t更小啊，所以t更可取，于是把Tt剪枝掉。</p>
<p>为此，对每一个t，计算</p>
<p>$$g(t)=$\frac{C(t)-C(T_t)}{\vert T_t \vert -1}$$</p>
<p>表示损失函数的减少程度，从T中剪枝掉g(t)最小的Tt，取新的α=g(t)，直到根节点。这样就得到了一个子树序列，对此序列，应用独立的验证数据集交叉验证，选取最优子树，剪枝完毕。</p>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a data-url="http://keyunluo.github.io/2016/09/23/2016-09-23-statical-learning-5.html" data-id="cix04cfye005f1jglnibb0q45" class="article-share-link">分享到</a><div class="tags"><a href="/tags/机器学习/">机器学习</a></div><div class="post-nav"><a href="/2016/09/24/2016-09-24-compress-1.html" class="pre">数据压缩(1) —— 概述</a><a href="/2016/09/22/2016-09-22-hadoop-filesystem-1.html" class="next">Hadoop源码学习(15)——文件系统(1)</a></div><div data-thread-key="2016/09/23/2016-09-23-statical-learning-5.html" data-title="统计学习方法——决策树" data-url="http://keyunluo.github.io/2016/09/23/2016-09-23-statical-learning-5.html" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2016/09/23/2016-09-23-statical-learning-5.html" data-title="统计学习方法——决策树" data-url="http://keyunluo.github.io/2016/09/23/2016-09-23-statical-learning-5.html" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-5"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AdvancedAlgorithms/">AdvancedAlgorithms</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/AlgorithmApplication/">AlgorithmApplication</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/BigData/">BigData</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DistributedSystem/">DistributedSystem</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/FileAndStorage/">FileAndStorage</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Life/">Life</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MachineLearning/">MachineLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming/">Programming</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SourceCodeLearning/">SourceCodeLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tools/">Tools</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/PlantUML/" style="font-size: 15px;">PlantUML</a> <a href="/tags/hadoop/" style="font-size: 15px;">hadoop</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/VPN/" style="font-size: 15px;">VPN</a> <a href="/tags/Oracle/" style="font-size: 15px;">Oracle</a> <a href="/tags/DataBase/" style="font-size: 15px;">DataBase</a> <a href="/tags/elasticsearch/" style="font-size: 15px;">elasticsearch</a> <a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/sqoop/" style="font-size: 15px;">sqoop</a> <a href="/tags/生活/" style="font-size: 15px;">生活</a> <a href="/tags/毕业/" style="font-size: 15px;">毕业</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/UML/" style="font-size: 15px;">UML</a> <a href="/tags/二次排序/" style="font-size: 15px;">二次排序</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/CentOS/" style="font-size: 15px;">CentOS</a> <a href="/tags/Cloudera/" style="font-size: 15px;">Cloudera</a> <a href="/tags/Markdown/" style="font-size: 15px;">Markdown</a> <a href="/tags/Latex/" style="font-size: 15px;">Latex</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/数据压缩/" style="font-size: 15px;">数据压缩</a> <a href="/tags/存储引擎/" style="font-size: 15px;">存储引擎</a> <a href="/tags/LSM树/" style="font-size: 15px;">LSM树</a> <a href="/tags/分布式系统/" style="font-size: 15px;">分布式系统</a> <a href="/tags/OpenCV/" style="font-size: 15px;">OpenCV</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/12/23/2016-12-23-distributed-system-1.html">分布式系统——课程总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/23/2016-12-22-data-mining.html">数据挖掘——课程总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/16/2016-11-16-skiplist1.html">SkipList 跳跃表(1) ——基本介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/16/2016-11-16-lsm1.html">LSM Tree (1)</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/15/2016-11-15-hash.html">Hash存储引擎</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/03/2016-11-03-advanced-algorithm-assignment2.html">高级算法--作业2</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/13/2016-10-13-advanced-algorithm-assignment1.html">高级算法--作业1</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/06/2016-10-06-advanced-algorithm-3.html">高级算法(3)--Min-Cut-Max-Flow(2)-近似算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/05/2016-10-05-advanced-algorithm-2.html">高级算法(2)--Min-Cut-Max-Flow(1)-确定性算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/04/2016-10-04-advanced-algorithm-1.html">高级算法(1)--NP完全性与近似算法</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 外部链接</i></div><ul></ul><a href="http://keyunluo.github.io/algorithm" title="个人算法练习" target="_blank">个人算法练习</a></div><div class="widget"><div class="widget-title"><i class="fa fa-bar-chart"> 访客</i></div><a href="http://info.flagcounter.com/uNxw"><img src="http://s07.flagcounter.com/count2/uNxw/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_3/labels_0/pageviews_0/flags_0/percent_0/" alt="" border="0"></a></div></div></div><div class="pure-u-1 pure-u-md-4-5"><div id="footer">© <a href="/." rel="nofollow">流光.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=1.0.0"><script>var duoshuoQuery = {short_name:'streamers'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-76470846-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?945b21807344d0260f24455bbd82dfea";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><div id="script" type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></div><script type="text/javascript" src="/js/mathjax/2.7-latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>