<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Coder On The Road"><title>数据挖掘——课程总结 | 流光</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.1.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.2/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/2.2.3/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">数据挖掘——课程总结</h1><a id="logo" href="/.">流光</a><p class="description">他跑啊跑啊，只为追上那个曾经被寄予厚望的自己</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/guestbook/"><i class="fa fa-comments"> 留言</i></a><a href="/resource/share"><i class="fa fa-download"> 资源</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-4-5"><div class="content_container"><div class="post"><h1 class="post-title">数据挖掘——课程总结</h1><div class="post-meta">Dec 23, 2016<span> | </span><span class="category"><a href="/categories/MachineLearning/">MachineLearning</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/12/23/2016-12-22-data-mining.html" href="/2016/12/23/2016-12-22-data-mining.html#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#降维"><span class="toc-number">1.</span> <span class="toc-text">降维</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Distance-Measure"><span class="toc-number">1.1.</span> <span class="toc-text">Distance Measure</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PCA"><span class="toc-number">1.2.</span> <span class="toc-text">PCA</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SVD"><span class="toc-number">1.3.</span> <span class="toc-text">SVD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MDS"><span class="toc-number">1.4.</span> <span class="toc-text">MDS</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#关联规则挖掘"><span class="toc-number">2.</span> <span class="toc-text">关联规则挖掘</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Support-Monotonicity-Property"><span class="toc-number">2.1.</span> <span class="toc-text">Support Monotonicity Property</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Downward-Closure-Property"><span class="toc-number">2.2.</span> <span class="toc-text">Downward Closure Property</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Apriori"><span class="toc-number">2.3.</span> <span class="toc-text">Apriori</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#聚类"><span class="toc-number">3.</span> <span class="toc-text">聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#k-means"><span class="toc-number">3.1.</span> <span class="toc-text">k-means</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spectral-Clustering"><span class="toc-number">3.2.</span> <span class="toc-text">Spectral Clustering</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分类"><span class="toc-number">4.</span> <span class="toc-text">分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Naive-Bayes"><span class="toc-number">4.1.</span> <span class="toc-text">Naive Bayes</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SVM"><span class="toc-number">4.2.</span> <span class="toc-text">SVM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Logistic-Regression"><span class="toc-number">4.3.</span> <span class="toc-text">Logistic Regression</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#凸优化"><span class="toc-number">5.</span> <span class="toc-text">凸优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Dual-Problem"><span class="toc-number">5.1.</span> <span class="toc-text">The Dual Problem</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#高级分类方法"><span class="toc-number">6.</span> <span class="toc-text">高级分类方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Semisupervised-Learning"><span class="toc-number">6.1.</span> <span class="toc-text">Semisupervised Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Active-Learning"><span class="toc-number">6.2.</span> <span class="toc-text">Active Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ensemble-Methods"><span class="toc-number">6.3.</span> <span class="toc-text">Ensemble Methods</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#线性回归"><span class="toc-number">7.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Least-Square"><span class="toc-number">7.1.</span> <span class="toc-text">Least Square</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Ridge-Regression"><span class="toc-number">7.2.</span> <span class="toc-text">Ridge Regression</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Web挖掘"><span class="toc-number">8.</span> <span class="toc-text">Web挖掘</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Page-Ranking"><span class="toc-number">8.1.</span> <span class="toc-text">Page Ranking</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Collaborative-Filtering"><span class="toc-number">8.2.</span> <span class="toc-text">Collaborative Filtering</span></a></li></ol></li></ol></div></div><div class="post-content"><blockquote>
<p><strong>本节内容：</strong>2016年秋南京大学计算机系数据挖掘课程学期总结。</p>
</blockquote>
<a id="more"></a>
<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><h3 id="Distance-Measure"><a href="#Distance-Measure" class="headerlink" title="Distance Measure"></a>Distance Measure</h3><ul>
<li><p><strong>$L_p-Norm$</strong>(Minkowski distance)</p>
<p>形式：给定$\bar{X}=(x_1,x_2,\ldots, x_d), \bar{Y}=(y_1,y_2,\ldots, y_d)$, 他们之间的Lp范数距离为：
$$Dist(\bar{X}, \bar{Y}) = \left( \sum_{i=1}^d \vert x_i - y_i |^p \right )^{1/p}$$</p>
<p>特例：p=1时为曼哈顿距离：各分量绝对值之和；p=2时为欧式距离：根号下和的平方；$p=\infty$时为无穷范数，又称为切比雪夫距离 ：绝对值最大的数；p=0时为0范数：非零元素个数，非凸；0&lt;p&lt;1时为分数范数，非凸。</p>
</li>
<li><p>Standardized Euclidean distance</p>
<p>  标准化欧式距离，两个分量间减去均值除以标准差进行标准化：</p>
<p>  $$Dist(\bar{X}, \bar{Y}) = \left( \sum_{i=1}^d \vert \frac{x_i - y_i}{s_i} |^2 \right )^{1/2}$$</p>
</li>
<li><p><strong>Mahalanobis Distance</strong></p>
<p>  马氏距离用来消除不同维度之间的相关性和和尺度不同的性质。样本矩阵$X$的协方差矩阵为$\Sigma$,则它的各个向量间的马氏距离为：</p>
<p>  $$Dist(X_i, X_j) = \sqrt{(X_i - X_j)^T\Sigma^{-1}(X_i-X_j)}$$</p>
<p>  若对协方差矩阵进行正交投影分解，即：$\Sigma = U \Lambda U^T = \sum_{i=1}^{d} \sigma_i u_iu_i^T$, 那么，$\Sigma^{-1} = U \Lambda U^T= \sum_{i=1}^{d} \sigma_i^{-1} u_iu_i^T$, 马氏距离可表示成如下：</p>
<p>  $$Dist(X_i, X_j) = \sqrt{(X_i - X_j)(\sum_{i=1}^d \sigma_i^{-1}u_iu_i^T)(X_i - X_j)^T} = \sqrt{\sum_{i=1}^d \frac{((X_i - X_j)u_i)^2}{\sigma_i}}$$</p>
</li>
<li><p>Cosine</p>
<p>  夹角余弦越大表示两个向量的夹角越小,越相似，夹角余弦越小表示两向量的夹角越大，越不相似。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。</p>
<p>  $$\cos(\theta) = \frac{\sum_{i=1}^d x_iy_i}{\sqrt{\sum_{i=1}^d x_i^2} \sqrt{\sum_{i=1}^d y_i^2}}$$</p>
</li>
<li><p>Hamming distance</p>
<p>  两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。</p>
</li>
<li><p>Jaccard similarity coefficient</p>
<ul>
<li><p>杰卡德相似系数：两个集合A和B的交集元素在A，B的并集中所占的比例: $J(A,B) = \frac{A \bigcap B}{A \bigcup B}$</p>
</li>
<li><p>杰卡德距离:两个集合中不同元素占所有元素的比例， $J_\delta(A,B) = 1 - \frac{A \bigcap B}{A \bigcup B}$</p>
</li>
</ul>
</li>
<li><p>Correlation coefficient And Correlation distance</p>
<ul>
<li><p>相关系数：相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。$\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}} = \frac{E((X-E(X))(Y-E(Y)))}{\sqrt{D(X)} \sqrt{D(Y)}}$</p>
</li>
<li><p>相关距离：$D_{xy} = 1 - \rho_{XY}$</p>
</li>
</ul>
</li>
<li><p>Information Entropy</p>
<p>  信息熵是衡量分布的混乱程度或分散程度的一种度量。$Entropy(X) = - \sum_{i=1}^{n} p_i \log_2 p_i$</p>
</li>
<li><p><strong>Nonlinear Distributions: ISOMAP</strong></p>
<ul>
<li>对每个点计算其k个近邻</li>
<li>构造一个带权图G，结点代表数据点，边的权值代表k个近邻间的欧式距离。</li>
<li>计算任意两点间的距离$Dist(\bar{X}, \bar{Y})$为图中点$\bar{X}, \bar{Y}$之间的最短距离。</li>
<li>计算 MDS(multidimensional scaling)：得到图的向量表示，即映设矩阵。</li>
</ul>
</li>
</ul>
<h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><ol>
<li><p>轴旋转： $x = [x^1, x^2, \ldots, x^d]^T \in \mathcal{R}^d \Leftrightarrow x = x^1e_1 + x^2e_2 + \ldots + x^de_d$，W:正交矩阵，$W = [w_1, w_2, \ldots, w_d]$, 则$x = WW^Tx = (\sum_{i=1}^dw_iw_i^T)x = \sum_{i=1}^dw_i(w_i^Tx) = (w_1^Tx)w_1 + \ldots + (w_d^Tx)w_d$, 因而，在坐标W的投影下，新坐标是$y = [w_1^Tx, w_2^Tx, \ldots, w_d^Tx]^T \in \mathcal{R}^d$. $w_i^T = <w\_i, x="">$表示新的坐标，含义是将$x$沿着$w_i$的方向投影。</w\_i,></p>
</li>
<li><p>降维目标：给定数据点集合${x_1,x_2, \ldots, x_n}, x_i \in \mathcal{R}^d$，寻找一个投影方向${w_1,w_2,\ldots,w_k}$，使得方差$y_1 = [w_1^Tx_1, w_2^Tx_1,\ldots, w_k^Tx_1]^T, y_2 = [w_1^Tx_2, w_2^Tx_2,\ldots, w_k^Tx_2]^T, \ldots, y_n = [w_1^Tx_n, w_2^Tx_n,\ldots, w_k^Tx_n]^T$ 最大，即最大化类间方差。</p>
<p>考虑一维的情况，形式化为：新坐标：$w_1^Tx_1,w_1^Tx_2, \ldots, w_1^Tx_n$,方差：$\frac{1}{n} \sum_{i=1}^n(w_1^Tx_i - \mu )^2, \mu = \frac{1}{n} \sum_{i=1}^nw_1^Tx_i$.</p>
</li>
<li><p>问题推导过程</p>
<ul>
<li>令$\bar{x} = \frac{1}{n} \sum_{i=1}^nx_i$为均值向量</li>
<li><p>那么，$\mu = w_1^T\bar{x}$,</p>
<p>  $$\begin{array} {lcl}
  \frac{1}{n} \sum_{i=1}^n(w_1^Tx_i - \mu)^2 &amp; = &amp; \frac{1}{n} \sum_{i=1}^n(w_1^Tx_i - w_1^T\bar{x})^2 \\
  &amp; = &amp; \frac{1}{n} \sum_{i=1}^n(w_1^T(x_i - \bar{x}))^2 \\
  &amp; = &amp; \frac{1}{n} \sum_{i=1}^nw_1^T(x_i - \bar{x})(x_i - \bar{x})^Tw_1 \\
  &amp; = &amp; w_1^T(\frac{1}{n} \sum_{i=1}^n(x_i - \bar{x})(x_i - \bar{x})^T)w_1
  \end{array}$$</p>
</li>
<li><p>于是， PCA转变成一个优化问题：</p>
<blockquote>
<p>$\max_{w \in \mathcal{R}^d}\ w^TCw$</p>
<p>$s.t. \Vert w \Vert_2^2 = 1$</p>
</blockquote>
<p>这里， $C = \sum_{i=1}^n(x_i - \bar{x})(x_i - \bar{x})^T)$为协方差矩阵。该协方差矩阵具有如下特征：对称性，半正定，矩阵的秩最大为n-1。</p>
</li>
</ul>
</li>
<li><p>解决方法</p>
<ul>
<li><p>拉格朗日法：$-w^TCw + \lambda(\Vert w \Vert_2^2 -1)$</p>
</li>
<li><p>对$w$求导，令导数为0：$-2Cw + 2\lambda w  = 0$, 得出$Cw = \lambda w$</p>
</li>
<li><p>$(w, \lambda)$ 是协方差矩阵C的特征向量和特征值</p>
</li>
<li><p>目标函数变成：$w^TCw = \lambda w^T w = \lambda$</p>
</li>
<li><p>因此， 我们选择C中最大的特征值和特征向量</p>
</li>
</ul>
</li>
<li><p><strong>PCA算法(1维)</strong></p>
<ul>
<li>计算均值向量：$\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$</li>
<li>计算协方差矩阵：$C = \sum_{i=1}^n(x_i - \bar{x})(x_i - \bar{x})^T)$</li>
<li>计算C中最大的特征值对应的特征向量</li>
</ul>
</li>
<li><p><strong>PCA算法(k维)</strong></p>
<ul>
<li><p><strong>优化目标</strong>:</p>
<blockquote>
<p>$\max_{w \in \mathcal{R}^{d*k}}\ trace(W^TCW)$</p>
<p>$s.t. W^TW = I$</p>
</blockquote>
<p>其中， $W=[w_1, \ldots, w_k]$ 是C的k个大的特征向量。</p>
</li>
<li><p><strong>算法流程</strong></p>
<ul>
<li>计算均值向量：$\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$</li>
<li>计算协方差矩阵：$C = \sum_{i=1}^n(x_i - \bar{x})(x_i - \bar{x})^T)$</li>
<li>计算C中最大的k个特征值对应的特征向量</li>
</ul>
</li>
<li><p><strong>特征值含义</strong></p>
<ul>
<li>$\lambda_i$是第i个特征坐标间的方差</li>
<li>度量PCA质量的好坏：所取的前k个特征值之和与所有的特征值和之比。</li>
</ul>
</li>
<li><p><strong>另一个思考视角</strong>：最小化投影误差</p>
</li>
<li><p>PCA算法是线性的，无监督的</p>
</li>
</ul>
</li>
</ol>
<h3 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h3><ol>
<li><p>奇异值分解: $X=U\Sigma V^T = \sum_{i=1}^d \sigma_iu_iv_i^T$:</p>
<ul>
<li>$U = [u_1, u_2,\ldots, u_d] \in \mathcal{R}^{d*d}, U^TU=UU^T = I$</li>
<li>$V = [v_1,v_2, \ldots,v_d] \in \mathcal{R}^{n*d}, V^TV=I$</li>
<li>$\Sigma = diag(\sigma_1, \sigma_2,\ldots, \sigma_d),\in \mathcal{R}^{d*d}, \sigma_1 \ge \sigma_2 \ge \ldots \ge \sigma_d \ge 0$</li>
</ul>
</li>
<li><p>紧凑SVD：$X=U_r\Sigma_r V_r^T = \sum_{i=1}^r \sigma_iu_iv_i^T$， <strong>rank(r) &lt; min(d,n)</strong>:</p>
<ul>
<li>$U_r = [u_1, u_2,\ldots, u_r] \in \mathcal{R}^{d*r}, U_r^TU_r=I$</li>
<li>$V_r = [v_1,v_2, \ldots,v_r] \in \mathcal{R}^{n*r}, V_r^TV_r=I$</li>
<li>$\Sigma_r = diag(\sigma_1, \sigma_2,\ldots, \sigma_r),\in \mathcal{R}^{r*r}, \sigma_1 \ge \sigma_2 \ge \ldots \ge \sigma_r \ge 0$</li>
</ul>
</li>
<li><p>使用SVD降维</p>
<ul>
<li>计算X的k个最大的左奇异向量$u_1,u_2,\ldots,u_k$</li>
<li>x的新坐标：$U_k^Tx = [u_1^Tx, u_2^Tx, \ldots, u_k^Tx] \in \mathcal{R}^k, U_k = [u_1, u_2,\ldots,u_k] \in \mathcal{R}^{d*k}$</li>
<li>X的新坐标：$U_k^TX=U_k^TU_r\Sigma_rV_r^T = \Sigma_kV_k^T$</li>
</ul>
</li>
<li><p>SVD的优化问题：</p>
<ul>
<li>一维：<blockquote>
<p>$\max_{w \in \mathcal{R}^d}\ w^T(XX^T)w$</p>
<p>$s.t. \Vert w \Vert_2^2 = 1$</p>
</blockquote>
</li>
<li>k维：<blockquote>
<p>$\max_{w \in \mathcal{R}^{d*k}}\ trace(W^T(XX^T)W)$</p>
<p>$s.t. W^TW = I$</p>
</blockquote>
</li>
</ul>
</li>
<li><p>PCA by SVD</p>
<ul>
<li>计算均值向量$\bar{x}$:$ \frac{1}{n} \sum_{i=1}^n x_i$</li>
<li>计算$\bar{X}=[x_1 - \bar{x},\ldots, x_n - \bar{x}]$最大的k个左奇异值</li>
</ul>
</li>
</ol>
<h3 id="MDS"><a href="#MDS" class="headerlink" title="MDS"></a>MDS</h3><ol>
<li><p>输入</p>
<ul>
<li>图G：n个结点</li>
<li>距离:$\delta_{ij} = \delta_{ji}$</li>
</ul>
</li>
<li><p>输出</p>
<ul>
<li>适配于距离的坐标集</li>
</ul>
</li>
<li><p>优化问题</p>
<p> $\min_{x_1,x_2,\ldots, x_n \in \mathcal{R}^k} \sum_{i,j:i&lt;j}(\Vert x_i - x_j \Vert_2 - \delta_{ij})^2$</p>
</li>
<li><p>MDS算法</p>
<ul>
<li>计算点积：$S= -\frac{1}{2}(I - \frac{np.ones^T}{n})G^2(\frac{np.ones^T}{n})$</li>
<li>对S进行奇异值分解：$S=U \Lambda U^T = \sum_{i=1}^n\lambda_iu_iu_i^T$</li>
<li>新坐标：$U_k\Lambda_k^{1/2} \in \mathcal{R}^{n*k}$</li>
</ul>
</li>
</ol>
<h2 id="关联规则挖掘"><a href="#关联规则挖掘" class="headerlink" title="关联规则挖掘"></a>关联规则挖掘</h2><h3 id="Support-Monotonicity-Property"><a href="#Support-Monotonicity-Property" class="headerlink" title="Support Monotonicity Property"></a>Support Monotonicity Property</h3><h3 id="Downward-Closure-Property"><a href="#Downward-Closure-Property" class="headerlink" title="Downward Closure Property"></a>Downward Closure Property</h3><h3 id="Apriori"><a href="#Apriori" class="headerlink" title="Apriori"></a>Apriori</h3><h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><h3 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h3><h3 id="Spectral-Clustering"><a href="#Spectral-Clustering" class="headerlink" title="Spectral Clustering"></a>Spectral Clustering</h3><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><h2 id="凸优化"><a href="#凸优化" class="headerlink" title="凸优化"></a>凸优化</h2><h3 id="The-Dual-Problem"><a href="#The-Dual-Problem" class="headerlink" title="The Dual Problem"></a>The Dual Problem</h3><h2 id="高级分类方法"><a href="#高级分类方法" class="headerlink" title="高级分类方法"></a>高级分类方法</h2><h3 id="Semisupervised-Learning"><a href="#Semisupervised-Learning" class="headerlink" title="Semisupervised Learning"></a>Semisupervised Learning</h3><h3 id="Active-Learning"><a href="#Active-Learning" class="headerlink" title="Active Learning"></a>Active Learning</h3><h3 id="Ensemble-Methods"><a href="#Ensemble-Methods" class="headerlink" title="Ensemble Methods"></a>Ensemble Methods</h3><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="Least-Square"><a href="#Least-Square" class="headerlink" title="Least Square"></a>Least Square</h3><h3 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h3><h2 id="Web挖掘"><a href="#Web挖掘" class="headerlink" title="Web挖掘"></a>Web挖掘</h2><h3 id="Page-Ranking"><a href="#Page-Ranking" class="headerlink" title="Page Ranking"></a>Page Ranking</h3><h3 id="Collaborative-Filtering"><a href="#Collaborative-Filtering" class="headerlink" title="Collaborative Filtering"></a>Collaborative Filtering</h3></div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a data-url="http://keyunluo.github.io/2016/12/23/2016-12-22-data-mining.html" data-id="cix1uny6z0076vlgluu35frvj" class="article-share-link">分享到</a><div class="tags"><a href="/tags/机器学习/">机器学习</a></div><div class="post-nav"><a href="/2016/12/23/2016-12-23-distributed-system-1.html" class="pre">分布式系统——课程总结</a><a href="/2016/11/16/2016-11-16-skiplist1.html" class="next">SkipList 跳跃表(1) ——基本介绍</a></div><div data-thread-key="2016/12/23/2016-12-22-data-mining.html" data-title="数据挖掘——课程总结" data-url="http://keyunluo.github.io/2016/12/23/2016-12-22-data-mining.html" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2016/12/23/2016-12-22-data-mining.html" data-title="数据挖掘——课程总结" data-url="http://keyunluo.github.io/2016/12/23/2016-12-22-data-mining.html" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-5"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AdvancedAlgorithms/">AdvancedAlgorithms</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/AlgorithmApplication/">AlgorithmApplication</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/BigData/">BigData</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DistributedSystem/">DistributedSystem</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/FileAndStorage/">FileAndStorage</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Life/">Life</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MachineLearning/">MachineLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming/">Programming</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SourceCodeLearning/">SourceCodeLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tools/">Tools</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Latex/" style="font-size: 15px;">Latex</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/Oracle/" style="font-size: 15px;">Oracle</a> <a href="/tags/DataBase/" style="font-size: 15px;">DataBase</a> <a href="/tags/sqoop/" style="font-size: 15px;">sqoop</a> <a href="/tags/hadoop/" style="font-size: 15px;">hadoop</a> <a href="/tags/elasticsearch/" style="font-size: 15px;">elasticsearch</a> <a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/生活/" style="font-size: 15px;">生活</a> <a href="/tags/毕业/" style="font-size: 15px;">毕业</a> <a href="/tags/二次排序/" style="font-size: 15px;">二次排序</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/Markdown/" style="font-size: 15px;">Markdown</a> <a href="/tags/VPN/" style="font-size: 15px;">VPN</a> <a href="/tags/UML/" style="font-size: 15px;">UML</a> <a href="/tags/PlantUML/" style="font-size: 15px;">PlantUML</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/Cloudera/" style="font-size: 15px;">Cloudera</a> <a href="/tags/CentOS/" style="font-size: 15px;">CentOS</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/数据压缩/" style="font-size: 15px;">数据压缩</a> <a href="/tags/OpenCV/" style="font-size: 15px;">OpenCV</a> <a href="/tags/LSM树/" style="font-size: 15px;">LSM树</a> <a href="/tags/存储引擎/" style="font-size: 15px;">存储引擎</a> <a href="/tags/分布式系统/" style="font-size: 15px;">分布式系统</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/12/23/2016-12-23-distributed-system-1.html">分布式系统——课程总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/23/2016-12-22-data-mining.html">数据挖掘——课程总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/16/2016-11-16-skiplist1.html">SkipList 跳跃表(1) ——基本介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/16/2016-11-16-lsm1.html">LSM Tree (1)</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/15/2016-11-15-hash.html">Hash存储引擎</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/03/2016-11-03-advanced-algorithm-assignment2.html">高级算法--作业2</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/13/2016-10-13-advanced-algorithm-assignment1.html">高级算法--作业1</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/06/2016-10-06-advanced-algorithm-3.html">高级算法(3)--Min-Cut-Max-Flow(2)-近似算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/05/2016-10-05-advanced-algorithm-2.html">高级算法(2)--Min-Cut-Max-Flow(1)-确定性算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/04/2016-10-04-advanced-algorithm-1.html">高级算法(1)--NP完全性与近似算法</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 外部链接</i></div><ul></ul><a href="http://keyunluo.github.io/algorithm" title="个人算法练习" target="_blank">个人算法练习</a></div><div class="widget"><div class="widget-title"><i class="fa fa-bar-chart"> 访客</i></div><a href="http://info.flagcounter.com/uNxw"><img src="http://s07.flagcounter.com/count2/uNxw/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_3/labels_0/pageviews_0/flags_0/percent_0/" alt="" border="0"></a></div></div></div><div class="pure-u-1 pure-u-md-4-5"><div id="footer">© <a href="/." rel="nofollow">流光.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=1.0.0"><script>var duoshuoQuery = {short_name:'streamers'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-76470846-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?945b21807344d0260f24455bbd82dfea";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><div id="script" type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></div><script type="text/javascript" src="/js/mathjax/2.7-latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>