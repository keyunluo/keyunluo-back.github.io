<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="Coder On The Road"><title>数据挖掘——试卷分析 | 流光</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.1.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.2/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/2.2.3/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">数据挖掘——试卷分析</h1><a id="logo" href="/.">流光</a><p class="description">他跑啊跑啊，只为追上那个曾经被寄予厚望的自己</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/guestbook/"><i class="fa fa-comments"> 留言</i></a><a href="/resource/share"><i class="fa fa-download"> 资源</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-4-5"><div class="content_container"><div class="post"><h1 class="post-title">数据挖掘——试卷分析</h1><div class="post-meta">Dec 27, 2016<span> | </span><span class="category"><a href="/categories/MachineLearning/">MachineLearning</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/12/27/2016-12-27-data-mining.html" href="/2016/12/27/2016-12-27-data-mining.html#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#PCA-3-points"><span class="toc-number">1.</span> <span class="toc-text">PCA(3 points)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Association-Pattern-Mining-2-points"><span class="toc-number">2.</span> <span class="toc-text">Association Pattern Mining(2 points)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NMF-3-points"><span class="toc-number">3.</span> <span class="toc-text">NMF(3 points)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SVM-6-points"><span class="toc-number">4.</span> <span class="toc-text">SVM(6 points)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ensemble-2-points"><span class="toc-number">5.</span> <span class="toc-text">Ensemble(2 points)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ridge-Regression-5-points"><span class="toc-number">6.</span> <span class="toc-text">Ridge Regression(5 points)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Advanced-Classification-4-points"><span class="toc-number">7.</span> <span class="toc-text">Advanced Classification(4 points)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Collaborative-Filtering-5-points"><span class="toc-number">8.</span> <span class="toc-text">Collaborative Filtering(5 points)</span></a></li></ol></div></div><div class="post-content"><blockquote>
<p><strong>本节内容：</strong>2015年秋南京大学计算机系数据挖掘期末试卷分析。</p>
</blockquote>
<a id="more"></a>
<h2 id="PCA-3-points"><a href="#PCA-3-points" class="headerlink" title="PCA(3 points)"></a>PCA(3 points)</h2><blockquote>
<p>Given n data points $x_1,\ldots,x_n$, where $x_i \in \mathcal{R}^d$. Describe how to find the top <strong>k</strong> principle components by <strong>SVD</strong>.</p>
</blockquote>
<p>解答：SVD奇异值分解</p>
<ul>
<li>对X进行奇异值分解：$X = U\Sigma V^T = \sum_{i=1}^d \sigma_iu_iv_i^T$</li>
<li>取X的k个最大的左奇异向量：$u_1,u_2,\ldots,u_k$</li>
<li>x的新坐标：$U_k^Tx = [u_1^Tx, u_2^Tx, \ldots, u_k^Tx] \in \mathcal{R}^k, U_k = [u_1, u_2,\ldots,u_k] \in \mathcal{R}^{d*k}$</li>
<li>X的新坐标：$U_k^TX=U_k^TU_r\Sigma_rV_r^T = \Sigma_kV_k^T$</li>
</ul>
<h2 id="Association-Pattern-Mining-2-points"><a href="#Association-Pattern-Mining-2-points" class="headerlink" title="Association Pattern Mining(2 points)"></a>Association Pattern Mining(2 points)</h2><blockquote>
<p>The <strong>downward closure property (i.e.,every subset of frequent itemset is also frequcent)</strong> is leveraged to design efficient algorithms for association pattern mining. Why does this property hold?</p>
</blockquote>
<p>解答：由于支持单调性，一个项集I包含在一个交易中，那么它的所有子集也包含在这个交易中，即子集的支持度不小于父集的支持度。</p>
<h2 id="NMF-3-points"><a href="#NMF-3-points" class="headerlink" title="NMF(3 points)"></a>NMF(3 points)</h2><blockquote>
<p>Suppose we want to find a rank-k approximation of matrix $X \in \mathcal{R}^{d*n}$ by nonnegative matrix factorization. What is the optimization problem? Is it convex?</p>
</blockquote>
<p>解答：</p>
<ul>
<li>优化问题：$\min_{U \in \mathcal{R}^{d*k}, V \in \mathcal{R}^{v*k}} \ \Vert X - UV^T \Vert_F^2 $， $s.t. \ U \ge 0, V \ge 0$</li>
<li>非负矩阵分解是非凸的</li>
</ul>
<h2 id="SVM-6-points"><a href="#SVM-6-points" class="headerlink" title="SVM(6 points)"></a>SVM(6 points)</h2><blockquote>
<p>Given a set of training data $(x_1,y_1),\ldots,(x_n,y_n)$, where $x_i \in \mathcal{R}^d$ and $y_i \in {\pm 1}$. The primal problem of SVM without intercept is given by: $$\min_{w \in \mathcal{R}^d} \sum_{i=1}^m \max(0,1-y_iw^Tx_i) + \frac{\lambda}{2} \Vert w \Vert_2^2$$. Show the derivation of the dual problem of SVM.</p>
<p>Hints: Let $\mathcal{l}(x) = max(0,1-x)$ be the hinge loss. Then, its conjugate function id given by
$$ \mathcal{l}^*(y) = sup_x(yx-\mathcal{l}(x)) = \begin{cases} y&amp; -1 \le y \le 0 \\
 \infty&amp; \ otherwise
 \end{cases}$$</p>
</blockquote>
<p> 解：SVM对偶问题的推导，无截距$b_0$</p>
<ul>
<li>由上述定义的hinge loss及其共轭函数知，原优化问题简化为：$\min_{w \in R^d} \sum_{i=1}^n \mathcal{l}(y_iw^Tx_i) + \frac{\lambda}{2}\Vert w \Vert_2^2$,等价于：
$$\sum_{i=1}^n \mathcal{l}(u_i) + \frac{\lambda}{2} \Vert w \Vert_2^2, s.t. \ u_i = y_iw^Tx_i, i = 1, \ldots, n$$</li>
</ul>
<ul>
<li>由拉格朗日乘法，得：$L(w,u,v) = \sum_{i=1}^n \mathcal{l}(u_i) + \frac{\lambda}{2} \Vert w \Vert_2^2 + \sum_{i=1}^n v_i(u_i - y_iw^Tx_i)$,其对偶问题：</li>
</ul>
<p>$$\begin{array} {lcl}
    g(v) &amp;=&amp; \inf_{w,u} L(w,u,v) \\
         &amp;=&amp; \inf_{w,u} \sum_{i=1}^n \mathcal{l}(u_i) + \frac{\lambda}{2} \Vert w \Vert_2^2 + \sum_{i=1}^n v_i(u_i - y_iw^Tx_i) \\
         &amp;=&amp; \inf_{w,u} \sum_{i=1}^n(\mathcal{l}(u_i) + v_iu_i) + (\frac{\lambda}{2} \Vert w \Vert_2^2 - w^T\sum_{i=1}^nv_iy_ix_i)
\end{array}$$</p>
<pre><code>依次最小化w,u:
</code></pre><ul>
<li>$\inf_{u_i}(\mathcal{l}(u_i) + v_iu_i) = -\sup_{u_i}(-v_iu_i - l(u_i)) = - \mathcal{l}^* (-v_i) = v_i $, if $0 \le v_i \le 1$</li>
<li>$\nabla_w L(w,u,v) = \lambda w - \sum_{i=1}^nv_iy_ix_i)$, 得：$w = \frac{1}{\lambda}  \sum_{i=1}^nv_iy_ix_i$</li>
<li>最后， $g(v) = \sum_{i=1}^n v_i - \frac{1}{2 \lambda}\sum_{i=1}^n\sum_{j=1}^n v_iv_jy_iy_jx_i^Tx_j$，得到对偶问题：
$$\max_{v \in R^n} \sum_{i=1}^n v_i - \frac{1}{2 \lambda}\sum_{i=1}^n\sum_{j=1}^n v_iv_jy_iy_jx_i^Tx_j, \ s.t.\ 0 \le v_i \le 1, i=1, \ldots,n$$</li>
</ul>
<h2 id="Ensemble-2-points"><a href="#Ensemble-2-points" class="headerlink" title="Ensemble(2 points)"></a>Ensemble(2 points)</h2><blockquote>
<p>Ensemble analysis is used to reduce the bias or variance of the classification process. Which of them is reduced by bagging/boosting?</p>
</blockquote>
<p> 解答：</p>
<p> a) Bagging aims to reduce the: <strong>variance</strong>(代表性算法：随机森林，随机化的决策树模型，每次随机选择一定大小的特征)</p>
<p> b) Boosting aims to reduce the:<strong>bias</strong>(代表性算法：AdaBoost，每个训练实例都有个权重，分类错误的实例会赋予一个更大的权重)</p>
<h2 id="Ridge-Regression-5-points"><a href="#Ridge-Regression-5-points" class="headerlink" title="Ridge Regression(5 points)"></a>Ridge Regression(5 points)</h2><blockquote>
<p>Given a set of training data $(x_1,y_1),\ldots,(x_n,y_n)$, where $x_i \in \mathcal{R}^d$ and $y_i \in \mathcal{R}$. Our goal is to learn a linear model $f(x) = x^Tw + b$ to predict the label $y \in \mathcal{R}$ of an instance $x \in \mathcal{R}^d$</p>
</blockquote>
<p>a) Show the optimization problem of the ridge regression for learning $w \in \mathcal{R}^d$ and $b \in \mathcal{R}$.
Notations:$X = [x_1,\ldots,x_n] \in \mathcal{R}^{d*n}, y = [y_1, \ldots, y_n]$</p>
<p>b) Derive the optimal solution $w_*$, and $b_*$ of the above problem.
Notations: I is the identity matrix, and $H = I - \frac{1}{n}1_n1_n^T$ is the centering matrix. Hints:$\frac{\partial \Vert u - A^Tw \Vert_2^2}{\partial w} = 2A(A^Tw-u)$</p>
<p>解答：</p>
<ol>
<li><p>优化问题：$\min_{b \in R,w \in R^d} \Vert y - Xw - 1_Nb \Vert_2^2 + \lambda \Vert w \Vert_2^2$, 其中：$1_N = [1,\ldots,1]^T \in R^d$</p>
</li>
<li><ul>
<li>对上式$b$进行求导，并令倒数为0：$-2*1_N^T(y-Xw -1_Nb)=0, b  = \frac{1}{N}1_N^T(y-Xw)$</li>
<li>对上式$w$进行求导，并令倒数为0：$2*X^T(Xw -y + 1_Nb) + 2 \lambda w=0, (X^T(I-\frac{1}{N}1_N1_N^T)X + \lambda I)w=X^T(I-\frac{1}{N}1_N1_N^T)y$</li>
<li>令$H = I - \frac{1}{N}1_N1_N^T$为中心矩阵，则得到：$w_* = (X^THX + \lambda I)^{-1}X^THy,b_* = \frac{1}{N}1_N^T(y-Xw_*)$</li>
</ul>
</li>
</ol>
<h2 id="Advanced-Classification-4-points"><a href="#Advanced-Classification-4-points" class="headerlink" title="Advanced Classification(4 points)"></a>Advanced Classification(4 points)</h2><blockquote>
<p>Give a brief introduction of <strong>semi-suppervised learning</strong> and <strong>active learning</strong></p>
</blockquote>
<p>解答：半监督学习和主动学习</p>
<ul>
<li><strong>semi-suppervised learning</strong>：<ul>
<li>标记数据的代价昂贵并且难于获得；无标签数据通常是大量可得到的；无标签数据是有用的：无标签数据可以用来评估数据的低维流型结构和特征的联合概率分布。</li>
<li>相关算法：<ul>
<li>元算法：使用任何现有的算法作为子程序，这类算法主要有Self-training和Co-training。</li>
<li>具体算法：半监督贝叶斯分类器，转导支持向量机，基于图的半监督学习算法。</li>
</ul>
</li>
<li>数据要求：数据的类别特征应该近似和它的聚类特征匹配；在实际应用中，当有标签的数据非常少时效率很高。</li>
</ul>
</li>
<li><strong>active learning</strong>：<ul>
<li>主动学习是半监督机器学习的一个特例，在主动学习中，一个学习算法可以主动地提出一些标注请求，将一些经过筛选的数据提交给专家进行标注。</li>
<li>两种查询系统：选择性取样和基于池的采样。</li>
<li>种类：基于异构的模型、基于性能的模型、基于代表的模型。</li>
</ul>
</li>
</ul>
<h2 id="Collaborative-Filtering-5-points"><a href="#Collaborative-Filtering-5-points" class="headerlink" title="Collaborative Filtering(5 points)"></a>Collaborative Filtering(5 points)</h2><blockquote>
<p>A merchant has an n*d ratings matrix D representing the preferences of n customers across d items. It is assumed that the matrix is sparse, and therefore each customer may have bought only a few items. Please provide one approach that the utilizes the rating matrix D to make recommendations to customers.</p>
</blockquote>
<p>解答：总的方法如下：</p>
<ul>
<li>基于邻居的方法：基于用户的评级相似性、基于商品的评级相似性</li>
<li>基于图的方法：</li>
<li>聚类方法：自适应k-means聚类，自适应协同聚类</li>
<li>潜伏因子模型：奇异值分解、矩阵分解、矩阵填充</li>
</ul>
<p>本题使用Matrix Completion方法：$\min_{X \in R^{n*d}} \Vert X \Vert_*, s.t. \ X_{ij} = D_{ij} \in \Omega$。对矩阵D进行低秩分解，用U<em>V来逼近M，用于填充，其中$D \in R^{m\</em>n}, U \in R{m*r}, V \in R^{r*n}$, 即$U*V^T$得到近似矩阵M来填充D上的缺失值。</p>
</div><script type="text/javascript" src="/js/share.js?v=1.0.0" async></script><a data-url="http://keyunluo.github.io/2016/12/27/2016-12-27-data-mining.html" data-id="cix8d8m9p005236gl9f3jia75" class="article-share-link">分享到</a><div class="tags"><a href="/tags/机器学习/">机器学习</a></div><div class="post-nav"><a href="/2016/12/23/2016-12-23-distributed-system-1.html" class="next">分布式系统——课程总结</a></div><div data-thread-key="2016/12/27/2016-12-27-data-mining.html" data-title="数据挖掘——试卷分析" data-url="http://keyunluo.github.io/2016/12/27/2016-12-27-data-mining.html" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2016/12/27/2016-12-27-data-mining.html" data-title="数据挖掘——试卷分析" data-url="http://keyunluo.github.io/2016/12/27/2016-12-27-data-mining.html" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-5"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"/><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/AdvancedAlgorithms/">AdvancedAlgorithms</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/AlgorithmApplication/">AlgorithmApplication</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/BigData/">BigData</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DataBase/">DataBase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DistributedSystem/">DistributedSystem</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/FileAndStorage/">FileAndStorage</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Life/">Life</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MachineLearning/">MachineLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming/">Programming</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/SourceCodeLearning/">SourceCodeLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tools/">Tools</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Latex/" style="font-size: 15px;">Latex</a> <a href="/tags/git/" style="font-size: 15px;">git</a> <a href="/tags/二次排序/" style="font-size: 15px;">二次排序</a> <a href="/tags/VPN/" style="font-size: 15px;">VPN</a> <a href="/tags/Oracle/" style="font-size: 15px;">Oracle</a> <a href="/tags/DataBase/" style="font-size: 15px;">DataBase</a> <a href="/tags/elasticsearch/" style="font-size: 15px;">elasticsearch</a> <a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/生活/" style="font-size: 15px;">生活</a> <a href="/tags/毕业/" style="font-size: 15px;">毕业</a> <a href="/tags/sqoop/" style="font-size: 15px;">sqoop</a> <a href="/tags/Markdown/" style="font-size: 15px;">Markdown</a> <a href="/tags/hadoop/" style="font-size: 15px;">hadoop</a> <a href="/tags/UML/" style="font-size: 15px;">UML</a> <a href="/tags/PlantUML/" style="font-size: 15px;">PlantUML</a> <a href="/tags/CentOS/" style="font-size: 15px;">CentOS</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/Cloudera/" style="font-size: 15px;">Cloudera</a> <a href="/tags/Hadoop/" style="font-size: 15px;">Hadoop</a> <a href="/tags/Java/" style="font-size: 15px;">Java</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/数据压缩/" style="font-size: 15px;">数据压缩</a> <a href="/tags/OpenCV/" style="font-size: 15px;">OpenCV</a> <a href="/tags/存储引擎/" style="font-size: 15px;">存储引擎</a> <a href="/tags/LSM树/" style="font-size: 15px;">LSM树</a> <a href="/tags/分布式系统/" style="font-size: 15px;">分布式系统</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/12/27/2016-12-27-data-mining.html">数据挖掘——试卷分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/23/2016-12-23-distributed-system-1.html">分布式系统——课程总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/22/2016-12-22-data-mining.html">数据挖掘——课程总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/16/2016-11-16-skiplist1.html">SkipList 跳跃表(1) ——基本介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/16/2016-11-16-lsm1.html">LSM Tree (1)</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/15/2016-11-15-hash.html">Hash存储引擎</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/03/2016-11-03-advanced-algorithm-assignment2.html">高级算法--作业2</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/13/2016-10-13-advanced-algorithm-assignment1.html">高级算法--作业1</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/06/2016-10-06-advanced-algorithm-3.html">高级算法(3)--Min-Cut-Max-Flow(2)-近似算法</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/05/2016-10-05-advanced-algorithm-2.html">高级算法(2)--Min-Cut-Max-Flow(1)-确定性算法</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 外部链接</i></div><ul></ul><a href="http://keyunluo.github.io/algorithm" title="个人算法练习" target="_blank">个人算法练习</a></div><div class="widget"><div class="widget-title"><i class="fa fa-bar-chart"> 访客</i></div><a href="http://info.flagcounter.com/uNxw"><img src="http://s07.flagcounter.com/count2/uNxw/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_3/labels_0/pageviews_0/flags_0/percent_0/" alt="" border="0"></a></div></div></div><div class="pure-u-1 pure-u-md-4-5"><div id="footer">© <a href="/." rel="nofollow">流光.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=1.0.0"><script>var duoshuoQuery = {short_name:'streamers'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script type="text/javascript" src="/js/search.js?v=1.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-76470846-1','auto');ga('send','pageview');
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?945b21807344d0260f24455bbd82dfea";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><div id="script" type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></div><script type="text/javascript" src="/js/mathjax/2.7-latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>