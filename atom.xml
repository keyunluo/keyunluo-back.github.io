<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>流光</title>
  <subtitle>他跑啊跑啊，只为追上那个曾经被寄予厚望的自己</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://keyunluo.github.io/"/>
  <updated>2017-07-06T08:08:06.889Z</updated>
  <id>http://keyunluo.github.io/</id>
  
  <author>
    <name>浮舟沧海</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>计算模型导引——递归函数论</title>
    <link href="http://keyunluo.github.io/2017/05/01/Course/computational-models-1.html"/>
    <id>http://keyunluo.github.io/2017/05/01/Course/computational-models-1.html</id>
    <published>2017-05-01T06:25:02.000Z</published>
    <updated>2017-07-06T08:08:06.889Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;2017年研究生课程——计算模型导引第一章课后习题(习题课整理)。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;知识点概要&quot;&gt;&lt;a href=&quot;#知识点概要&quot; class=&quot;headerlink&quot; title=&quot;知识点概要&quot;&gt;&lt;/a&gt;知识点概要&lt;/h2&gt;&lt;h3 id=&quot;数论函数&quot;&gt;&lt;a href=&quot;#数论函数&quot; class=&quot;headerlink&quot; title=&quot;数论函数&quot;&gt;&lt;/a&gt;数论函数&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;以下三类数论函数称为本原函数(&lt;strong&gt;Initial Function, IF&lt;/strong&gt;)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;零函数Z:$\mathbb{N} \rightarrow \mathbb{N}, \forall x \in \mathbb{N}.Z(x)=0$&lt;/li&gt;
&lt;li&gt;后继函数S:$\mathbb{N} \rightarrow \mathbb{N}, \forall x \in \mathbb{N}.S(x)=x+1$&lt;/li&gt;
&lt;li&gt;投影函数$P_i^n:\mathbb{N}^n \rightarrow \mathbb{N}, \forall x_1, x_2, \ldots, x_n \in \mathbb{N}. P_i^n(x_1, x_2, \ldots, x_n) = x_i$&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;基本函数(Basic Function, BF)&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{IF} \subseteq \mathcal{BF}$&lt;/li&gt;
&lt;li&gt;$\mathcal{BF}$对复合封闭，即对任意的$m,n \in \mathbb{N}^+, f:\mathbb{N}^m \rightarrow \mathbb{N}, g_1, g_2, \ldots, g_m:\mathbb{N}^n \rightarrow \mathbb{N}, \ 若 \ f, g_1, \ldots, g_m \in \mathcal{BF},\ 则 \ Comp_m^n[f,g_1, \ldots, g_m] \in \mathcal{BF} $&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;配对函数&quot;&gt;&lt;a href=&quot;#配对函数&quot; class=&quot;headerlink&quot; title=&quot;配对函数&quot;&gt;&lt;/a&gt;配对函数&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设$pg(x,y):\mathbb{N}^2 \rightarrow \mathbb{N}, K(x): \mathbb{N} \rightarrow \mathbb{N}, L(x):\mathbb{N} \rightarrow \mathbb{N}$为数论函数， 若它们对任意的$x,y \in \mathbb{N}$, 满足：$K(pg(x,y)) = x, L(pg(x,y)) = y$, 则称pg为配对函数， K和L分别称为左函数、右函数，{pg, K, L}称为配对组。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;初等函数&quot;&gt;&lt;a href=&quot;#初等函数&quot; class=&quot;headerlink&quot; title=&quot;初等函数&quot;&gt;&lt;/a&gt;初等函数&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;初等函数(Elementary Function)类$\mathcal{EF}$是满足以下条件的最小集：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{IF} \subseteq \mathcal{EF}$&lt;/li&gt;
&lt;li&gt;$x+y, x\ddot{-}y, x × y， \left \lfloor x/y \right \rfloor \in \mathcal{EF}$&lt;/li&gt;
&lt;li&gt;$\mathcal{EF}$对于复合、有界迭加算子$\Sigma[ \cdot ]$和有界迭乘算子$\prod[\cdot ]$封闭&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;原始递归函数&quot;&gt;&lt;a href=&quot;#原始递归函数&quot; class=&quot;headerlink&quot; title=&quot;原始递归函数&quot;&gt;&lt;/a&gt;原始递归函数&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;(1)设$n \in \mathbb{N}^+, f:\mathbb{N}^n \rightarrow \mathbb{N}, g:\mathbb{N}^{n+2} \rightarrow \mathbb{N}$, 定义函数$h: \mathbb{N}^{n+1} \rightarrow \mathbb{N}$如下：$h(\vec{x}, 0) = f(\vec{x}), h(\vec{x}, y+1) = g(\vec{x}, y, h(\vec{x}, y))$称h由f和g经带参原始递归算子$Prim^n[\cdot, \cdot]$而得，记作$h=Prim^n[f,g]$.
(2)设$a\in \mathbb{N}, g:\mathbb{N}^2 \rightarrow \mathbb{N},$ 定义函数$h:\mathbb{N} \rightarrow \mathbb{N}$如下：$h(0) = a, h(y+1) = g(y, h(y))$, 这时则称h由g经无参原始递归算子$Prim^0[\cdot, \cdot]$而得，记作$h=Prim^0[a,g]$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;原始递归函数(Primitive Recursive Functions, PRF)&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{IF} \subseteq \mathcal{BF}$&lt;/li&gt;
&lt;li&gt;$\mathcal{PRF}$对于复合、带参原始递归算子和无参原始递归算子封闭。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;递归函数&quot;&gt;&lt;a href=&quot;#递归函数&quot; class=&quot;headerlink&quot; title=&quot;递归函数&quot;&gt;&lt;/a&gt;递归函数&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;一般递归函数&lt;strong&gt;(General Recursive Functions,GRF)&lt;/strong&gt;为满足如下条件的最小集合：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{IF} \subseteq \mathcal{GRF}$&lt;/li&gt;
&lt;li&gt;$\mathcal{GRF}$对于复合和原始递归算子封闭&lt;/li&gt;
&lt;li&gt;$\mathcal{GRF}$对于正则$\mu-$算子封闭&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;习题&quot;&gt;&lt;a href=&quot;#习题&quot; class=&quot;headerlink&quot; title=&quot;习题&quot;&gt;&lt;/a&gt;习题&lt;/h2&gt;&lt;h3 id=&quot;1-1&quot;&gt;&lt;a href=&quot;#1-1&quot; class=&quot;headerlink&quot; title=&quot;1.1&quot;&gt;&lt;/a&gt;1.1&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;证明： 对于固定的$k$, 一元数论函数 $x+k \in \mathcal{BF}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：
$x + 0 = P_1^1 \in \mathcal{BF}, x + k = \underbrace{S ◦ S ◦ \ldots ◦ S}_{k-1 \ 次}  = S^{k-1}(x) \in \mathcal{BF}, \forall k &amp;gt; 1$, 故结论成立。&lt;/p&gt;
&lt;h3 id=&quot;1-2&quot;&gt;&lt;a href=&quot;#1-2&quot; class=&quot;headerlink&quot; title=&quot;1.2&quot;&gt;&lt;/a&gt;1.2&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;证明：对任意 $k \in \mathbb{N}^{+}$$，$$f: \mathbb{N}^k \rightarrow \mathbb{N}$，若 $f \in \mathcal{BF}$，则存在 $h$，使得 $f(\vec{x}) &amp;lt; |\vec{x}|+h$
其中$|\vec{x}| = \max{x_i; 1\leq i \leq k}.$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：设$f \in \mathcal{BF}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;case 1：如果 $f$ 为零函数 $Z$，后继函数 $S$，或投影函数$P_i^n$ 之一，
$Z(x) &amp;lt; x+1, \quad S(x)&amp;lt;x+2, \quad P_i^n(\vec{x})&amp;lt;|\vec{x}|+1$
显然存在这样的 $h$；&lt;/li&gt;
&lt;li&gt;case 2：设 $f(\vec{x}) = g(g_1(\vec{x}), g_2(\vec{x}), \cdots, g_m(\vec{x}))$
设 $g(\vec{y}) &amp;lt; |\vec{y}| + h_0，g_i(\vec{x_i}) &amp;lt; |\vec{x}|+h_i\ (i = 1,2,3, \cdots, m)$
从而$f(\vec{x}) &amp;lt; \max_{1\leq i \leq m} g_i(\vec{x}) + h_0&amp;lt;\max_{1\leq i \leq m}(|\vec{x}|+ h_i) + h_0 &amp;lt; |\vec{x}|+ h_0+h_1+ \cdots + h_m$
取 $h = h_0+h_1+ \cdots + h_m$ 即可。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;1-3&quot;&gt;&lt;a href=&quot;#1-3&quot; class=&quot;headerlink&quot; title=&quot;1.3&quot;&gt;&lt;/a&gt;1.3&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;证明：二元数论函数 $x+y \notin \mathcal{BF}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答： 假设$ x+y \in \mathcal{BF}$, 根据上述习题1.2， 从而有h使得$x + y &amp;lt; \max (x,y) + h$, 令$x=y=h+1$, 得到$2h+2 &amp;lt; 2h+1$矛盾， 故假设不成立。&lt;/p&gt;
&lt;h3 id=&quot;1-4&quot;&gt;&lt;a href=&quot;#1-4&quot; class=&quot;headerlink&quot; title=&quot;1.4&quot;&gt;&lt;/a&gt;1.4&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;证明：二元数论函数 $x \dot{-}y \notin \mathcal{BF}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答： 假设$x \dot{-} y \in \mathcal{BF}$, 从而$x\dot{-}1 \in \mathcal{BF}$， 然而$f(x) \in \mathcal{BF}\ 时，f(x)\geq 0 \ 或 \ =0$，得出矛盾！&lt;/p&gt;
&lt;h3 id=&quot;1-5&quot;&gt;&lt;a href=&quot;#1-5&quot; class=&quot;headerlink&quot; title=&quot;1.5&quot;&gt;&lt;/a&gt;1.5&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设 $pg(x, y) = 2^x(2y+1)\dot{-}1$，证明：存在初等函数 $K(z)$ 和 $ L(z)$ 使得
$$
K(pg(x,y))=x, L(pg(x,y))=y, pg(K(x), L(y)) = z.
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：$z = 2^x(2y+1) \dot{-}1,\ iff\  z+1 = 2^x(2y+1), \ iff\ x = ep_0(z+1), 且 \ 2y +1 = \frac{z+1}{2^x}$, 带人x, 得$2y+1 = \left[\frac{z+1}{2^{\text{ep}_0(z+1)}}\right], \ iff \ x = K(z), 且 \ y=L(z)$, 这里$K(z) = ep_0(z+1), L(z) = \left[\frac{\left[\frac{z+1}{2^{\text{ep}_0(z+1)}}\right]\dot{-}1}{2}\right]$&lt;/p&gt;
&lt;h3 id=&quot;1-6&quot;&gt;&lt;a href=&quot;#1-6&quot; class=&quot;headerlink&quot; title=&quot;1.6&quot;&gt;&lt;/a&gt;1.6&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设 $f: \mathbb{N} \rightarrow \mathbb{N}$，证明：$f$ 可以作为配对函数的左函数当且仅当对任何 $i \in \mathbb{N}$，
$|\ {\ x\in \mathbb{N}: f(x)=i\ }\ | = \aleph_0.$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“$\Rightarrow$”，设 $f$ 为配对函数 $pg(x,y)$ 的左函数
$\because f(pg(i, j)) = i,  \forall j$
从而对任何 $i \in \mathbb{N}$，$\therefore {\ x\ |\ f(x) = i\ } \supseteq {\ pg(i,j) \ |\  j \in \mathbb{N}\ } \sim {\ j \ | \ j \in \mathbb{N}\ } \sim \mathbb{N}$
因此，${\ x\ |\ f(x) = i\ }$ 无穷。&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;“$\Leftarrow$”，设任何 $i \in \mathbb{N}$，$f^{-1}[{i}]$ 无穷，
$\because \mathbb{N}$ 良序，
$\therefore$ 可设 $f^{-1}[{i}] = {\ a_{ij}\  |\ j \in \mathbb{N}\ }$
$g: \mathbb{N} \rightarrow \mathbb{N}$ 定义如下：&lt;/p&gt;
&lt;p&gt;$$
g(x) =
\begin{cases}
j, &amp;amp; \text{if } x = a_{ij} \\
0, &amp;amp; \text{else}
\end{cases}
$$
从而对任何 $i,j \in \mathbb{N}$，$f(a_{ij}) = i$，$g(a_{ij}) = j$，&lt;/p&gt;
&lt;p&gt;令 $pg(i, j) = a_{ij}$，$f$ 即为左函数。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;1-7&quot;&gt;&lt;a href=&quot;#1-7&quot; class=&quot;headerlink&quot; title=&quot;1.7&quot;&gt;&lt;/a&gt;1.7&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;证明：从本原函数出发，经复合和算子 $\prod\limits_{i=n}^{m}[\cdot]$ 可以生成所有的初等函数，这里&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$$
\prod\limits_{i=n}^{m}[f(\vec{i})] =
\begin{cases}
f(n) \cdot f(n-1) \cdot\ \cdots \ \cdot f(m), &amp;amp; \text{if } m\geq n\\
1, &amp;amp; \text{if } m &amp;lt; n
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;解答：&lt;/p&gt;
&lt;p&gt;只需证 $\sum\limits_{i=0}^n$ ， $\prod\limits_{i=0}^n$ 和函数 $\ddot{-}$ 可表示出。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\prod\limits_{i=0}^n$ 为 $\prod\limits_{i=n}^{m}$ 的特例，取 $n=0$ 即可；&lt;/li&gt;
&lt;li&gt;$x^y = \prod\limits_{i=1}^{y}P_1^1(x)$，从而 $2^y = \prod\limits_{i=1}^{y}SSZ(i)$；&lt;/li&gt;
&lt;li&gt;$N(x) = \prod\limits_{i=1}^{x}Z(i)$ ；&lt;/li&gt;
&lt;li&gt;$\text{leq}(x,y) = \prod\limits_{i=x}^{y}Z(i) = \begin{cases}0, &amp;amp; \text{if } x \leq y \\
1, &amp;amp; \text{if } x &amp;gt; y\end{cases}$&lt;/li&gt;
&lt;li&gt;$\text{geq}(x, y) = \prod\limits_{i=y}^{x} Z(i)= \begin{cases}0, &amp;amp; \text{if } x \geq y \\
1, &amp;amp; \text{if } x &amp;lt; y
\end{cases}$&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$\text{eq}(x,y) = \text{leq}(x,y)^{N\text{geq}(x,y)}=\begin{cases}0, &amp;amp; \text{if } x = y \\
1, &amp;amp; \text{if } x \neq y
\end{cases}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$\log(x) = \prod\limits_{i=0}^x i^{N \text{ eq}(2^i,x)}$
注意：$\log(2^y) = \prod\limits_{i=0}^{2^y}i^{N\text{ eq}(2^i, 2^y)}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;$\sum\limits_{i=n}^{m} f(i, \vec{x}) = \log(2^{\sum_{i=n}^mf(i, \vec{x})}) = \log(\prod\limits_{i=n}^{m}2^{f(i, \vec{x})})$&lt;/li&gt;
&lt;li&gt;$x \cdot y = \sum\limits_{i=1}^{x}P_1^1(y)$&lt;/li&gt;
&lt;li&gt;$x+y = \log(2^x \cdot 2^y)$&lt;/li&gt;
&lt;li&gt;$x \ddot{-} y = \left(\sum\limits_{i=y+1}^{x}SZ(i)\right) + \left(\sum\limits_{i=x+1}^{y}SZ(i)\right)$&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;1-8&quot;&gt;&lt;a href=&quot;#1-8&quot; class=&quot;headerlink&quot; title=&quot;1.8&quot;&gt;&lt;/a&gt;1.8&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设
$$
M(x) =
\begin{cases}
M(M(x+11)), &amp;amp; \text{if } x \leq 100, \\
x-10, &amp;amp; \text{if } x&amp;gt;100,
\end{cases}
$$
证明：
$$
M(x) =
\begin{cases}
91, &amp;amp; \text{if } x \leq 100, \\
x-10, &amp;amp; \text{else.}
\end{cases}
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：
只需证，当 $0\leq x \leq 100$$ 时，$$M(x) = 91$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$M(90) = M(M(101))=M(91)=M(92) = \cdots = M(100) = M(M(111))=M(101)=91$；&lt;/li&gt;
&lt;li&gt;当 $0\leq x \leq 100$ 时，存在 $k$ 使得 $90 \leq x+11k \leq 100$，从而 $M(x) = M^2(x+1*11)= M^{k+1}(x+11k) = M^kM(x+11k)=M^k(91) = 91$。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;1-9&quot;&gt;&lt;a href=&quot;#1-9&quot; class=&quot;headerlink&quot; title=&quot;1.9&quot;&gt;&lt;/a&gt;1.9&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;证明：
$$
\min x \leq n. [f(x, \vec{y})] = n \dot{-}\max x \leq n. [f(n \dot{-}x, \vec{y})], \\
\max x \leq n. [f(x, \vec{y})] = n \dot{-}\min x \leq n. [f(n \dot{-}x, \vec{y})].
$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;case 1: $f(x)\ 在 \ [0, n]$ 中有零点。
设 $\min{x\leq n.}[f(x)] = k$，故 $f(k) = 0$，从而 $f(n \dot{-}(n\dot{-}k)) = 0$，$n-k$ 为 $g(x) = f(n\dot{-}x)$ 的零点。
当 $k$ 为 $f(x)$ 的最小零点时，$n \dot{-} k$ 为 $g(x)$ 的最大零点，从而$n \dot{-} k = \max{x \leq n}. [f(n\dot{-}x)]$
因此，$k = n \dot{-} \max{x\leq n}. [f(n \dot{-}x)]&lt;/li&gt;
&lt;li&gt;case 2: $f(x)$ 在 $[0, n]$ 中无零点，等式左边$=n$，右边$=n\dot{-}0=n$，相等。
同理，可证 $\max x \leq n. [f(x, \vec{y})] = n \dot{-}\min x \leq n. [f(n \dot{-}x, \vec{y})].$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;1-10&quot;&gt;&lt;a href=&quot;#1-10&quot; class=&quot;headerlink&quot; title=&quot;1.10&quot;&gt;&lt;/a&gt;1.10&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;证明：$\mathcal{EF}$ 对有界 $\max$-算子封闭.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：
$\because \max x \leq n. f(x, \vec{y}) = n \dot{-} \min x\leq n. f(n \dot{-}x, \vec{y})$ (习题 1.9)
$\therefore \mathcal{EF}$ 对有界 $\max$-算子封闭.&lt;/p&gt;
&lt;h3 id=&quot;1-11&quot;&gt;&lt;a href=&quot;#1-11&quot; class=&quot;headerlink&quot; title=&quot;1.11&quot;&gt;&lt;/a&gt;1.11&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;Euler 函数 $\varphi : \mathbb{N} \rightarrow \mathbb{N}$ 定义为
$\varphi(n) = |{\ x: 0&amp;lt;x\leq n \wedge \gcd(x,n) = 1\ }|,$
即 $\varphi(n)$ 表示小于等于$n$ 且与 $n$ 互素的正整数个数，例如 $\varphi(1)=1$，因为 1 与其本身互素；$\varphi(9) = 6$，因为 9 与 1, 2, 4, 5, 7, 8 互素。证明：$\varphi \in \mathcal{EF}$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：
我们有 $\varphi(n) = n \prod\limits_{p\ \vert \ n}(1- \frac{1}{p})$
$\because \prod\limits_{p\ \vert \ n} p = \prod\limits_{i=0}^n\ \text{ IF } \  P_i|n \ \text{ THEN }\ P_i \ \text{ ELSE }\ 1$
$=\prod\limits_{i=0}^n\ \text{ IF } \  \text{ep}_i \geq 1 \ \text{ THEN }\ P_i \ \text{ ELSE }\ 1$
$=\prod\limits_{i=0}^n(P_iN(1\dot{-} \text{ep}_i(n))+N^2(1\dot{-}\text{ep}_i(n))) \in \mathcal{EF}$
同理，$\prod\limits_{p\ \vert \ n} (p-1) =\prod\limits_{i=0}^n((P_i-1)N(1\dot{-} \text{ep}_i(n))+N^2(1\dot{-}\text{ep}_i(n))) \in \mathcal{EF} $
$\therefore \varphi(n) = N(n\dot{-}1)+N^2(n \dot{-}1)\left[\dfrac{\prod\limits_{p\ \vert \ n} (p-1)}{\prod\limits_{p\ \vert \ n} p}\right] \in \mathcal{EF}$.&lt;/p&gt;
&lt;h3 id=&quot;1-12&quot;&gt;&lt;a href=&quot;#1-12&quot; class=&quot;headerlink&quot; title=&quot;1.12&quot;&gt;&lt;/a&gt;1.12&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设 $h(x)$ 为 $x$ 的最大素因子的下标，约定 $h(0)=0, h(1) = 0$。例如 $h(88) = 4$，因为 $88=2^3 \times 11$ 的最大素因子 11 是第 4 个素数 $p_4$，其下标为 4。证明：$h \in \mathcal{EF}$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：$h(0)=h(1) = 0$，当 $x \geq 2$ 时：&lt;/p&gt;
&lt;p&gt;$\because h(x) = \max y\leq x. [\text{ep}_y{x} \geq 1]$
$h(x) = \max y \leq x. [1 \dot{-}\text{ep}_y(x)]$
$\therefore h \in \mathcal{EF}$&lt;/p&gt;
&lt;h3 id=&quot;1-13&quot;&gt;&lt;a href=&quot;#1-13&quot; class=&quot;headerlink&quot; title=&quot;1.13&quot;&gt;&lt;/a&gt;1.13&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设 $f:\mathbb{N} \rightarrow \mathbb{N}$ 满足$f(0) = 1, f(1) = 1,f(x+2) =f(x)+f(x+1)$
证明：
(1) $f \in \mathcal{PRF}$；
(2) $f \in \mathcal{EF}$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：
(1) 令$[a_0, a_1, \cdots, a_n] \prod\limits_{i=0}^{n} p_i^{a_i}$
这里 $p_i$ 为第 $i$ 个素数，比如$p_0=2，p_1=3，p_2=5$，那么 $[a_0, a_1, a_2] = 2^{a_0} \cdot 3^{a_1} \cdot 5^{a_2}$，为 Gödel 编码形式$ep_i(a) = a$ 的素因子分解式中第 $i$ 个素数的指数。易见
$$
ep_i[a_0, a_1, \cdots, a_n] =
\begin{cases}
 a_i, &amp;amp; i\leq n \\
0, &amp;amp; i&amp;gt;n
\end{cases}
$$
从而令 $F(n) = [f(n), f(n+1)]，F(0) = [f(0), f(1)] = 2^1 \cdot 3^1 = 6$
$$
\begin{array}{rl} F(n+1) &amp;amp; = [f(n+1), f(n+2)]\\
&amp;amp; = [f(n+1), f(n+1)+f(n)] \\
&amp;amp; = [ep_1F(n), ep_1F(n)+ep_0F(n)] \\
&amp;amp; = H(F(n))\end{array}
$$
这里，$$H(x) = [ep_1x, ep_1x+ep_0x] = 2^{ep_1x} \cdot 3^{ep_1x} \cdot 3^{ep_0x}$$&lt;/p&gt;
&lt;p&gt;$\therefore H(x)$ 是初等的。
又 $\because F(0)=6, F(n+1) = H(F(n))$ ，以及 $H$ 为原始递归函数
$\therefore F(n)$ 为原始递归函数
又 $\because f(n)=ep_0F(x)$
$\therefore f(n)$ 为原始递归函数。&lt;/p&gt;
&lt;p&gt;(2) 现在证明 $f(n)$ 是初等的。&lt;/p&gt;
&lt;p&gt;&lt;u&gt;&lt;strong&gt;证法一：&lt;/strong&gt;&lt;/u&gt;&lt;/p&gt;
&lt;p&gt;首先有 $f(n) \leq 2^n$ ，归纳证明如下：
当 $n=0,1$ 时，$f(0)=1 \leq 2^0 ，f(1) = 1 \leq 2^1$
归纳假设：$\forall k \leq n, f(k) \leq 2^k$
归纳步骤：当 $n&amp;lt;2$ 时，$f(n) \leq 2^n$ 为真，当 $ n \geq 2$ 时，
$f(n) = f(n-1)+f(n-2) \leq 2^{n-1} +2^{n-2} \leq 2^{n-2} \cdot 3 \leq 2^{n-2} \cdot 4 \leq 2^n$&lt;/p&gt;
&lt;p&gt;其次，还有 $F(n) \leq G(n)$ ，这里 $G(n) = 2^{2^n}\cdot 3^{2^{n+1}}$，且 $G(n)$ 是初等的。&lt;/p&gt;
&lt;p&gt;$F(n) = [f(n), f(n+1)] = 2^{f(n)} \cdot 3^{f(n+1)} \leq  2^{2^n}\cdot 3^{2^{n+1}} = G(n)$，易见 $G(n)$ 的初等性。&lt;/p&gt;
&lt;p&gt;令$\alpha(n) = [F(0), \cdots, F(n)] \leq [G(0), \cdots, G(n)]$，有
$\alpha(n) \leq \prod\limits_{i=0}^np_i^{G(i)} \leq \prod\limits_{i=0}^np_n^{(2^{2^n}\cdot 3^{2^{n+1}} \cdot (n+1))} = \beta(n)
$
易见，$\beta(n)$ 为初等函数。
因为
$$
\begin{array}{rl}\alpha(n)  &amp;amp; = \mu x \leq \beta(n) \cdot (ep0x = F(0)) \wedge \forall i&amp;lt;n, ep{i+1}x = H(ep_ix)  \
 &amp;amp; = \mu x \leq \beta(n) \cdot[ep_0x=6 \wedge \forall i &amp;lt;n , (ep_{i+1}x \ddot{-}H(ep_ix)=0)] \
  &amp;amp;= \mu x \leq \beta(n) \cdot\left[ep_0x=6 + \sum_{i \rightarrow n \ddot{-}1}(ep_{i+1}x\ddot{-}H(ep_ix))N^2n\right] \
   &amp;amp; = \mu x \leq \beta(n) \cdot \gamma(n)
\end{array}
$$
易见，$\gamma(n)$ 是初等的，所以 $\alpha(n)$ 是初等的。
因为 $f(n) = ep_0(F(n)) = ep_0(ep_n(\alpha(n)))$，所以 $f(n)$ 是初等的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;u&gt;证法二：&lt;/u&gt;&lt;/strong&gt;
$f(n) = \frac{1}{\sqrt{5}}\left(\frac{1+\sqrt{5}}{2}\right)^{n+1} - \frac{1}{\sqrt{5}}\left(\frac{1-\sqrt{5}}{2}\right)^{n+1}
$
（该公式的证明参见 Kolman B, Busby R C. Ross S. Discrete mathematical structures. Prentice-Hall, Inc. 1996 (3rd) ）&lt;/p&gt;
&lt;p&gt;从而
$$
\begin{array}{rl}f(n) &amp;amp; = \frac{1}{2^{n+1}\sqrt5}\left[(1+\sqrt 5)^{n+1} - (1-\sqrt 5)^{n+1}\right] \\
&amp;amp; = \frac{1}{2^{n+1}\sqrt5}\left[\sum_{i=0}^{n+1}C_{n+1}^i (\sqrt 5)^i - \sum_{i=0}^{n+1}C_{n+1}^i (-\sqrt 5)^i \right] \\
&amp;amp; = \frac{1}{2^{n+1}\sqrt5}\left[\sum_{i=0}^{n+1}C_{n+1}^i 2(\sqrt 5)^i N^2rs(i,2) \right] \\
&amp;amp; = \frac{1}{2^{n}}\left[\sum_{i=0}^{n+1}C_{n+1}^i 5^{\lfloor\frac{i}{2}\rfloor} N^2rs(i,2) \right] \\
\end{array}
$$&lt;/p&gt;
&lt;h3 id=&quot;1-14&quot;&gt;&lt;a href=&quot;#1-14&quot; class=&quot;headerlink&quot; title=&quot;1.14&quot;&gt;&lt;/a&gt;1.14&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设数论谓词 $Q(x,y,z,v)$ 定义为
$Q(x, y, z, v) \equiv p(\langle x,y,z\rangle) ~|~ v,$其中，$p(n)$ 表示第 $n$ 个素数，$\langle x,y,z\rangle$ 是 $x,y,z$ 的 Godel 编码。证明：$Q(x,y,z,v)$ 是初等的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：$\because \langle x,y,z\rangle= 2^x \cdot 3^y \cdot 5^z \in \mathcal{EF} $
又 $Q(x,y,z,v)$ 的特征函数为 $N^2rs(v, p(\langle x,y,z\rangle))$
$\therefore Q \in \mathcal{EF}$&lt;/p&gt;
&lt;h3 id=&quot;1-15&quot;&gt;&lt;a href=&quot;#1-15&quot; class=&quot;headerlink&quot; title=&quot;1.15&quot;&gt;&lt;/a&gt;1.15&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设 $f:\mathbb{N} \rightarrow \mathbb{N}$ 满足
$f(0) = 1,f(1) = 4,f(2)=6, f(x+3) = f(x)+(f(x+1))^2+(f(x+2))^3$
证明：$f(x) \in \mathcal{PRF}$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：
令 $g(x) = \langle f(x), f(x+1), f(x+2) \rangle, f(x) = (g(x))_1$
$g(0) = \langle 1, 4, 6 \rangle$，
$g(x+1) = \langle (g(x))_2, (g(x))_3, (g(x))_1+(g(x))_2^2+(g(x))_3^3 \rangle = B(g(x)) $
这里，$B(z) = \langle (z)_2, (z)_3, (z)_1+((z)_2)^2+((z)_3)^3 \rangle \in \mathcal{PRF}$
故 $g \in \mathcal{PRF}$，从而 $f \in \mathcal{PRF}$。&lt;/p&gt;
&lt;h3 id=&quot;1-16&quot;&gt;&lt;a href=&quot;#1-16&quot; class=&quot;headerlink&quot; title=&quot;1.16&quot;&gt;&lt;/a&gt;1.16&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设 $f: \mathbb{N}\rightarrow \mathbb{N}$ 满足
$f(0)=0,f(1) = 1,f(2) = 2^2,f(3) = 3^{3^3} \cdots \cdots,
f(n) = n^{.^{.^{.^n}}} (\text{the number of } n \text{ is } n) $
证明：$f \in \mathcal{PRF} - \mathcal{EF}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：&lt;/p&gt;
&lt;p&gt;令 $g(m,n) = n^{.^{.^{.^n}}}$，具有 $m$ 个 $n$ 的形式，
$$
\begin{cases}
 g(0, n) = N^2n \\
 g(m+1, n) = n^{g(m,n)}
 \end{cases}
 $$，从而 $f(n) = g(n, n)$
$\because g \in \mathcal{PRF}$
$\therefore f \in \mathcal{PRF}$
以下证 $ f \notin \mathcal{EF}$
从而 $\exists k ~\forall n, f(n) &amp;lt; 2^{.^{.^{.^{2^n}}}} } k$ 个 2，取 $n = k+2$，从而
$(k+2)^{.^{.^{.^{(k+2)}}}} }k+2$ 个 $&amp;lt; 2^{.^{.^{.^{2^{(k+2)}}}}} }k$ 个 2，矛盾。&lt;/p&gt;
&lt;h3 id=&quot;1-17&quot;&gt;&lt;a href=&quot;#1-17&quot; class=&quot;headerlink&quot; title=&quot;1.17&quot;&gt;&lt;/a&gt;1.17&lt;/h3&gt;&lt;p&gt;设 $g: \mathbb{N}\rightarrow \mathbb{N} \in \mathcal{PRF}, f: \mathbb{N}^2 \rightarrow \mathbb{N}$ ，满足
$
f(x, 0)=g(x)， f(x,y+1) = f(f(\cdots f(f(x,y), y-1), \cdots),0),$
证明：$f \in \mathcal{PRF}$&lt;/p&gt;
&lt;p&gt;解答：
证明 $f(x,y)$ 呈形 $g^{a(y)}(x)$
Basis: $ y = 0, f(x,y) = f(x,0) = g(x), a(0) = 1$
假设 $f(x,y) = g^{a(y)}(x)$，那么，
$$
\begin{array}{rl}
f(x, y+1) &amp;amp;=f(f(\cdots f(f(x,y), y-1), \cdots), 0) \\
&amp;amp; = g^{a(0)}(f(\cdots f(f(x,y), y-1), \cdots), 1) \\
&amp;amp; = g^{a(0)+a(1)}(f(\cdots f(f(x,y), y-1), \cdots), 2) \\
&amp;amp; = g^{a(0)+a(1)+ \cdots a(n)}(x)
\end{array}
$$
从而，$a(0)=1, a(y+1) = a(0)+a(1)+\cdots +a(y)$
易见 $a(y) \in \mathcal{PRF}$，故 $f(x,y) = g^{a(y)} (x)$
令 $h(x,y) = g^y(x)$，因为
$$
\begin{cases}
h(x, 0) = x \\
h(x, y+1) = g(h(x,y))
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;$\therefore h \in \mathcal{PRF}$
$\because f(x,y)= h(x, a(y))$
$\therefore f \in \mathcal{PRF}$&lt;/p&gt;
&lt;h3 id=&quot;1-18&quot;&gt;&lt;a href=&quot;#1-18&quot; class=&quot;headerlink&quot; title=&quot;1.18&quot;&gt;&lt;/a&gt;1.18&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设 $k \in \mathbb{N}^+$，函数 $f: \mathbb{N}^ k\rightarrow \mathbb{N}$ 和 $g: \mathbb{N}^k \rightarrow \mathbb{N}$尽在有穷个点取不同值，证明：$f $ 为递归函数当且仅当 $g$ 为递归函数。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：
设 $f$ 与 $g$ 在有穷个点取不同值，从而有 $k \in \mathbb{N}$，使当 $x &amp;gt; k$ 时，$f(x) =g(x)$ ，从而，&lt;/p&gt;
&lt;p&gt;$$
f(x) =
\begin{cases}
f(x), &amp;amp; \text{if } x \leq k\\
g(x), &amp;amp; \text{if } x &amp;gt;k
\end{cases}
$$
令
$$
f^\prime(x) =
\begin{cases}
f(x), &amp;amp; \text{if }x \leq k\\
0, &amp;amp;\text{if }x &amp;gt; k
\end{cases}
$$
易见，$f^\prime \in \mathcal{PRF}$
$f(x) = f^\prime(x)N(x\dot{-}k) + g(x)N^2(x\dot{-}k)$
易见，$g \in \mathcal{PRF} \Rightarrow f \in \mathcal{PRF}$
同理，$f \in \mathcal{PRF} \Rightarrow g\in \mathcal{PRF}$&lt;/p&gt;
&lt;h3 id=&quot;1-19&quot;&gt;&lt;a href=&quot;#1-19&quot; class=&quot;headerlink&quot; title=&quot;1.19&quot;&gt;&lt;/a&gt;1.19&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;证明：$f(n) = \Bigl\lfloor\left(\frac{\sqrt{5}+1}{2}\right)n\Bigr\rfloor$
为初等函数。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：
$f(n) = \max x \leq 2n. x \leq \frac{\sqrt{5}+1}{2}n$
$f(n) = \max x \leq 2n. 2x \leq \sqrt{5} n+n$
$f(n) = \max x \leq 2n. 2x \ddot{-}n \leq \sqrt{5}n$
$f(n) = \max x \leq 2n. (2x \ddot{-}n)^2 \leq 5n^2$
$f(n) = \max x \leq 2n. 4x^2  \leq 4n^2 + 4xn$
$f(n) = \max x \leq 2n. x^2 \dot{-} (n^2 + xn) \in \mathcal{EF}$&lt;/p&gt;
&lt;h3 id=&quot;1-20&quot;&gt;&lt;a href=&quot;#1-20&quot; class=&quot;headerlink&quot; title=&quot;1.20&quot;&gt;&lt;/a&gt;1.20&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;证明：$Ack(4,n) \in \mathcal{PRF} - \mathcal{EF}$，其中 $Ack(x,y)$ 是 Ackermann 函数。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：
令 $f(n) = Ack(4,n)$，
$f(0) = Ack(4,0) = Ack(3,1) = f_3(1)=2^{1+3}-3=13$
$f(n+1) = Ack(4, n+1) = Ack(3, A(4,n)) = Ack(3, f(n)) = 2^{f(n)+3}\dot{-}3$
令 $g(n) = f(n)+3$，从而$g(0) = 16 = 2^4$，$g(n+1) = g(n)$&lt;/p&gt;
&lt;p&gt;因此，$g(n) = \left . 2^{\cdot^{\cdot^{\cdot^2}}}  \right \} n+3 $ 个2.&lt;/p&gt;
&lt;p&gt;从而，$Ack(4,n) = 2^{\cdot^{\cdot^{\cdot^2}}} \dot{-}3 \in \mathcal{PRF}$&lt;/p&gt;
&lt;p&gt;又因为：
$\lim \limits_{n \rightarrow \infty} \frac{\left. 2^{\cdot^{\cdot^{\cdot^2}}}  \right\} k \text{ 个 }2}{g(n)} = 0$&lt;/p&gt;
&lt;p&gt;$\therefore g \notin \mathcal{EF}$，$\therefore f \notin \mathcal{EF}$ 。&lt;/p&gt;
&lt;h3 id=&quot;1-21&quot;&gt;&lt;a href=&quot;#1-21&quot; class=&quot;headerlink&quot; title=&quot;1.21&quot;&gt;&lt;/a&gt;1.21&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设 $f: \mathbb{N} \rightarrow \mathbb{N}$，$f$ 为单射（1-1）且满射（onto），证明：$f\in \mathcal{GRF} \Leftrightarrow f^{-1} \in \mathcal{GRF}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：
设 $f$ 为单射且满射，令 $g(x) =f^{-1}(x)$，从而
$y=g(x) \text{ iff } f(y) = x \text{ iff } y = \mu z. f(z) = x$
从而，$g(x) = \mu z. (f(z) \ddot{-}z)$
因此，$g \in \mathcal{GRF} \Leftarrow f \in \mathcal{GRF}$
同理，$f\in \mathcal{GRF} \Leftarrow g \in \mathcal{GRF}$&lt;/p&gt;
&lt;h3 id=&quot;1-22&quot;&gt;&lt;a href=&quot;#1-22&quot; class=&quot;headerlink&quot; title=&quot;1.22&quot;&gt;&lt;/a&gt;1.22&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设 $p(x)$ 为整系数多项式，令 $f: \mathbb{N} \rightarrow \mathbb{N}$ 定义为 $f(a) = p(x)-a$ 对于 $x$ 的非负整数根，证明：$f \in \mathcal{RF}$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：
$f(n) = \mu x. (p(x) = n), \because p(x)$ 为整系数多项式, $\therefore$ 存在正整系数多项式 $s(x)$ 与 $t(x)$ 使 $p(x) = s(x)-t(x)$,从而$f(n) = \mu x. (s(x) \ddot{-} (n+t(x)))$. 易见，$s(x), t(x) \in \mathcal{PRF}$，故 $f \in \mathcal{RF}$。&lt;/p&gt;
&lt;h3 id=&quot;1-23&quot;&gt;&lt;a href=&quot;#1-23&quot; class=&quot;headerlink&quot; title=&quot;1.23&quot;&gt;&lt;/a&gt;1.23&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设
$$
f(x) =
\begin{cases}
x / y, &amp;amp; \text{if } y \neq 0 \text{ and } y~|~x,\\
\uparrow, &amp;amp; \text{else.}
\end{cases}
$$
证明：$f \in \mathcal{RF}$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：$f(x) = \mu z. (zy =x) \text{ and } (y \neq 0) = \mu z.(x \ddot{-}zy)\cdot Ny$&lt;/p&gt;
&lt;h3 id=&quot;1-24&quot;&gt;&lt;a href=&quot;#1-24&quot; class=&quot;headerlink&quot; title=&quot;1.24&quot;&gt;&lt;/a&gt;1.24&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设 $g: \mathbb{N} \rightarrow \mathbb{N}$ 满足
$g(0)=0,g(1) = 1,g(n+2) = rs((2002g(n+1)+2003g(n)), 2005)$
(1) 试求 $g(2006)$，
(2) 证明：$g \in \mathcal{EF}$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：&lt;/p&gt;
&lt;p&gt;先证明(2)，令 $h: \mathbb{N} \rightarrow \mathbb{Z}$ 如下：
$h(0) = 0,h(1) = 1,h(n+2) = -3h(n+1)-2h(n)$
易见 $g(n) = rs(h(n), 2005)$
(注：为何这样构造？$g(n+2) = rs((2005-3)g(n+1)+(2005-2)g(n), 2005) $)&lt;/p&gt;
&lt;p&gt;这是因为 $h(n)$ 的特征方程 $\lambda^2= -3\lambda-2$，其根为 $-1, -2$，从而 $h(n)$ 呈形 $a(-1)^n+b(-2)^n$，由 $h(0)=0, h(1) = 1$，故得 $a = 1, b = -1$，因此 $h(n) = (-1)^n-(-2)^n = (-1)^{n+1}(2^n \dot{-}1)$&lt;/p&gt;
&lt;p&gt;从而，&lt;/p&gt;
&lt;p&gt;$$g(n) =
\begin{cases}
2005 \dot{-} rs(2^n \dot{-}1, 2005), &amp;amp; \text{if } n \text{ is even,}\\
rs(2^n \dot{-}1, 2005), &amp;amp; \text{if } n \text{ is odd.}
\end{cases}$$&lt;/p&gt;
&lt;p&gt;$g(n) = (2005 \dot{-}rs(2^n\dot{-}1, 2005) Nrs(n,2)) + rs(2^n\dot{-}1, 2005)N^2rs(n,2)$
故 $g \in \mathcal{EF}$。
再计算(1)，$2005 = 5 \times 401$，$5, 401$ 均为素数&lt;/p&gt;
&lt;p&gt;由Fermat’s little theorem（费马小定理：假如 $p$ 是素数，且 $(a,p) =1$，那么 $a^{p-1} \equiv 1 (\text{mod } p)$)知，
$2^4 \equiv 1 \mod 5, 2^{400} \equiv 1 \mod 401$
$\because (5, 401) = 1, \therefore 2^{400} \equiv 1(\text{mod }5 \times 401)$$，即 $$rs(2^{400}, 2005) = 1$
从而，$g(n+400) = g(n)$，$g$ 的周期为 $400$
故 $g(2006) = g(6) = rs(h(6), 2005)) = 2005 \dot{-} rs(2^6-1, 2005) = 1942$。&lt;/p&gt;
&lt;h3 id=&quot;1-25&quot;&gt;&lt;a href=&quot;#1-25&quot; class=&quot;headerlink&quot; title=&quot;1.25&quot;&gt;&lt;/a&gt;1.25&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;设 $f: \mathbb{N} \rightarrow \mathbb{N}$ 定义为
$f(n)=\pi \text{ 的十进制展开式中第 }n\text{ 位数字}$
例如 $f(0) = 3, f(1) = 1, f(2) = 4$。证明：$f \in \mathcal{GRF}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：
首先，我们有$\frac{1}{1+x^2} = \sum\limits_{i=0}^{n}(-1)^ix^{2i} + \frac{(-1)^{n+1}x^{2n+2}}{1+x^2}$
$\therefore \arctan x = \int_0^x\frac{\text{d}t}{1+t^2} =\sum\limits_{i=0}^{n}(-1)^i\int_0^xt^{2i}\text{d}t + \int_0^x\frac{(-1)^{n+1}}{1+t^2}t^{2n+2}\text{d}t$
$
=\sum\limits_{i=0}^{n}(-1)^i\frac{x^{2i+1}}{2i+1} + \int_0^x\frac{(-1)^{n+1}}{1+t^2}t^{2n+2}\text{d}t \quad \cdots (*)
$&lt;/p&gt;
&lt;p&gt;由 Hutton’s Formula 知，
$
\frac{\pi}{4} = 2\arctan\frac{1}{3}+\arctan\frac{1}{7} \Rightarrow \pi = 8\arctan\frac{1}{3}+4\arctan\frac{1}{7}
$
在 (*) 式中取 $$n = 2k+1$$，这是使余项为正且估计更精确
$
\pi = 8 \sum\limits_{i=0}^{2k+1}(-1)^i\frac{1}{(2i+1)3^{2i+1}} + 8 \int_0^{\frac{1}{3}}\frac{t^{4k+4}}{1+t^2}\text{d}t + 4 \sum\limits_{i=0}^{2k+1}(-1)^i\frac{1}{(2i+1)7^{2i+1}}+ 4 \int_0^{\frac{1}{7}}\frac{t^{4k+4}}{1+t^2}\text{d}t
$
$
t_k =8 \sum\limits_{i=0}^{2k+1}(-1)^i\frac{1}{(2i+1)}\left(\frac{1}{3^{2i+1}} + \frac{1}{2 \cdot 7^{2i+1}}\right)
$
$
r_k =8 \int_0^{\frac{1}{3}}\frac{t^{4k+4}}{1+t^2}\text{d}t + 4\int_0^{\frac{1}{7}}\frac{t^{4k+4}}{1+t^2}\text{d}t
$
$
\leq 8 \int_0^{\frac{1}{3}}t^{4k+4}\text{d}t + 4\int_0^{\frac{1}{7}}t^{4k+4}\text{d}t
$
$
\leq 8 \cdot \frac{1}{3}\cdot \frac{1}{3^{4k+4}} + 4\cdot \frac{1}{7} \cdot \frac{1}{7^{4k+4}} \leq \frac{1}{3^{4k}} \leq \frac{1}{80^k}
$&lt;/p&gt;
&lt;p&gt;因此，$\pi = t_k + r_k$，且 $0 &amp;lt;r_k &amp;lt; \frac{1}{80^k}$，$t$ 为有理数，设 $t_k$ 的十进制展开式为
$t_k = a_{k0}a_{k1} a_{k2} \cdots a_{kn} \cdots$
对于 $n \in \mathbb{N}$，存在 $l \geq n+1$ 使在 $t_l$ 中并非$a_{l,n+1}, a_{l, n+2}, \cdots, a_{l,l}$ 皆为 $9$，若不然，对任何 $l \geq n+1$，$a_l, a_{l,n+1}, a_{l, n+2}, \cdots, a_{l,l}$ 皆为 $9$。&lt;/p&gt;
&lt;p&gt;$\because 10^{l}\pi = 10^lt_l + 10^lr_l$，$\therefore 10^lt_l &amp;lt; 10^l \pi &amp;lt; 10^lt_l+\frac{1}{8^l}$
这样在 $\pi$ 的展开式中，从某位起皆为 $9$，从而 $\pi$ 为有理数
令$l=l(n) = \mu l. (l\geq n+1$ 且在 $a_{l, n+1}, \cdots, a_{l,l}$ 中并非皆为 $9)$
$\because a(l,i) = a_{l, i} = t_l$ 展开式的第 $i$ 个数字 $\in \mathcal{EF}$
$\therefore l(n) = \mu l .((l \geq n+1) \wedge \prod\limits_{i = n+1}^l(a_{l,i}\dot{-}9 \neq 0)) \in\mathcal{GRF}$
（注：若知道 $\pi$ 展开式中连续 $9$ 的个数有上限，则 $l(n) \in \mathcal{EF}$）
我们有 $t_k &amp;lt; \pi &amp;lt; t_l+\frac{1}{80^l}$，
对于 $n \in \mathbb{N}$，取 $l = l(n)$
$\because a_{l, n+1}, \cdots, a_{l,l}$ 并非皆为 $9$，&lt;/p&gt;
&lt;p&gt;$\therefore$ 设 $a_{l,m} &amp;lt; 9$，这里 $n+1 \leq m \leq l$$，设 $$\pi = \pi_0\pi_1\pi_2\cdots$&lt;/p&gt;
&lt;p&gt;从而 $t_l &amp;lt; \pi &amp;lt; t_l + \frac{1}{80^m}$，$10^mt_l &amp;lt; 10^m\pi&amp;lt;10^mt_l + \frac{1}{8^m}$&lt;/p&gt;
&lt;p&gt;从而
$a_{l0}a_{l1}a_{l2}\cdots a_{ln}a_{l\overline{n+1}} \cdots a_{lm}a_{l\overline{m+1}} \cdots$
$&amp;lt;\pi_0\pi_1\pi_2 \cdots \pi_n\pi_{n+1}\cdots \pi_m \pi_{m+1} \cdots$
$&amp;lt; a_{l0}a_{l1}a_{l2}\cdots a_{ln}a_{l\overline{n+1}} \cdots a_{lm}a_{l\overline{m+1}} \cdots + \frac{1}{8^m}(m \geq 1)$
$&amp;lt; a_{l0}a_{l1}a_{l2}\cdots a_{ln}a_{l\overline{n+1}} \cdots (a_{lm}+1)a_{l\overline{m+1}} \cdots$&lt;/p&gt;
&lt;p&gt;因此，
$$
\begin{array}{rl}
a_{l0}a_{l1}\cdots a_{ln}a_{l\overline{n+1}} \cdots a_{lm} &amp;amp; \leq \pi_0\pi_1\pi_2 \cdots \pi_n\pi_{n+1}\cdots \pi_m \
&amp;amp; \leq a_{l0}a_{l1}a_{l2}\cdots a_{ln}a_{l\overline{n+1}} \cdots (a_{lm}+1)
\end{array}
$$
$\because m \geq n+1$， $\therefore \pi_n = a_{ln}$&lt;/p&gt;
&lt;p&gt;因此，$f(n) = \pi_n = a(l(n), n) \in \mathcal{GRF}$。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;2017年研究生课程——计算模型导引第一章课后习题(习题课整理)。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Course" scheme="http://keyunluo.github.io/categories/Course/"/>
    
    
      <category term="计算模型导引" scheme="http://keyunluo.github.io/tags/%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9E%8B%E5%AF%BC%E5%BC%95/"/>
    
  </entry>
  
  <entry>
    <title>Paxos分布式一致性算法</title>
    <link href="http://keyunluo.github.io/2017/03/18/DistributedSystem/paxos-algorithm.html"/>
    <id>http://keyunluo.github.io/2017/03/18/DistributedSystem/paxos-algorithm.html</id>
    <published>2017-03-18T02:50:02.000Z</published>
    <updated>2017-07-06T08:08:06.891Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;Paxos算法是莱斯利·兰伯特（英语：Leslie Lamport，LaTeX中的“La”）于1990年提出的一种基于消息传递且具有高度容错特性的一致性算法。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;基本模型&quot;&gt;&lt;a href=&quot;#基本模型&quot; class=&quot;headerlink&quot; title=&quot;基本模型&quot;&gt;&lt;/a&gt;基本模型&lt;/h2&gt;&lt;h3 id=&quot;环境假设&quot;&gt;&lt;a href=&quot;#环境假设&quot; class=&quot;headerlink&quot; title=&quot;环境假设&quot;&gt;&lt;/a&gt;环境假设&lt;/h3&gt;&lt;p&gt;异步(执行时间和消息传递时间不确定)的没有拜占庭式错误(即：消息可能重复传输或丢失，但不会被篡改)。&lt;/p&gt;
&lt;h3 id=&quot;基本问题&quot;&gt;&lt;a href=&quot;#基本问题&quot; class=&quot;headerlink&quot; title=&quot;基本问题&quot;&gt;&lt;/a&gt;基本问题&lt;/h3&gt;&lt;p&gt;在分布式系统中经常会发生机器宕机以及网络异常，需要快速正确的在集群内部实现对某个数据的值达成一致，并且保证上述异常不会破坏整个系统的一致性。&lt;/p&gt;
&lt;p&gt;一致性问题是分布式系统中的经典问题。&lt;/p&gt;
&lt;h3 id=&quot;一致性基本要求&quot;&gt;&lt;a href=&quot;#一致性基本要求&quot; class=&quot;headerlink&quot; title=&quot;一致性基本要求&quot;&gt;&lt;/a&gt;一致性基本要求&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;在这些被提出的提案中，只有一个会被选中&lt;/li&gt;
&lt;li&gt;如果没有提案被提出，那么就不会有被选定的提案&lt;/li&gt;
&lt;li&gt;当一个提案被选定后，进程可以获取被选定的提案的信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;同时，基于安全性考虑，一致性需要满足：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只有被提出的提案才能被选定&lt;/li&gt;
&lt;li&gt;只能有一个值被选定&lt;/li&gt;
&lt;li&gt;如果某个进程认为某个提案被选定了，那么这个提案必须是真的被选定的那个。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;基本假设&quot;&gt;&lt;a href=&quot;#基本假设&quot; class=&quot;headerlink&quot; title=&quot;基本假设&quot;&gt;&lt;/a&gt;基本假设&lt;/h3&gt;&lt;p&gt;在Paxos一致性算法中，有三种参与角色：Proposer(提案提出者)、Acceptor(提案批准者)、Learner(提案学习者)，另外为了防止死锁问题，可以引入一个主Proposer：Leader，规定只有主Proposer才可以提出提案,提案编号$M_n$,提案内容为$v_n$。&lt;/p&gt;
&lt;p&gt;假设参与者充当上述角色，那么基本假设如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;参与者： 每个参与者可以以任意的速度执行，可能会因为出错而停止，也可能会重启， 同时，即使一个提案被选定后，所有的参与者也可能失败或重启，因此除非那些失败或重启的参与者可以记录某些信息，否则将无法确定最终的值。&lt;/li&gt;
&lt;li&gt;消息：消息在传输的过程中可能会出现不可预知的延迟，也有可能会重复或丢失，但是消息不会被篡改。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;具体算法&quot;&gt;&lt;a href=&quot;#具体算法&quot; class=&quot;headerlink&quot; title=&quot;具体算法&quot;&gt;&lt;/a&gt;具体算法&lt;/h2&gt;&lt;h3 id=&quot;提案批准&quot;&gt;&lt;a href=&quot;#提案批准&quot; class=&quot;headerlink&quot; title=&quot;提案批准&quot;&gt;&lt;/a&gt;提案批准&lt;/h3&gt;&lt;p&gt;Paxos协议流程划分为两个阶段，第一阶段是准备阶段，Proposer学习提案最新的状态；第二阶段是提交阶段，根据学习到的状态组成正确提案，完整的协议过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Prepare阶段&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;选择一个提案编号$M_n$,并向半数以上的Acceptor发送编号为$M_n$的Prepare请求；&lt;/li&gt;
&lt;li&gt;如果一个Acceptor收到一个编号为$M_n$的Prepare请求，并且$M_n$大于它已经响应的所有Prepare请求的编号，那么它就会保证不会再批准(Accept)任何编号小于$M_n$的提案，同时将它已经通过的最大编号的提案(如果存在的话)作为响应。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Accept阶段&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果Proposer收到来自半数以上的Acceptor对于它的prepare请求(编号为$M_n$)的响应，那么它就会发送一个针对编号为$M_n$，value值为$v_n$的提案的Accept请求给Acceptor，在这里$v_n$是收到的响应中编号最大的提案的值，如果响应中不包含提案，那么它可以是任意值;&lt;ul&gt;
&lt;li&gt;如果Acceptor收到一个针对编号$v_n$的提案的Accept请求，只要它还未对编号大于$v_n$的Prepare请求作出响应，它就可以通过这个提案。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;提案学习&quot;&gt;&lt;a href=&quot;#提案学习&quot; class=&quot;headerlink&quot; title=&quot;提案学习&quot;&gt;&lt;/a&gt;提案学习&lt;/h3&gt;&lt;p&gt;为了减少通信开销以及防止单点故障问题，将主Learner的范围扩大，Acceptor可以将已经批准的提案发送给一个指定的Learner的集合，该集合中的每一个Learner都可以在一个提案被通过后通知其他的Learner。&lt;/p&gt;
&lt;h3 id=&quot;活锁问题&quot;&gt;&lt;a href=&quot;#活锁问题&quot; class=&quot;headerlink&quot; title=&quot;活锁问题&quot;&gt;&lt;/a&gt;活锁问题&lt;/h3&gt;&lt;p&gt;如果两个Proposer依次交叉提出一系列编号递增的提案，但是最终都没有被选定通过，则会陷入死循环状态。因此必须选择一个主Proposer，只有主Proposer才能提出提案，即选举出一个Proposer作为Leader，所有的Proposal都通过Leader来提交，当Leader宕机时马上再选举其他的Leader。&lt;/p&gt;
&lt;h3 id=&quot;推导过程&quot;&gt;&lt;a href=&quot;#推导过程&quot; class=&quot;headerlink&quot; title=&quot;推导过程&quot;&gt;&lt;/a&gt;推导过程&lt;/h3&gt;&lt;p&gt;分析：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;单个Acceptor:
  所有的Proposal都发给这个Acceptor,它接受最先收到的Proposal，若这个Acceptor发生故障，则整个系统都将无法工作，因此不可取(单点问题)。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;多个Acceptor:
  可以解决单点问题, 某个Acceptor故障不会影响到提案批准，如果超半数的Acceptor批准某个具有编号$M_n$的值$v_n$，则该值被选择。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Acceptor只能Accept一个Proposal. 并为了保证当只有一个value的情况下也会被choose,在没有失败和消息丢失的情况下，可将上述多个Acceptor加强为：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;P1:Acceptor必须批准它接收到的第一个决议。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;上面这个要求导致如下问题：如果多个提案被不同的Proposer同时提出，这可能导致每个Acceptor都批准了它收到的第一个提案，但没有一个提案是由多数人选择批准的；或者即使只有两个提案被提出，每个提案都差不多被一半的Acceptor批准，此时当一个Acceptor出错，都导致无法选择任意一个提案。&lt;/p&gt;
&lt;p&gt;因此，在P1的基础上，需要避免多个value被选择批准，即允许多个提案被选定，但必须保证所有被选定的提案都有相同的值value，引出如下定义：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;P2： 如果一个提案{n, v}被选择，那么所有被选择的提案（编号更高）包含的决议都是v。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;得到P2后关键是如何保持P2成立，需要进一步的具体和细化。&lt;/p&gt;
&lt;p&gt;因为提案的编号是全序的，条件P2保证了只有一个value值被选定，同时一个提案被选定，首先必须被至少一个Acceptor批准，因而可以通过如下条件满足P2：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; $P2^a$： 如果一个提案{n, v}被选择，那么任何acceptor批准的提案（编号更高）包含的决议都是v。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;同时，我们仍然需要P1来保证提案被选定，同时因为通信是异步的，一个提案可能会在某个Acceptor还没有收到任何提案时就被选定了，因此，如果要同时满足P1和$P2^a$，需要对$P2^a$进行如下强化：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; $P2^b$:如果一个提案{n, v}被选择，那么此后，任何proposer提出的提案（编号更高）包含的决议都是v。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;因为一个提案必须在被Proposer提出后才能被Acceptor批准，因此$P2^b$包含了$P2^a$，进而包含了$P2$， 于是，下一步就是证明$P2^b$成立。为此，我们使用第二是数学归纳法证明如下结论：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; 假设编号为M0到Mn的提案已经被选定，其值都是v，证明编号为Mn的提案的值也为v。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;因为编号为M0的提案已经被选定了，这就意味着肯定存在一个由半数以上的Acceptor组成的集合C，C中的每个Acceptor都批准了这个提案，由归纳假设知，“编号为M0的提案被选定”意味着：C中的每一个Acceptor都批准了一个编号在M0到Mn-1范围内的提案，并且每个编号在M0到Mn-1范围内的被批准的提案，它的值也是v。&lt;/p&gt;
&lt;p&gt;因为任何包含半数以上的Acceptor的集合都至少包含C中的一个成员，因此如果一下$P2^c$保持不变，那么编号为Mn的提案的值也为v:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; $P2^c$: 对于任意的v和n，如果提案{n, v}被提出，那么存在一个由acceptor的多数派组成的集合S,满足如下两个条件中的任意一个&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;S中没有acceptor批准过编号小于n的提案&lt;/li&gt;
&lt;li&gt;在S的任何acceptor批准的所有提案（编号小于n）中，v是编号最大的提案的决议。}&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$P2^c$包含$P2^b$是显而易见的,因此，如果能保证$P2^c$,就满足了$P2^b$。&lt;/p&gt;
&lt;p&gt;为了保持$P2^c$的不变性，准备提出议案（编号为Mn）的Proposer必须知道所有编号小于Mn的议案中编号最大的那个，如果存在的话，它已经或将要被Acceptor的某个多数派批准。获取已经批准的议案是简单的，但是预知将来可能批准的议案是困难的。Proposer并不做预测，而是假定不会有这样的情况。也就是说，Proposer要求Acceptor不能批准任何编号小于Mn的议案。这引出了下面提出议案的两阶段算法:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一步：准备阶段： Proposer选择一个新编号Mn，向某个Acceptor集合中的所有成员发送请求，并要求Acceptor集合回应：&lt;ul&gt;
&lt;li&gt;向Proposer承诺永不批准编号小于Mn的议案&lt;/li&gt;
&lt;li&gt;如果Acceptor已经批准过任何提案，那么其就向Acceptor反馈当前已经批准的编号小于Mn的编号最大的议案。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;第二步：接收请求： 如果Proposer收到了多数Acceptor的回应，那么它就可以提出议案{Mn, v}，其中v是所有回应中编号最高的议案的决议，或者是如果Acceptor回应说还没有批准过议案,Proposer则回应任意值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在确定提案后，Proposer就会将该提案再次发送给某个Acceptor集合，并期望获得它们的批准，我们称此请求为Accept请求。&lt;/p&gt;
&lt;p&gt;对于Acceptor,其批准提案如下：&lt;/p&gt;
&lt;p&gt;一个Acceptor可能会收到来自Proposer的两种请求，分别是Prepare和Accept，对这两类请求做出的响应条件分别如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prepare: Acceptor可以在任何时候响应一个Prepare请求&lt;/li&gt;
&lt;li&gt;Accept: 在不违背Accept现有承诺的前提下，可以任意响应Accept请求&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，对于Acceptor处理的约束条件为：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;$P1^a$: acceptor可以批准一个编号为n的提案，当且仅当它没有回应过一个编号大于n的prepare请求。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;主要贡献与存在问题&quot;&gt;&lt;a href=&quot;#主要贡献与存在问题&quot; class=&quot;headerlink&quot; title=&quot;主要贡献与存在问题&quot;&gt;&lt;/a&gt;主要贡献与存在问题&lt;/h2&gt;&lt;h3 id=&quot;贡献&quot;&gt;&lt;a href=&quot;#贡献&quot; class=&quot;headerlink&quot; title=&quot;贡献&quot;&gt;&lt;/a&gt;贡献&lt;/h3&gt;&lt;p&gt;Paxos算法解决的问题是在一个可能发生异常（进程可能会慢、被杀死或者重启，消息可能会延迟、丢失、重复，不考虑消息篡改即拜占庭错误的情况）的分布式系统中如何就某个值达成一致，保证不论发生以上任何异常，都不会破坏决议的一致性。&lt;/p&gt;
&lt;p&gt;Paxos协议提出只要系统中2f+1个节点中的f+1个节点可用，那么系统整体就可用并且能保证数据的强一致性，它对于可用性的提升是极大的。&lt;/p&gt;
&lt;p&gt;Paxos算法是分布式计算领域的一个开创性算法，后面有很多算法都是在其基础上改进演化的。&lt;/p&gt;
&lt;h3 id=&quot;存在问题&quot;&gt;&lt;a href=&quot;#存在问题&quot; class=&quot;headerlink&quot; title=&quot;存在问题&quot;&gt;&lt;/a&gt;存在问题&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;如何保证全局编号问题&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在分布式环境下, 编号无法直接确定, 只能根据自身知道的最大number先提一个Proposal, 如果被拒绝, 那么拒绝的Acceptor应该通知他知道的最大的number是多少.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;活锁问题&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如前文所述，需要Proposer Leader,而这个问题又引入Leader选举问题，存在称之为PaxosLease的Paxos算法简化版可以完成leader的选举.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;持久存储问题
在算法执行的过程中会产生很多的异常情况, 各种角色都可能失败，为了保证paxos算法的正确性，需要各个节点都做持久存储，以便重启后还能恢复，典型的有：&lt;ul&gt;
&lt;li&gt;Proposer存储已提交的最大Proposal编号&lt;/li&gt;
&lt;li&gt;Learner存储已学习过的Proposal被Accepted情况&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;Paxos算法是莱斯利·兰伯特（英语：Leslie Lamport，LaTeX中的“La”）于1990年提出的一种基于消息传递且具有高度容错特性的一致性算法。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="DistributedSystem" scheme="http://keyunluo.github.io/categories/DistributedSystem/"/>
    
    
      <category term="分布式一致性" scheme="http://keyunluo.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统——试卷分析</title>
    <link href="http://keyunluo.github.io/2016/12/28/Course/distributed-system-2.html"/>
    <id>http://keyunluo.github.io/2016/12/28/Course/distributed-system-2.html</id>
    <published>2016-12-28T06:25:02.000Z</published>
    <updated>2017-07-06T08:08:06.891Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;2015年秋南京大学计算机系分布式系统期末试卷分析。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;1-分布式系统的定义与架构-15分&quot;&gt;&lt;a href=&quot;#1-分布式系统的定义与架构-15分&quot; class=&quot;headerlink&quot; title=&quot;1. 分布式系统的定义与架构(15分)&quot;&gt;&lt;/a&gt;1. 分布式系统的定义与架构(15分)&lt;/h2&gt;&lt;h3 id=&quot;1-1-分别描述网络操作系统与分布式系统的定义&quot;&gt;&lt;a href=&quot;#1-1-分别描述网络操作系统与分布式系统的定义&quot; class=&quot;headerlink&quot; title=&quot;1.1 分别描述网络操作系统与分布式系统的定义&quot;&gt;&lt;/a&gt;1.1 分别描述网络操作系统与分布式系统的定义&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;网络操作系统：一种面向计算机网络的操作系统，允许网络中的多台计算机访问共享的文件和打印机，允许共享数据，用户，组，安全，应用和其他网络功能。&lt;/li&gt;
&lt;li&gt;分布式系统：一个分布式系统是一些独立的计算机集合，但是对这个系统的用户来说，系统就像一台计算机一样，即从硬件角度来讲，每台计算机都是自主的，从软件角度来讲，用户将整个系统看做是一台计算机。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;1-2-解释分布式系统中透明性的含义&quot;&gt;&lt;a href=&quot;#1-2-解释分布式系统中透明性的含义&quot; class=&quot;headerlink&quot; title=&quot;1.2 解释分布式系统中透明性的含义&quot;&gt;&lt;/a&gt;1.2 解释分布式系统中透明性的含义&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;透明性：将分布式系统中的进程和资源实际上在多台计算机上分布这样一个事实隐藏起来，如果一个分布式系统能够在用户和应用程序面前呈现为单个计算机系统，这样的分布式系统就称为是透明的。&lt;/li&gt;
&lt;li&gt;种类：访问透明性、位置透明性、迁移透明性、重定位透明性、复制透明性、并发透明性、故障透明性&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;1-3-如何理解分布式系统中的机制与策略&quot;&gt;&lt;a href=&quot;#1-3-如何理解分布式系统中的机制与策略&quot; class=&quot;headerlink&quot; title=&quot;1.3 如何理解分布式系统中的机制与策略&quot;&gt;&lt;/a&gt;1.3 如何理解分布式系统中的机制与策略&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;策略：主要定义一些功能完成的程度及要求，如客户端缓存数据时需要什么程度的一致性等&lt;/li&gt;
&lt;li&gt;机制：能提供或不能提供什么功能，如允许动态设置缓存策略，支持对移动代码的不同级别的信任&lt;/li&gt;
&lt;li&gt;分布式系统中仅提供机制&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;2-进程与线程-10分&quot;&gt;&lt;a href=&quot;#2-进程与线程-10分&quot; class=&quot;headerlink&quot; title=&quot;2. 进程与线程(10分)&quot;&gt;&lt;/a&gt;2. 进程与线程(10分)&lt;/h2&gt;&lt;h3 id=&quot;2-1-简述进程与线程的异同&quot;&gt;&lt;a href=&quot;#2-1-简述进程与线程的异同&quot; class=&quot;headerlink&quot; title=&quot;2.1 简述进程与线程的异同&quot;&gt;&lt;/a&gt;2.1 简述进程与线程的异同&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;进程：进程是正在运行的程序的实例，是系统进行资源分配和调度的一个独立单位。&lt;/li&gt;
&lt;li&gt;线程：线程是CPU调度和分派的基本单位。&lt;/li&gt;
&lt;li&gt;相同点：都能进行并发。&lt;/li&gt;
&lt;li&gt;不同点：线程不拥有系统资源，同一个进程内的线程共享资源，进程至少有一个线程，有自己的独立地址空间。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;2-2-代码迁移与虚拟机迁移相比，有什么优势与劣势&quot;&gt;&lt;a href=&quot;#2-2-代码迁移与虚拟机迁移相比，有什么优势与劣势&quot; class=&quot;headerlink&quot; title=&quot;2.2 代码迁移与虚拟机迁移相比，有什么优势与劣势&quot;&gt;&lt;/a&gt;2.2 代码迁移与虚拟机迁移相比，有什么优势与劣势&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;优势：迁移量比较少，能快速完成，有效减少网络带宽&lt;/li&gt;
&lt;li&gt;劣势：机制复杂，迁移后的代码可能在新机器上不能很好地运行，所引用的资源在新机器上不可用&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;3-通信机制-15分&quot;&gt;&lt;a href=&quot;#3-通信机制-15分&quot; class=&quot;headerlink&quot; title=&quot;3. 通信机制(15分)&quot;&gt;&lt;/a&gt;3. 通信机制(15分)&lt;/h2&gt;&lt;h3 id=&quot;3-1-在远程过程调用-RPC-过程中，如果客户端出现故障，请给出至少3中可能的解决方式&quot;&gt;&lt;a href=&quot;#3-1-在远程过程调用-RPC-过程中，如果客户端出现故障，请给出至少3中可能的解决方式&quot; class=&quot;headerlink&quot; title=&quot;3.1 在远程过程调用(RPC)过程中，如果客户端出现故障，请给出至少3中可能的解决方式&quot;&gt;&lt;/a&gt;3.1 在远程过程调用(RPC)过程中，如果客户端出现故障，请给出至少3中可能的解决方式&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;客户端故障：&lt;ul&gt;
&lt;li&gt;客户端无法定位服务器：可能原因：服务器关闭，服务器使用了新版本的存根而客户端使用了较旧版本的存根；解决方法：使用一个特殊码，例如-1作为过程的返回值来指示失败；让错误触发一个异常或信号。&lt;/li&gt;
&lt;li&gt;客户端向服务器端发送的请求消息丢失：当发送一个消息的时候内核开启一个计时器：在响应或ACK到来之前定时器过期，则内核重发；如果消息真的丢失：服务器不会区分原始和重传的消息，一切正常；如果许多请求都丢失，内核放弃并假装服务器已经关闭了。&lt;/li&gt;
&lt;li&gt;客户端发送请求后崩溃：&lt;ul&gt;
&lt;li&gt;消除 extermination: 在日志文件中纪录 RPC 请求，重启后清除孤儿&lt;/li&gt;
&lt;li&gt;再生 reincarnation: 按时间顺序编号不同的时间段。当客户端重启时，广播一条消息宣布新的时间段开始，当广播到达时终止所有远程计算，无需日志&lt;/li&gt;
&lt;li&gt;温和再生 gentle reincarnation: 与再生相似，但是当广播到达时，每台机器会寻找远程计算的所有者，仅当找不到所有者时，计算才会被终止。&lt;/li&gt;
&lt;li&gt;过期 expiration: 赋予每个 RPC 一个标准时间配额，未完成任务明确申请额外配额。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;3-2-为什么要采用动态绑定机制来实现服务器的定位？动态绑定有什么缺点？&quot;&gt;&lt;a href=&quot;#3-2-为什么要采用动态绑定机制来实现服务器的定位？动态绑定有什么缺点？&quot; class=&quot;headerlink&quot; title=&quot;3.2 为什么要采用动态绑定机制来实现服务器的定位？动态绑定有什么缺点？&quot;&gt;&lt;/a&gt;3.2 为什么要采用动态绑定机制来实现服务器的定位？动态绑定有什么缺点？&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;优点：&lt;ul&gt;
&lt;li&gt;灵活性&lt;/li&gt;
&lt;li&gt;能支持多个支持同一接口的服务器&lt;ul&gt;
&lt;li&gt;绑定程序(binder)可以随机地将服务器上的客户端传播到均匀负载&lt;/li&gt;
&lt;li&gt;绑定程序(binder)可以定期轮询服务器，自动取消注册失败的服务器，以达到一定的容错能力&lt;/li&gt;
&lt;li&gt;绑定程序(binder)可以帮助身份验证&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;绑定程序(binder)可以验证客户端和服务器都使用相同版本的接口&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;缺点&lt;ul&gt;
&lt;li&gt;导出/导入接口的额外开销花费时间&lt;/li&gt;
&lt;li&gt;绑定程序(binder)可能成为大型分布式系统中的瓶颈&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;3-3-分别说明持久性通信和非持久性通信的含义，电子邮件和QQ留言分别属于什么样的通信模式？&quot;&gt;&lt;a href=&quot;#3-3-分别说明持久性通信和非持久性通信的含义，电子邮件和QQ留言分别属于什么样的通信模式？&quot; class=&quot;headerlink&quot; title=&quot;3.3 分别说明持久性通信和非持久性通信的含义，电子邮件和QQ留言分别属于什么样的通信模式？&quot;&gt;&lt;/a&gt;3.3 分别说明持久性通信和非持久性通信的含义，电子邮件和QQ留言分别属于什么样的通信模式？&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;持久性通信：通信机制本身会对消息进行持久存储，直到它被传递给目的。&lt;/li&gt;
&lt;li&gt;非持久性通信：消息的发送者和接收者必须同时存在才能进行，传输服务仅仅提供临时的对消息的存储。&lt;/li&gt;
&lt;li&gt;电子邮件：持久通信；QQ留言：持久通信&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;4-命名与同步-20分&quot;&gt;&lt;a href=&quot;#4-命名与同步-20分&quot; class=&quot;headerlink&quot; title=&quot;4. 命名与同步(20分)&quot;&gt;&lt;/a&gt;4. 命名与同步(20分)&lt;/h2&gt;&lt;h3 id=&quot;4-1-请简述“基于本部的方法”（home-based-approach）是如何解决移动实体定位的&quot;&gt;&lt;a href=&quot;#4-1-请简述“基于本部的方法”（home-based-approach）是如何解决移动实体定位的&quot; class=&quot;headerlink&quot; title=&quot;4.1 请简述“基于本部的方法”（home-based approach）是如何解决移动实体定位的&quot;&gt;&lt;/a&gt;4.1 请简述“基于本部的方法”（home-based approach）是如何解决移动实体定位的&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;实体的本部位置地址在命名服务中注册；&lt;/li&gt;
&lt;li&gt;本部位置上注册了实体的外部地址(Foreign address)；&lt;/li&gt;
&lt;li&gt;客户端先访问本部位置，随后访问外部地址。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;4-2-为什么要进行同步？分布式系统的同步与集中式系统有何区别？&quot;&gt;&lt;a href=&quot;#4-2-为什么要进行同步？分布式系统的同步与集中式系统有何区别？&quot; class=&quot;headerlink&quot; title=&quot;4.2 为什么要进行同步？分布式系统的同步与集中式系统有何区别？&quot;&gt;&lt;/a&gt;4.2 为什么要进行同步？分布式系统的同步与集中式系统有何区别？&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;同步：&lt;ul&gt;
&lt;li&gt;与进程间通信紧密相关的问题是进程间如何协作和同步&lt;/li&gt;
&lt;li&gt;多个进程不能同时访问访问一个共享资源，而是相互协作，彼此暂时地独占访问&lt;/li&gt;
&lt;li&gt;多个进程有时可能需要就时间的顺序达成一致&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;区别：相较于集中式系统的同步，分布式系统没有共享的存储，也难以实现物理时钟的一致性。集中式系统中的同步问题不适合于分布式系统。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;4-3-假设有-3-个机器采用-Lamport-算法进行同步，设左边图给出了机器原始时钟下的交互图。&quot;&gt;&lt;a href=&quot;#4-3-假设有-3-个机器采用-Lamport-算法进行同步，设左边图给出了机器原始时钟下的交互图。&quot; class=&quot;headerlink&quot; title=&quot;4.3 假设有 3 个机器采用 Lamport 算法进行同步，设左边图给出了机器原始时钟下的交互图。&quot;&gt;&lt;/a&gt;4.3 假设有 3 个机器采用 Lamport 算法进行同步，设左边图给出了机器原始时钟下的交互图。&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;在右边方框内画出经过 Lamport 算法调整后的时钟图。&lt;/li&gt;
&lt;li&gt;判定下面 4 组事件中两个消息发送事件的先后顺序：在先发生事件后打钩，如果不能判定，则在“不能判定”后面打钩。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;i.  P1 (send, m3) P3(send, m4) 不能判定 yes&lt;/p&gt;
&lt;p&gt;ii. P2 (send, m4) P1(send, m5) 不能判定 yes&lt;/p&gt;
&lt;p&gt;iii.P3 (send, m4) yes P2(send, m6) 不能判定&lt;/p&gt;
&lt;p&gt;iv. P3 (send, m7) yes P2(send, m8) 不能判定&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-12/distributed.png&quot; alt=&quot;时钟图&quot;&gt;
&lt;img src=&quot;/resource/blog/2016-12/lamport.png&quot; alt=&quot;Lamport时钟&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;5-一致性与容错-15分&quot;&gt;&lt;a href=&quot;#5-一致性与容错-15分&quot; class=&quot;headerlink&quot; title=&quot;5. 一致性与容错(15分)&quot;&gt;&lt;/a&gt;5. 一致性与容错(15分)&lt;/h2&gt;&lt;h3 id=&quot;5-1-下图所示两组进程读写是否满足因果一致性和顺序一致性？如果不满足，需要说明原因。&quot;&gt;&lt;a href=&quot;#5-1-下图所示两组进程读写是否满足因果一致性和顺序一致性？如果不满足，需要说明原因。&quot; class=&quot;headerlink&quot; title=&quot;5.1 下图所示两组进程读写是否满足因果一致性和顺序一致性？如果不满足，需要说明原因。&quot;&gt;&lt;/a&gt;5.1 下图所示两组进程读写是否满足因果一致性和顺序一致性？如果不满足，需要说明原因。&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-12/consistency.png&quot; alt=&quot;一致性&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a: 不满足顺序一致性：读到的最终值应该b，不满足因果一致性：p2中a在b之前，不应该先读b再读a&lt;/li&gt;
&lt;li&gt;b: 满足顺序一致性：最终读到b,满足因果一致性：先a后b&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;5-2-下面的已编号小竖线表示进程的检查点（check-point），箭头表示消息发送，横轴表示时间，设-P2-在图所示处出现故障，3-个进程从检查点进行恢复，请从这些检查点中构建一个最近的一致全局状态，并说明理由。&quot;&gt;&lt;a href=&quot;#5-2-下面的已编号小竖线表示进程的检查点（check-point），箭头表示消息发送，横轴表示时间，设-P2-在图所示处出现故障，3-个进程从检查点进行恢复，请从这些检查点中构建一个最近的一致全局状态，并说明理由。&quot; class=&quot;headerlink&quot; title=&quot;5.2 下面的已编号小竖线表示进程的检查点（check point），箭头表示消息发送，横轴表示时间，设 P2 在图所示处出现故障，3 个进程从检查点进行恢复，请从这些检查点中构建一个最近的一致全局状态，并说明理由。&quot;&gt;&lt;/a&gt;5.2 下面的已编号小竖线表示进程的检查点（check point），箭头表示消息发送，横轴表示时间，设 P2 在图所示处出现故障，3 个进程从检查点进行恢复，请从这些检查点中构建一个最近的一致全局状态，并说明理由。&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-12/checkpoint.png&quot; alt=&quot;检查点&quot;&gt;&lt;/p&gt;
&lt;p&gt;检查点为258：因为全局一致要求任意两个检查点作恢复点时中间连线与事件没有交点。&lt;/p&gt;
&lt;h2 id=&quot;6-社会网络-10分&quot;&gt;&lt;a href=&quot;#6-社会网络-10分&quot; class=&quot;headerlink&quot; title=&quot;6. 社会网络(10分)&quot;&gt;&lt;/a&gt;6. 社会网络(10分)&lt;/h2&gt;&lt;h3 id=&quot;6-1-比较随机图（Random-Graph）、小世界（Small-World）和无标度（Scale-free）三种网络模型。&quot;&gt;&lt;a href=&quot;#6-1-比较随机图（Random-Graph）、小世界（Small-World）和无标度（Scale-free）三种网络模型。&quot; class=&quot;headerlink&quot; title=&quot;6.1 比较随机图（Random Graph）、小世界（Small-World）和无标度（Scale-free）三种网络模型。&quot;&gt;&lt;/a&gt;6.1 比较随机图（Random Graph）、小世界（Small-World）和无标度（Scale-free）三种网络模型。&lt;/h3&gt;&lt;h3 id=&quot;6-2-请描述社会影响力最大化问题，并阐述解决该问题的思路。&quot;&gt;&lt;a href=&quot;#6-2-请描述社会影响力最大化问题，并阐述解决该问题的思路。&quot; class=&quot;headerlink&quot; title=&quot;6.2 请描述社会影响力最大化问题，并阐述解决该问题的思路。&quot;&gt;&lt;/a&gt;6.2 请描述社会影响力最大化问题，并阐述解决该问题的思路。&lt;/h3&gt;&lt;hr&gt;
&lt;h2 id=&quot;6-本学期未上这一章节&quot;&gt;&lt;a href=&quot;#6-本学期未上这一章节&quot; class=&quot;headerlink&quot; title=&quot;6. 本学期未上这一章节&quot;&gt;&lt;/a&gt;6. 本学期未上这一章节&lt;/h2&gt;&lt;hr&gt;
&lt;h2 id=&quot;7-云计算-15分&quot;&gt;&lt;a href=&quot;#7-云计算-15分&quot; class=&quot;headerlink&quot; title=&quot;7. 云计算(15分)&quot;&gt;&lt;/a&gt;7. 云计算(15分)&lt;/h2&gt;&lt;h3 id=&quot;7-1-简要说明虚拟化技术所解决的问题，并说明虚拟化技术与云计算的关系。&quot;&gt;&lt;a href=&quot;#7-1-简要说明虚拟化技术所解决的问题，并说明虚拟化技术与云计算的关系。&quot; class=&quot;headerlink&quot; title=&quot;7.1 简要说明虚拟化技术所解决的问题，并说明虚拟化技术与云计算的关系。&quot;&gt;&lt;/a&gt;7.1 简要说明虚拟化技术所解决的问题，并说明虚拟化技术与云计算的关系。&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;虚拟化：细粒度资源分配，动态迁移，增加资源利用率&lt;/li&gt;
&lt;li&gt;关系：&lt;ul&gt;
&lt;li&gt;封装与隔离：保证每个用户有安全可信的工作环境&lt;/li&gt;
&lt;li&gt;多实例：保证较高的资源利用率，为服务器合并提供基础&lt;/li&gt;
&lt;li&gt;硬件无关性：整合异构硬件资源可实现虚拟机迁移，使资源调度、负载平衡容易实现&lt;/li&gt;
&lt;li&gt;特权功能：入侵检测和病毒检测&lt;/li&gt;
&lt;li&gt;动态调整资源：细粒度的可扩展性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;7-2-以-Openstack-为代表的这一类云计算平台的基本功能是什么？&quot;&gt;&lt;a href=&quot;#7-2-以-Openstack-为代表的这一类云计算平台的基本功能是什么？&quot; class=&quot;headerlink&quot; title=&quot;7.2 以 Openstack 为代表的这一类云计算平台的基本功能是什么？&quot;&gt;&lt;/a&gt;7.2 以 Openstack 为代表的这一类云计算平台的基本功能是什么？&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;OpenStack是一个开源的云计算管理平台，主要为了管理：计算、存储、网络三个方面的资源，是IaaS组件，这些资源可以通过OpenStack进行配置分配并提供给上层应用或用户去使用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;7-3-应用场景&quot;&gt;&lt;a href=&quot;#7-3-应用场景&quot; class=&quot;headerlink&quot; title=&quot;7.3 应用场景&quot;&gt;&lt;/a&gt;7.3 应用场景&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;应用场景1：某校计算中心需要不同资源需求的桌面（Linux、Windows 等），应用于不同类型的计算机课程教学；&lt;/p&gt;
&lt;p&gt;应用场景2：某校物理系多个实验室需要三种软件进行模拟实验，这些模拟实验对计算资源消耗极大，但这些实验往往不是同时进行。&lt;/p&gt;
&lt;p&gt;上述两个应用场景分别是否适合采用 Openstack 平台来搭建系统，说明理由。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;ol&gt;
&lt;li&gt;能：教学是一个典型的多用户，每个用户使用资源不是很多，同时性的环境。终端用户对资源需求较少，数据中心资源利用率较低。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ol&gt;
&lt;li&gt;不能：OpenStack主要用来进行资源分配给多个用户同时使用，但模拟实验对计算资源消耗极大，划分资源后运行软件资源会不够用，并且往往也不是同时运行。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;2015年秋南京大学计算机系分布式系统期末试卷分析。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Course" scheme="http://keyunluo.github.io/categories/Course/"/>
    
    
      <category term="分布式系统" scheme="http://keyunluo.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘——试卷分析</title>
    <link href="http://keyunluo.github.io/2016/12/27/Course/data-mining.html"/>
    <id>http://keyunluo.github.io/2016/12/27/Course/data-mining.html</id>
    <published>2016-12-27T06:25:02.000Z</published>
    <updated>2017-07-06T08:08:06.890Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;2015年秋南京大学计算机系数据挖掘期末试卷分析。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;PCA-3-points&quot;&gt;&lt;a href=&quot;#PCA-3-points&quot; class=&quot;headerlink&quot; title=&quot;PCA(3 points)&quot;&gt;&lt;/a&gt;PCA(3 points)&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Given n data points $x_1,\ldots,x_n$, where $x_i \in \mathcal{R}^d$. Describe how to find the top &lt;strong&gt;k&lt;/strong&gt; principle components by &lt;strong&gt;SVD&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：SVD奇异值分解&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对X进行奇异值分解：$X = U\Sigma V^T = \sum_{i=1}^d \sigma_iu_iv_i^T$&lt;/li&gt;
&lt;li&gt;取X的k个最大的左奇异向量：$u_1,u_2,\ldots,u_k$&lt;/li&gt;
&lt;li&gt;x的新坐标：$U_k^Tx = [u_1^Tx, u_2^Tx, \ldots, u_k^Tx] \in \mathcal{R}^k, U_k = [u_1, u_2,\ldots,u_k] \in \mathcal{R}^{d*k}$&lt;/li&gt;
&lt;li&gt;X的新坐标：$U_k^TX=U_k^TU_r\Sigma_rV_r^T = \Sigma_kV_k^T$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Association-Pattern-Mining-2-points&quot;&gt;&lt;a href=&quot;#Association-Pattern-Mining-2-points&quot; class=&quot;headerlink&quot; title=&quot;Association Pattern Mining(2 points)&quot;&gt;&lt;/a&gt;Association Pattern Mining(2 points)&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;The &lt;strong&gt;downward closure property (i.e.,every subset of frequent itemset is also frequcent)&lt;/strong&gt; is leveraged to design efficient algorithms for association pattern mining. Why does this property hold?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：由于支持单调性，一个项集I包含在一个交易中，那么它的所有子集也包含在这个交易中，即子集的支持度不小于父集的支持度。&lt;/p&gt;
&lt;h2 id=&quot;NMF-3-points&quot;&gt;&lt;a href=&quot;#NMF-3-points&quot; class=&quot;headerlink&quot; title=&quot;NMF(3 points)&quot;&gt;&lt;/a&gt;NMF(3 points)&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Suppose we want to find a rank-k approximation of matrix $X \in \mathcal{R}^{d*n}$ by nonnegative matrix factorization. What is the optimization problem? Is it convex?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;优化问题：$\min_{U \in \mathcal{R}^{d*k}, V \in \mathcal{R}^{v*k}} \ \Vert X - UV^T \Vert_F^2 $， $s.t. \ U \ge 0, V \ge 0$&lt;/li&gt;
&lt;li&gt;非负矩阵分解是非凸的&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;SVM-6-points&quot;&gt;&lt;a href=&quot;#SVM-6-points&quot; class=&quot;headerlink&quot; title=&quot;SVM(6 points)&quot;&gt;&lt;/a&gt;SVM(6 points)&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Given a set of training data $(x_1,y_1),\ldots,(x_n,y_n)$, where $x_i \in \mathcal{R}^d$ and $y_i \in {\pm 1}$. The primal problem of SVM without intercept is given by: $$\min_{w \in \mathcal{R}^d} \sum_{i=1}^m \max(0,1-y_iw^Tx_i) + \frac{\lambda}{2} \Vert w \Vert_2^2$$. Show the derivation of the dual problem of SVM.&lt;/p&gt;
&lt;p&gt;Hints: Let $\mathcal{l}(x) = max(0,1-x)$ be the hinge loss. Then, its conjugate function id given by
$$ \mathcal{l}^*(y) = sup_x(yx-\mathcal{l}(x)) = \begin{cases} y&amp;amp; -1 \le y \le 0 \\
 \infty&amp;amp; \ otherwise
 \end{cases}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt; 解：SVM对偶问题的推导，无截距$b_0$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;由上述定义的hinge loss及其共轭函数知，原优化问题简化为：$\min_{w \in R^d} \sum_{i=1}^n \mathcal{l}(y_iw^Tx_i) + \frac{\lambda}{2}\Vert w \Vert_2^2$,等价于：
$$\sum_{i=1}^n \mathcal{l}(u_i) + \frac{\lambda}{2} \Vert w \Vert_2^2, s.t. \ u_i = y_iw^Tx_i, i = 1, \ldots, n$$&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;由拉格朗日乘法，得：$L(w,u,v) = \sum_{i=1}^n \mathcal{l}(u_i) + \frac{\lambda}{2} \Vert w \Vert_2^2 + \sum_{i=1}^n v_i(u_i - y_iw^Tx_i)$,其对偶问题：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\begin{array} {lcl}
    g(v) &amp;amp;=&amp;amp; \inf_{w,u} L(w,u,v) \\
         &amp;amp;=&amp;amp; \inf_{w,u} \sum_{i=1}^n \mathcal{l}(u_i) + \frac{\lambda}{2} \Vert w \Vert_2^2 + \sum_{i=1}^n v_i(u_i - y_iw^Tx_i) \\
         &amp;amp;=&amp;amp; \inf_{w,u} \sum_{i=1}^n(\mathcal{l}(u_i) + v_iu_i) + (\frac{\lambda}{2} \Vert w \Vert_2^2 - w^T\sum_{i=1}^nv_iy_ix_i)
\end{array}$$&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;依次最小化w,u:
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;$\inf_{u_i}(\mathcal{l}(u_i) + v_iu_i) = -\sup_{u_i}(-v_iu_i - l(u_i)) = - \mathcal{l}^* (-v_i) = v_i $, if $0 \le v_i \le 1$&lt;/li&gt;
&lt;li&gt;$\nabla_w L(w,u,v) = \lambda w - \sum_{i=1}^nv_iy_ix_i)$, 得：$w = \frac{1}{\lambda}  \sum_{i=1}^nv_iy_ix_i$&lt;/li&gt;
&lt;li&gt;最后， $g(v) = \sum_{i=1}^n v_i - \frac{1}{2 \lambda}\sum_{i=1}^n\sum_{j=1}^n v_iv_jy_iy_jx_i^Tx_j$，得到对偶问题：
$$\max_{v \in R^n} \sum_{i=1}^n v_i - \frac{1}{2 \lambda}\sum_{i=1}^n\sum_{j=1}^n v_iv_jy_iy_jx_i^Tx_j, \ s.t.\ 0 \le v_i \le 1, i=1, \ldots,n$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Ensemble-2-points&quot;&gt;&lt;a href=&quot;#Ensemble-2-points&quot; class=&quot;headerlink&quot; title=&quot;Ensemble(2 points)&quot;&gt;&lt;/a&gt;Ensemble(2 points)&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Ensemble analysis is used to reduce the bias or variance of the classification process. Which of them is reduced by bagging/boosting?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt; 解答：&lt;/p&gt;
&lt;p&gt; a) Bagging aims to reduce the: &lt;strong&gt;variance&lt;/strong&gt;(代表性算法：随机森林，随机化的决策树模型，每次随机选择一定大小的特征)&lt;/p&gt;
&lt;p&gt; b) Boosting aims to reduce the:&lt;strong&gt;bias&lt;/strong&gt;(代表性算法：AdaBoost，每个训练实例都有个权重，分类错误的实例会赋予一个更大的权重)&lt;/p&gt;
&lt;h2 id=&quot;Ridge-Regression-5-points&quot;&gt;&lt;a href=&quot;#Ridge-Regression-5-points&quot; class=&quot;headerlink&quot; title=&quot;Ridge Regression(5 points)&quot;&gt;&lt;/a&gt;Ridge Regression(5 points)&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Given a set of training data $(x_1,y_1),\ldots,(x_n,y_n)$, where $x_i \in \mathcal{R}^d$ and $y_i \in \mathcal{R}$. Our goal is to learn a linear model $f(x) = x^Tw + b$ to predict the label $y \in \mathcal{R}$ of an instance $x \in \mathcal{R}^d$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;a) Show the optimization problem of the ridge regression for learning $w \in \mathcal{R}^d$ and $b \in \mathcal{R}$.
Notations:$X = [x_1,\ldots,x_n] \in \mathcal{R}^{d*n}, y = [y_1, \ldots, y_n]$&lt;/p&gt;
&lt;p&gt;b) Derive the optimal solution $w_*$, and $b_*$ of the above problem.
Notations: I is the identity matrix, and $H = I - \frac{1}{n}1_n1_n^T$ is the centering matrix. Hints:$\frac{\partial \Vert u - A^Tw \Vert_2^2}{\partial w} = 2A(A^Tw-u)$&lt;/p&gt;
&lt;p&gt;解答：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;优化问题：$\min_{b \in R,w \in R^d} \Vert y - Xw - 1_Nb \Vert_2^2 + \lambda \Vert w \Vert_2^2$, 其中：$1_N = [1,\ldots,1]^T \in R^d$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;对上式$b$进行求导，并令倒数为0：$-2*1_N^T(y-Xw -1_Nb)=0, b  = \frac{1}{N}1_N^T(y-Xw)$&lt;/li&gt;
&lt;li&gt;对上式$w$进行求导，并令倒数为0：$2*X^T(Xw -y + 1_Nb) + 2 \lambda w=0, (X^T(I-\frac{1}{N}1_N1_N^T)X + \lambda I)w=X^T(I-\frac{1}{N}1_N1_N^T)y$&lt;/li&gt;
&lt;li&gt;令$H = I - \frac{1}{N}1_N1_N^T$为中心矩阵，则得到：$w_* = (X^THX + \lambda I)^{-1}X^THy,b_* = \frac{1}{N}1_N^T(y-Xw_*)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;Advanced-Classification-4-points&quot;&gt;&lt;a href=&quot;#Advanced-Classification-4-points&quot; class=&quot;headerlink&quot; title=&quot;Advanced Classification(4 points)&quot;&gt;&lt;/a&gt;Advanced Classification(4 points)&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Give a brief introduction of &lt;strong&gt;semi-suppervised learning&lt;/strong&gt; and &lt;strong&gt;active learning&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：半监督学习和主动学习&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;semi-suppervised learning&lt;/strong&gt;：&lt;ul&gt;
&lt;li&gt;标记数据的代价昂贵并且难于获得；无标签数据通常是大量可得到的；无标签数据是有用的：无标签数据可以用来评估数据的低维流型结构和特征的联合概率分布。&lt;/li&gt;
&lt;li&gt;相关算法：&lt;ul&gt;
&lt;li&gt;元算法：使用任何现有的算法作为子程序，这类算法主要有Self-training和Co-training。&lt;/li&gt;
&lt;li&gt;具体算法：半监督贝叶斯分类器，转导支持向量机，基于图的半监督学习算法。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;数据要求：数据的类别特征应该近似和它的聚类特征匹配；在实际应用中，当有标签的数据非常少时效率很高。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;active learning&lt;/strong&gt;：&lt;ul&gt;
&lt;li&gt;主动学习是半监督机器学习的一个特例，在主动学习中，一个学习算法可以主动地提出一些标注请求，将一些经过筛选的数据提交给专家进行标注。&lt;/li&gt;
&lt;li&gt;两种查询系统：选择性取样和基于池的采样。&lt;/li&gt;
&lt;li&gt;种类：基于异构的模型、基于性能的模型、基于代表的模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Collaborative-Filtering-5-points&quot;&gt;&lt;a href=&quot;#Collaborative-Filtering-5-points&quot; class=&quot;headerlink&quot; title=&quot;Collaborative Filtering(5 points)&quot;&gt;&lt;/a&gt;Collaborative Filtering(5 points)&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;A merchant has an n*d ratings matrix D representing the preferences of n customers across d items. It is assumed that the matrix is sparse, and therefore each customer may have bought only a few items. Please provide one approach that the utilizes the rating matrix D to make recommendations to customers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;解答：总的方法如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于邻居的方法：基于用户的评级相似性、基于商品的评级相似性&lt;/li&gt;
&lt;li&gt;基于图的方法：&lt;/li&gt;
&lt;li&gt;聚类方法：自适应k-means聚类，自适应协同聚类&lt;/li&gt;
&lt;li&gt;潜伏因子模型：奇异值分解、矩阵分解、矩阵填充&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本题使用Matrix Completion方法：$\min_{X \in R^{n*d}} \Vert X \Vert_*, s.t. \ X_{ij} = D_{ij} \in \Omega$。对矩阵D进行低秩分解，用$U*V^T$来逼近D，用于填充，其中$D \in R^{n*d}, U \in R^{n*r}, V \in R^{d*r}$, 即$U*V^T$得到近似矩阵M来填充D上的缺失值。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;2015年秋南京大学计算机系数据挖掘期末试卷分析。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Course" scheme="http://keyunluo.github.io/categories/Course/"/>
    
    
      <category term="数据挖掘" scheme="http://keyunluo.github.io/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title>分布式系统——课程总结</title>
    <link href="http://keyunluo.github.io/2016/12/23/Course/distributed-system-1.html"/>
    <id>http://keyunluo.github.io/2016/12/23/Course/distributed-system-1.html</id>
    <published>2016-12-23T06:25:02.000Z</published>
    <updated>2017-07-06T08:08:06.890Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;2016年秋南京大学计算机系分布式系统课程总结。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;分布式系统模型&quot;&gt;&lt;a href=&quot;#分布式系统模型&quot; class=&quot;headerlink&quot; title=&quot;分布式系统模型&quot;&gt;&lt;/a&gt;分布式系统模型&lt;/h2&gt;&lt;p&gt;1 分布式系统定义：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一个分布式系统是一些独立的计算机集合，但是对这个系统的用户来说，系统就像一台计算机一样&lt;/strong&gt;。 这个定义有两方面的含义：第一，从硬件角度来讲，每台计算机都是自主的；第二，从软件角度来讲，用户将整个系统看做是一台计算机。这两者都是必需的，缺一不可。&lt;/p&gt;
&lt;p&gt;2 涉及到的技术：网络、处理器、内存、存储、协议。&lt;/p&gt;
&lt;p&gt;3 需要分布式的原因&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;相对于集中系统，分布式系统的优点&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;优点&lt;/th&gt;
&lt;th&gt;解释&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Economics（经济性）&lt;/td&gt;
&lt;td&gt;微处理器能提供比大型机更好的性价比&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Speed（速度）&lt;/td&gt;
&lt;td&gt;分布式系统能提供比大型机更强的计算能力&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Inherent distribution（固有的分布性）&lt;/td&gt;
&lt;td&gt;有一些应用包含物理上分布的机器&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reliability（可靠性）&lt;/td&gt;
&lt;td&gt;当某台机器崩溃时，整个系统仍能正常工作&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Incremental growth（可扩展性）&lt;/td&gt;
&lt;td&gt;计算能力逐步增加&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;相对于独立的PC，分布式系统的优点&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;优点&lt;/th&gt;
&lt;th&gt;解释&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;数据共享&lt;/td&gt;
&lt;td&gt;允许用户共享一个数据库&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;外设共享&lt;/td&gt;
&lt;td&gt;允许用户共享昂贵的外设，如彩色打印机&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;通信&lt;/td&gt;
&lt;td&gt;使得个人与个人之间的通信更为方便，如Email&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;灵活性&lt;/td&gt;
&lt;td&gt;将工作负载更有效的分派到合适的机器上&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;缺点&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;缺点&lt;/th&gt;
&lt;th&gt;解释&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;软件&lt;/td&gt;
&lt;td&gt;分布式系统的软件开发困难&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;通信网络&lt;/td&gt;
&lt;td&gt;网络可能饱和或有损传输&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;安全&lt;/td&gt;
&lt;td&gt;数据共享造成机密数据容易被窃取&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;4 分布式系统的目标&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;资源可用性：系统资源被所有计算机共享&lt;/li&gt;
&lt;li&gt;分布透明:Access,Location,Migration,Relocation,Replication,Concurrency,Failure&lt;/li&gt;
&lt;li&gt;开放性：能够与其他开放系统进行服务交互，能够使分布式系统独立于底层环境的异构性&lt;/li&gt;
&lt;li&gt;可扩展：用户、处理可扩展（大小扩展性），节点之间最大距离（地理可扩展性），管理主机的数目（管理可扩展性）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;5 分布式系统透明性和开放性&lt;/p&gt;
&lt;p&gt;透明性：将它的进程和资源实际上在多台计算机上分布这样一个事实隐藏起来，如果一个分布式系统能够在用户和应用程序面前呈现为单个计算机系统，这样的分布式系统就称为是透明的。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;访问透明性：对不同数据表示形式以及资源访问方式的隐藏。&lt;/li&gt;
&lt;li&gt;位置透明性：用户无法判别资源在系统中的物理位置。&lt;/li&gt;
&lt;li&gt;迁移透明性：如果一个分布式系统中的资源移动不会影响该资源的访问方式，就可以说这种分布式系统能够提供迁移透明性。&lt;/li&gt;
&lt;li&gt;重定位透明性：如果资源可以在接受访问的同时进行重新定位，而不引起用户和应用程序的注意，拥有这种资源的系统无疑会更加强壮。&lt;/li&gt;
&lt;li&gt;复制透明性：对同一个资源存在多个副本这样一个事实的隐藏。&lt;/li&gt;
&lt;li&gt;并发透明性：让任何一个用户都不会感觉到他人也在使用自己所使用的资源。&lt;/li&gt;
&lt;li&gt;故障透明性：意味着用户不会注意到某个资源无法正常工作，以及系统随后从故障中恢复的过程。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;开放性：一个开放的分布式系统，它根据一系列准则来提供服务，这些准则描述了所提供服务的语法和语义。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;能够与其他通过服务交互而不用考虑底层环境：&lt;ul&gt;
&lt;li&gt;符合明确定义的接口&lt;/li&gt;
&lt;li&gt;支持应用的可移植性&lt;/li&gt;
&lt;li&gt;易于互操作&lt;/li&gt;
&lt;li&gt;屏蔽底层环境的区别&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;开放性要求分布式系统的策略与机制分离：&lt;ul&gt;
&lt;li&gt;单一系统趋向于封闭&lt;/li&gt;
&lt;li&gt;为分离提供相对应的开放接口&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;6 分布式系统类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分布式计算系统：集群计算、网格计算&lt;/li&gt;
&lt;li&gt;分布式信息系统：事务处理系统（ACID:原子性、一致性、隔离性、持久性）&lt;/li&gt;
&lt;li&gt;分布式普适系统：普适计算、移动计算、传感器网络&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;分布式系统架构&quot;&gt;&lt;a href=&quot;#分布式系统架构&quot; class=&quot;headerlink&quot; title=&quot;分布式系统架构&quot;&gt;&lt;/a&gt;分布式系统架构&lt;/h2&gt;&lt;p&gt;1 分布式系统架构风格&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;组织成逻辑上的不同组件，并分发这些组件到各个机器上：分层样式用于客户端-服务器系统、基于对象样式的分布式对象系统&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;空间(匿名)和时间(异步)的解耦过程导致多种风格：发布/订阅模式(空间解耦)、共享空间(时间和空间解耦)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;或：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分层体系结构&lt;/li&gt;
&lt;li&gt;基于对象的体系结构&lt;/li&gt;
&lt;li&gt;以数据为中心的体系结构&lt;/li&gt;
&lt;li&gt;基于事件的体系结构&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2 分布式系统组织形式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中心化：&lt;ul&gt;
&lt;li&gt;基本的客户端-服务器模式：服务器提供服务，客户端使用服务，客户端和服务器端可以位于不同的机器上，客户端遵从请求/响应模式来使用服务。&lt;/li&gt;
&lt;li&gt;多客户端-服务器模式：服务器形成瓶颈，容易产生单点故障，系统调用困难。&lt;/li&gt;
&lt;li&gt;多客户端-多服务器：Web代理，Web Applets&lt;/li&gt;
&lt;li&gt;应用分层(应用于传统分布式信息系统)：传统三层视图：用户接口层，处理层，数据层&lt;/li&gt;
&lt;li&gt;多层架构(User interface, Application, DataBase)：单层(哑终端/主机配置)、两层(客户端-单服务器配置)、三层(每一层都在一个独立的机器上)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;去中心化：&lt;ul&gt;
&lt;li&gt;结构化的P2P系统：将节点组织成一个结构化覆盖网络，例如一个逻辑环或超立方体，并创建负责仅基于其ID的服务的特定节点。&lt;/li&gt;
&lt;li&gt;非结构化的P2P系统：组织成一个随机的覆盖层，两个节点连接的概率是p,因而不能够确定性地查找信息，而不不得不求助于搜索：洪泛发送信息到周围节点；以及随机游走：随机选择一个邻居。&lt;/li&gt;
&lt;li&gt;混合P2P：有时选择一些特殊节点来做一些特殊事务(保存索引，监控网络状态，建立连接)有助于系统性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;混合模式：客户端-服务器端结合P2P；边缘服务器架构，通常用来实现内容分发网络。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3 分布式系统组织为中间件&lt;/p&gt;
&lt;p&gt;在通常情况下，分布式系统/应用依照具体的架构风格开发，所选择的分布式系统架构风格在所有的案例下可能不是最佳的，因而需要动态地适应中间件的行为。&lt;/p&gt;
&lt;p&gt;拦截器：当激发一个远程调用对象时拦截普通的控制流。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-12/middleware.png&quot; alt=&quot;分布式中间件&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;进程与线程&quot;&gt;&lt;a href=&quot;#进程与线程&quot; class=&quot;headerlink&quot; title=&quot;进程与线程&quot;&gt;&lt;/a&gt;进程与线程&lt;/h2&gt;&lt;p&gt;1 进程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;进程： 进程状态上下文中的执行流&lt;/li&gt;
&lt;li&gt;执行流：执行指令流、运行的一块代码、顺序指令序列、控制的线程&lt;/li&gt;
&lt;li&gt;进程状态：运行的代码能够影响或被影响的一切&lt;/li&gt;
&lt;li&gt;进程与程序：程序是静态的代码和静态的数据，进程是代码和数据的动态实例，进程和代码间不存在一对一的映设，同一份程序能对应多个进程实例，一个程序能调用多个进程。&lt;/li&gt;
&lt;li&gt;进程运行过程中的三种状态：运行态(仅在CPU里的一个核里)、准备状态(等待CPU)、阻塞状态(睡眠状态，等待IO或同步来完成)&lt;/li&gt;
&lt;li&gt;进程的性能低下：资源管理(创建进程需要分配空间，拷贝数据)、调度(上下文切换：CPU和存储上下文)、协作(IPC,内部通信，共享内存)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2 线程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线程：是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。&lt;/li&gt;
&lt;li&gt;上下文切换：处理器上下文(寄存器：栈指针，地址寄存器，程序计数器)、线程上下文(寄存器和内存中：处理器上下文，状态)、进程上下文(寄存器和内存中：线程上下文，MMU寄存器值)&lt;/li&gt;
&lt;li&gt;上下文切换：线程共享地址空间，线程上传下文切换能够独立于操作系统完成；进程切换上下文的代价更高，涉及到在循环中获取系统信息，陷入到系统内核；创建和销毁线程的代价比进程要便宜。&lt;/li&gt;
&lt;li&gt;用户级线程：所有的线程在用户地址空间创建，优点是所有的操作能在单个进程中完全处理，实现起来效率高，缺点是很难获取系统以及块的支持。&lt;/li&gt;
&lt;li&gt;系统级线程：系统内核包含线程包的实现，所有的操作都返回一个系统调用：阻塞一个线程的操作将不再是问题，处理外部事件变得简单，缺点是损失一些效率，因为每一个线程操作都陷入到内核里面了。&lt;/li&gt;
&lt;li&gt;总结：尽量融合用户级和内核级线程到一个概念中，然而然而性能的增加并没有抵消复杂度的增加。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3 线程和分布式系统&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隐藏了网络延时：多个线程预取文件同时响应一个HTTP请求&lt;/li&gt;
&lt;li&gt;对其他机器的多个请求-响应调用(RPC):客户端同时做多个调用，每个调用由一个不同的线程完成&lt;/li&gt;
&lt;li&gt;提高性能：启动线程比启动进程代价低，单个线程服务器禁止简单扩容到一个多处理器系统，客户端：当前一个请求应用后通过响应下一个请求隐藏了网络延时&lt;/li&gt;
&lt;li&gt;更好的结构：IO请求多时能有效简化整个结构，由于简化的控制流多线程程序会更小和更容易理解&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;4 多线程下的分布式C/S架构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;客户端软件：完全透明:获取透明性、位置/迁移透明性、错误透明性、复制透明性&lt;/li&gt;
&lt;li&gt;服务端软件：端口和服务间一对一映设，组织成dispatcher/worker模型&lt;/li&gt;
&lt;li&gt;服务器类型：超级server：监听多个端口，提供多个独立的服务&lt;/li&gt;
&lt;li&gt;有状态：保持跟踪客户端状态：记录文件打开情况以及缓存情况，性能好，客户端能保存本地缓存&lt;/li&gt;
&lt;li&gt;无状态：服务器不保存客户端的处理请求状态，客户端和服务器端完全独立，它们之间的状态不一致性以及冲突避免了，可能丢失一些性能&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-12/multithreadedserver.png&quot; alt=&quot;多线程服务器&quot;&gt;&lt;/p&gt;
&lt;p&gt;5 代码迁移：代码段、数据段、执行状态&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;强弱移动性：&lt;ul&gt;
&lt;li&gt;仅移动代码段和数据段：相对简单，区分代码推送和拉取&lt;/li&gt;
&lt;li&gt;移动所有组件，包括执行状态：迁移：将整个对象从一台机器迁移到另一台机器；拷贝：开始拷贝，并设置成相同的执行状态&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;管理本地资源&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用本地资源的对象的资源在目标点可能不可用&lt;/li&gt;
&lt;li&gt;资源类型：固定资源(资源不能迁移，如本地硬件)，紧固资源(理论上能够迁移，但代价高昂)，未附加的(能轻易地移动资源，例如缓存)&lt;/li&gt;
&lt;li&gt;对象和资源间的绑定：identifier(对象需要一个具体资源的实例，如特定的数据库)、value(对象需要资源的内容，如缓存的入口)、
type(仅需要一种类型的资源是可用的，如彩色显示器)
&lt;img src=&quot;/resource/blog/2016-12/localresources.png&quot; alt=&quot;管理本地资源&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;异构系统中的迁移问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;目标机器可能不适合执行迁移的代码&lt;/li&gt;
&lt;li&gt;进程/线程/处理器上下文强烈依赖于本地硬件，操作系统和运行系统&lt;/li&gt;
&lt;li&gt;解决方案：充分利用在不同平台上的抽象机器的实现&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;通信&quot;&gt;&lt;a href=&quot;#通信&quot; class=&quot;headerlink&quot; title=&quot;通信&quot;&gt;&lt;/a&gt;通信&lt;/h2&gt;&lt;p&gt;1 通信的类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;瞬间通信(数据无法传送就丢弃)与持久通信(数据被暂存在通行服务器上直到被传送出去)、异步通信与同步通信&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2 远程过程调用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RPC的工作过程：客户端过程调用客户端stub;Stub 构建消息，调用本地OS；OS发送消息到远程OS；远程OS接收消息至stub；Stub解包参数并调用服务器；服务器生成本地调用并返回结果至stub；Stub构建消息，调用OS；OS发送消息到客户端OS；客户端OS传送消息至stub；客户端Stub解压结果并返回给客户端。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;故障处理：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;客户端无法定位服务器：可能原因：服务器关闭，服务器使用了新版本的存根而客户端使用了较旧版本的存根；解决方法：使用一个特殊码，例如-1作为过程的返回值来指示失败；让错误触发一个异常或信号。&lt;/li&gt;
&lt;li&gt;客户端向服务器端发送的请求消息丢失：当发送一个消息的时候内核开启一个计时器：在响应或ACK到来之前定时器过期，则内核重发；如果消息真的丢失：服务器不会区分原始和重传的消息，一切正常；如果许多请求都丢失，内核放弃并假装服务器已经关闭了。&lt;/li&gt;
&lt;li&gt;服务器端向客户端发送的应答消息丢失：当发送一个消息的时候内核开启一个计时器：在回复到来之前定时器过期则重传请求；如果服务器速度很慢，则该过程被执行多次；使用序列数字标志请求号，让服务器内核区分是重传消息还是原始消息。&lt;/li&gt;
&lt;li&gt;服务器端接收到一个消息后崩溃：等待服务器重启后继续重复该操作；立即放弃并报告故障；客户端得不到任何帮助。&lt;/li&gt;
&lt;li&gt;客户端发送请求后崩溃：客户端发送请求后崩溃：客户端stub发送RPC之前先写log，重启后检查log然后杀死孤立进程；把时间划分成连续的时隙，客户端重启后广播一个新的时隙，所有的计算都被杀死；每一个RPC都给定了一个标准的时间T来完成工作，如果不能完成则需要明确地向其他法官申请额外配额。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;动态绑定&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;优点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;灵活性&lt;/li&gt;
&lt;li&gt;能支持多个支持同一接口的服务器&lt;ul&gt;
&lt;li&gt;绑定程序(binder)可以随机地将服务器上的客户端传播到均匀负载&lt;/li&gt;
&lt;li&gt;绑定程序(binder)可以定期轮询服务器，自动取消注册失败的服务器，以达到一定的容错能力&lt;/li&gt;
&lt;li&gt;绑定程序(binder)可以帮助身份验证&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;绑定程序(binder)可以验证客户端和服务器都使用相同版本的接口&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;缺点&lt;ul&gt;
&lt;li&gt;导出/导入接口的额外开销花费时间&lt;/li&gt;
&lt;li&gt;绑定程序(binder)可能成为大型分布式系统中的瓶颈&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3 基于消息的通信&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;持久性/非持久性通信&lt;ul&gt;
&lt;li&gt;持久性通信：通信机制本身会对消息进行持久存储，直到它被传递给目的。&lt;/li&gt;
&lt;li&gt;非持久性通信：消息的发送者和接收者必须同时存在才能进行，传输服务仅仅提供临时的对消息的存储。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;同步/异步&lt;ul&gt;
&lt;li&gt;同步通信：发信方在到达同步点前保持阻塞&lt;/li&gt;
&lt;li&gt;异步通信：发信方发信后立即继续，消息存储在发信方主机或者通信服务器的缓冲区中&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;流数据&lt;ul&gt;
&lt;li&gt;流式数据：一组消息按照指定的顺序排序后构成一个有意义的数据流&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;命名&quot;&gt;&lt;a href=&quot;#命名&quot; class=&quot;headerlink&quot; title=&quot;命名&quot;&gt;&lt;/a&gt;命名&lt;/h2&gt;&lt;p&gt;1 命名方式&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Human-friendly name&lt;/li&gt;
&lt;li&gt;Address&lt;/li&gt;
&lt;li&gt;Identifier&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2 移动实体的定位&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;广播(Broadcasting)&lt;/li&gt;
&lt;li&gt;转发指针(Forwarding Pointers)&lt;/li&gt;
&lt;li&gt;基于家(Home-based)的方法&lt;/li&gt;
&lt;li&gt;分布式哈希表(Distributed Hash Table)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;3 命名解析&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;迭代命名解析：每次解析一个节点以后都会把结果返回给客户名称解析程序，然后将由客户重新向下一级服务器发送请求。&lt;/li&gt;
&lt;li&gt;递归命名解析：直接把解析的中间结果传递给下一级服务器，并且逐级解析直到返回指定文件结果：要求名称服务器都具有较高性能，增加了额外负担，减少通信开销，而且可以更好地使用缓存来提高性能&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;同步与资源管理&quot;&gt;&lt;a href=&quot;#同步与资源管理&quot; class=&quot;headerlink&quot; title=&quot;同步与资源管理&quot;&gt;&lt;/a&gt;同步与资源管理&lt;/h2&gt;&lt;p&gt;1 同步问题&lt;/p&gt;
&lt;p&gt;2 时钟同步机制&lt;/p&gt;
&lt;p&gt;3 逻辑时钟&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lamport时钟&lt;/li&gt;
&lt;li&gt;向量时戳&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;4 分布式系统中的互斥访问&lt;/p&gt;
&lt;p&gt;5 分布式系统中的选举机制&lt;/p&gt;
&lt;h2 id=&quot;复制与一致性&quot;&gt;&lt;a href=&quot;#复制与一致性&quot; class=&quot;headerlink&quot; title=&quot;复制与一致性&quot;&gt;&lt;/a&gt;复制与一致性&lt;/h2&gt;&lt;p&gt;1 复制的优势与不足&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2 数据一致性模型&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;3 数据一致性协议&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于法定数量的一致性&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;容错&quot;&gt;&lt;a href=&quot;#容错&quot; class=&quot;headerlink&quot; title=&quot;容错&quot;&gt;&lt;/a&gt;容错&lt;/h2&gt;&lt;p&gt;1 可信系统的特征&lt;/p&gt;
&lt;p&gt;2 提高系统可信性的途径&lt;/p&gt;
&lt;p&gt;3 K容错系统&lt;/p&gt;
&lt;p&gt;4 拜占庭问题&lt;/p&gt;
&lt;p&gt;5 系统恢复&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;回退恢复&lt;/li&gt;
&lt;li&gt;前向恢复&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;6 检查点&lt;/p&gt;
&lt;h2 id=&quot;云计算&quot;&gt;&lt;a href=&quot;#云计算&quot; class=&quot;headerlink&quot; title=&quot;云计算&quot;&gt;&lt;/a&gt;云计算&lt;/h2&gt;&lt;h2 id=&quot;OpenStack&quot;&gt;&lt;a href=&quot;#OpenStack&quot; class=&quot;headerlink&quot; title=&quot;OpenStack&quot;&gt;&lt;/a&gt;OpenStack&lt;/h2&gt;</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;2016年秋南京大学计算机系分布式系统课程总结。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Course" scheme="http://keyunluo.github.io/categories/Course/"/>
    
    
      <category term="分布式系统" scheme="http://keyunluo.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘——课程总结</title>
    <link href="http://keyunluo.github.io/2016/12/22/Course/data-mining-summary.html"/>
    <id>http://keyunluo.github.io/2016/12/22/Course/data-mining-summary.html</id>
    <published>2016-12-22T06:25:02.000Z</published>
    <updated>2017-07-06T08:08:06.890Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;2016年秋南京大学计算机系数据挖掘课程学期总结。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;降维&quot;&gt;&lt;a href=&quot;#降维&quot; class=&quot;headerlink&quot; title=&quot;降维&quot;&gt;&lt;/a&gt;降维&lt;/h2&gt;&lt;h3 id=&quot;Distance-Measure&quot;&gt;&lt;a href=&quot;#Distance-Measure&quot; class=&quot;headerlink&quot; title=&quot;Distance Measure&quot;&gt;&lt;/a&gt;Distance Measure&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;$L_p-Norm$&lt;/strong&gt;(Minkowski distance)&lt;/p&gt;
&lt;p&gt;形式：给定$\bar{X}=(x_1,x_2,\ldots, x_d), \bar{Y}=(y_1,y_2,\ldots, y_d)$, 他们之间的Lp范数距离为：
$$Dist(\bar{X}, \bar{Y}) = \left( \sum_{i=1}^d \vert x_i - y_i |^p \right )^{1/p}$$&lt;/p&gt;
&lt;p&gt;特例：p=1时为曼哈顿距离：各分量绝对值之和；p=2时为欧式距离：根号下和的平方；$p=\infty$时为无穷范数，又称为切比雪夫距离 ：绝对值最大的数；p=0时为0范数：非零元素个数，非凸；0&amp;lt;p&amp;lt;1时为分数范数，非凸。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Standardized Euclidean distance&lt;/p&gt;
&lt;p&gt;  标准化欧式距离，两个分量间减去均值除以标准差进行标准化：&lt;/p&gt;
&lt;p&gt;  $$Dist(\bar{X}, \bar{Y}) = \left( \sum_{i=1}^d \vert \frac{x_i - y_i}{s_i} |^2 \right )^{1/2}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Mahalanobis Distance&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;  马氏距离用来消除不同维度之间的相关性和和尺度不同的性质。样本矩阵$X$的协方差矩阵为$\Sigma$,则它的各个向量间的马氏距离为：&lt;/p&gt;
&lt;p&gt;  $$Dist(X_i, X_j) = \sqrt{(X_i - X_j)^T\Sigma^{-1}(X_i-X_j)}$$&lt;/p&gt;
&lt;p&gt;  若对协方差矩阵进行正交投影分解，即：$\Sigma = U \Lambda U^T = \sum_{i=1}^{d} \sigma_i u_iu_i^T$, 那么，$\Sigma^{-1} = U \Lambda U^T= \sum_{i=1}^{d} \sigma_i^{-1} u_iu_i^T$, 马氏距离可表示成如下：&lt;/p&gt;
&lt;p&gt;  $$Dist(X_i, X_j) = \sqrt{(X_i - X_j)(\sum_{i=1}^d \sigma_i^{-1}u_iu_i^T)(X_i - X_j)^T} = \sqrt{\sum_{i=1}^d \frac{((X_i - X_j)u_i)^2}{\sigma_i}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Cosine&lt;/p&gt;
&lt;p&gt;  夹角余弦越大表示两个向量的夹角越小,越相似，夹角余弦越小表示两向量的夹角越大，越不相似。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。&lt;/p&gt;
&lt;p&gt;  $$\cos(\theta) = \frac{\sum_{i=1}^d x_iy_i}{\sqrt{\sum_{i=1}^d x_i^2} \sqrt{\sum_{i=1}^d y_i^2}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hamming distance&lt;/p&gt;
&lt;p&gt;  两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Jaccard similarity coefficient&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;杰卡德相似系数：两个集合A和B的交集元素在A，B的并集中所占的比例: $J(A,B) = \frac{A \bigcap B}{A \bigcup B}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;杰卡德距离:两个集合中不同元素占所有元素的比例， $J_\delta(A,B) = 1 - \frac{A \bigcap B}{A \bigcup B}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Correlation coefficient And Correlation distance&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;相关系数：相关系数是衡量随机变量X与Y相关程度的一种方法，相关系数的取值范围是[-1,1]。相关系数的绝对值越大，则表明X与Y相关度越高。当X与Y线性相关时，相关系数取值为1（正线性相关）或-1（负线性相关）。$\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}} = \frac{E((X-E(X))(Y-E(Y)))}{\sqrt{D(X)} \sqrt{D(Y)}}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;相关距离：$D_{xy} = 1 - \rho_{XY}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Information Entropy&lt;/p&gt;
&lt;p&gt;  信息熵是衡量分布的混乱程度或分散程度的一种度量。$Entropy(X) = - \sum_{i=1}^{n} p_i \log_2 p_i$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Nonlinear Distributions: ISOMAP&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对每个点计算其k个近邻&lt;/li&gt;
&lt;li&gt;构造一个带权图G，结点代表数据点，边的权值代表k个近邻间的欧式距离。&lt;/li&gt;
&lt;li&gt;计算任意两点间的距离$Dist(\bar{X}, \bar{Y})$为图中点$\bar{X}, \bar{Y}$之间的最短距离。&lt;/li&gt;
&lt;li&gt;计算 MDS(multidimensional scaling)：得到图的向量表示，即映设矩阵。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;PCA&quot;&gt;&lt;a href=&quot;#PCA&quot; class=&quot;headerlink&quot; title=&quot;PCA&quot;&gt;&lt;/a&gt;PCA&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;轴旋转： $x = [x^1, x^2, \ldots, x^d]^T \in \mathcal{R}^d \Leftrightarrow x = x^1e_1 + x^2e_2 + \ldots + x^de_d$，W:正交矩阵，$W = [w_1, w_2, \ldots, w_d]$, 则$x = WW^Tx = (\sum_{i=1}^dw_iw_i^T)x = \sum_{i=1}^dw_i(w_i^Tx) = (w_1^Tx)w_1 + \ldots + (w_d^Tx)w_d$, 因而，在坐标W的投影下，新坐标是$y = [w_1^Tx, w_2^Tx, \ldots, w_d^Tx]^T \in \mathcal{R}^d$. $w_i^T = &lt;w\_i, x=&quot;&quot;&gt;$表示新的坐标，含义是将$x$沿着$w_i$的方向投影。&lt;/w\_i,&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;降维目标：给定数据点集合${x_1,x_2, \ldots, x_n}, x_i \in \mathcal{R}^d$，寻找一个投影方向${w_1,w_2,\ldots,w_k}$，使得方差$y_1 = [w_1^Tx_1, w_2^Tx_1,\ldots, w_k^Tx_1]^T, y_2 = [w_1^Tx_2, w_2^Tx_2,\ldots, w_k^Tx_2]^T, \ldots, y_n = [w_1^Tx_n, w_2^Tx_n,\ldots, w_k^Tx_n]^T$ 最大，即最大化类间方差。&lt;/p&gt;
&lt;p&gt;考虑一维的情况，形式化为：新坐标：$w_1^Tx_1,w_1^Tx_2, \ldots, w_1^Tx_n$,方差：$\frac{1}{n} \sum_{i=1}^n(w_1^Tx_i - \mu )^2, \mu = \frac{1}{n} \sum_{i=1}^nw_1^Tx_i$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;问题推导过程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;令$\bar{x} = \frac{1}{n} \sum_{i=1}^nx_i$为均值向量&lt;/li&gt;
&lt;li&gt;&lt;p&gt;那么，$\mu = w_1^T\bar{x}$,&lt;/p&gt;
&lt;p&gt;  $$\begin{array} {lcl}
  \frac{1}{n} \sum_{i=1}^n(w_1^Tx_i - \mu)^2 &amp;amp; = &amp;amp; \frac{1}{n} \sum_{i=1}^n(w_1^Tx_i - w_1^T\bar{x})^2 \\
  &amp;amp; = &amp;amp; \frac{1}{n} \sum_{i=1}^n(w_1^T(x_i - \bar{x}))^2 \\
  &amp;amp; = &amp;amp; \frac{1}{n} \sum_{i=1}^nw_1^T(x_i - \bar{x})(x_i - \bar{x})^Tw_1 \\
  &amp;amp; = &amp;amp; w_1^T(\frac{1}{n} \sum_{i=1}^n(x_i - \bar{x})(x_i - \bar{x})^T)w_1
  \end{array}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;于是， PCA转变成一个优化问题：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$\max_{w \in \mathcal{R}^d}\ w^TCw$&lt;/p&gt;
&lt;p&gt;$s.t. \Vert w \Vert_2^2 = 1$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这里， $C = \frac{1}{n}\sum_{i=1}^n(x_i - \bar{x})(x_i - \bar{x})^T)$为协方差矩阵。该协方差矩阵具有如下特征：对称性，半正定，矩阵的秩最大为n-1。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;解决方法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;拉格朗日法：$-w^TCw + \lambda(\Vert w \Vert_2^2 -1)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;对$w$求导，令导数为0：$-2Cw + 2\lambda w  = 0$, 得出$Cw = \lambda w$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$(w, \lambda)$ 是协方差矩阵C的特征向量和特征值&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;目标函数变成：$w^TCw = \lambda w^T w = \lambda$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;因此， 我们选择C中最大的特征值和特征向量&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;PCA算法(1维)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算均值向量：$\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$&lt;/li&gt;
&lt;li&gt;计算协方差矩阵：$C = \frac{1}{n} \sum_{i=1}^n(x_i - \bar{x})(x_i - \bar{x})^T)$&lt;/li&gt;
&lt;li&gt;计算C中最大的特征值对应的特征向量&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;PCA算法(k维)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;优化目标&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$\max_{w \in \mathcal{R}^{d*k}}\ trace(W^TCW)$&lt;/p&gt;
&lt;p&gt;$s.t. W^TW = I$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其中， $W=[w_1, \ldots, w_k]$ 是C的k个大的特征向量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;算法流程&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算均值向量：$\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$&lt;/li&gt;
&lt;li&gt;计算协方差矩阵：$C = \frac{1}{n} \sum_{i=1}^n(x_i - \bar{x})(x_i - \bar{x})^T)$&lt;/li&gt;
&lt;li&gt;计算C中最大的k个特征值对应的特征向量&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;特征值含义&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\lambda_i$是第i个特征坐标间的方差&lt;/li&gt;
&lt;li&gt;度量PCA质量的好坏：所取的前k个特征值之和与所有的特征值和之比。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;另一个思考视角&lt;/strong&gt;：最小化投影误差&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;PCA算法是线性的，无监督的&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;SVD&quot;&gt;&lt;a href=&quot;#SVD&quot; class=&quot;headerlink&quot; title=&quot;SVD&quot;&gt;&lt;/a&gt;SVD&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;奇异值分解: $X=U\Sigma V^T = \sum_{i=1}^d \sigma_iu_iv_i^T$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$U = [u_1, u_2,\ldots, u_d] \in \mathcal{R}^{d*d}, U^TU=UU^T = I$&lt;/li&gt;
&lt;li&gt;$V = [v_1,v_2, \ldots,v_d] \in \mathcal{R}^{n*d}, V^TV=I$&lt;/li&gt;
&lt;li&gt;$\Sigma = diag(\sigma_1, \sigma_2,\ldots, \sigma_d),\in \mathcal{R}^{d*d}, \sigma_1 \ge \sigma_2 \ge \ldots \ge \sigma_d \ge 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;紧凑SVD：$X=U_r\Sigma_r V_r^T = \sum_{i=1}^r \sigma_iu_iv_i^T$， &lt;strong&gt;rank(r) &amp;lt; min(d,n)&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$U_r = [u_1, u_2,\ldots, u_r] \in \mathcal{R}^{d*r}, U_r^TU_r=I$&lt;/li&gt;
&lt;li&gt;$V_r = [v_1,v_2, \ldots,v_r] \in \mathcal{R}^{n*r}, V_r^TV_r=I$&lt;/li&gt;
&lt;li&gt;$\Sigma_r = diag(\sigma_1, \sigma_2,\ldots, \sigma_r),\in \mathcal{R}^{r*r}, \sigma_1 \ge \sigma_2 \ge \ldots \ge \sigma_r \ge 0$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;使用SVD降维&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算X的k个最大的左奇异向量$u_1,u_2,\ldots,u_k$&lt;/li&gt;
&lt;li&gt;x的新坐标：$U_k^Tx = [u_1^Tx, u_2^Tx, \ldots, u_k^Tx] \in \mathcal{R}^k, U_k = [u_1, u_2,\ldots,u_k] \in \mathcal{R}^{d*k}$&lt;/li&gt;
&lt;li&gt;X的新坐标：$U_k^TX=U_k^TU_r\Sigma_rV_r^T = \Sigma_kV_k^T$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;SVD的优化问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一维：&lt;blockquote&gt;
&lt;p&gt;$\max_{w \in \mathcal{R}^d}\ w^T(XX^T)w$&lt;/p&gt;
&lt;p&gt;$s.t. \Vert w \Vert_2^2 = 1$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;k维：&lt;blockquote&gt;
&lt;p&gt;$\max_{w \in \mathcal{R}^{d*k}}\ trace(W^T(XX^T)W)$&lt;/p&gt;
&lt;p&gt;$s.t. W^TW = I$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;PCA by SVD&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算均值向量$\bar{x}$:$ \frac{1}{n} \sum_{i=1}^n x_i$&lt;/li&gt;
&lt;li&gt;计算$\bar{X}=[x_1 - \bar{x},\ldots, x_n - \bar{x}]$最大的k个左奇异值&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;MDS&quot;&gt;&lt;a href=&quot;#MDS&quot; class=&quot;headerlink&quot; title=&quot;MDS&quot;&gt;&lt;/a&gt;MDS&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;输入&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;图G：n个结点&lt;/li&gt;
&lt;li&gt;距离:$\delta_{ij} = \delta_{ji}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;输出&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;适配于距离的坐标集&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;优化问题&lt;/p&gt;
&lt;p&gt; $\min_{x_1,x_2,\ldots, x_n \in \mathcal{R}^k} \sum_{i,j:i&amp;lt;j}(\Vert x_i - x_j \Vert_2 - \delta_{ij})^2$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;MDS算法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算点积：$S= -\frac{1}{2}(I - \frac{np.ones^T}{n})G^2(I - \frac{np.ones^T}{n})$&lt;/li&gt;
&lt;li&gt;对S进行奇异值分解：$S=U \Lambda U^T = \sum_{i=1}^n\lambda_iu_iu_i^T$&lt;/li&gt;
&lt;li&gt;新坐标：$U_k\Lambda_k^{1/2} \in \mathcal{R}^{n*k}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;关联规则挖掘&quot;&gt;&lt;a href=&quot;#关联规则挖掘&quot; class=&quot;headerlink&quot; title=&quot;关联规则挖掘&quot;&gt;&lt;/a&gt;关联规则挖掘&lt;/h2&gt;&lt;h3 id=&quot;The-Frequent-Pattern-Mining-Model&quot;&gt;&lt;a href=&quot;#The-Frequent-Pattern-Mining-Model&quot; class=&quot;headerlink&quot; title=&quot;The Frequent Pattern Mining Model&quot;&gt;&lt;/a&gt;The Frequent Pattern Mining Model&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;U : d个item集合&lt;/li&gt;
&lt;li&gt;T ：n个交易的集合,$T_1,T_2, \ldots, T_n, T_i \in U$&lt;/li&gt;
&lt;li&gt;$T_i$二进制表示&lt;/li&gt;
&lt;li&gt;Itemset,k-itemset: item集合， k个items的集合&lt;/li&gt;
&lt;li&gt;Support(支持度)：包含itemset I的数据集T的一个子集，用sup(I)表示，表示某个item集合在数据表中出现的比例。&lt;/li&gt;
&lt;li&gt;minsup(最小支持度)：预先定义的阈值，只有支持度大于minsup的子集才被视为一个频繁项。&lt;/li&gt;
&lt;li&gt;frequent itemset(频繁项集)：项集X的支持度超过最小门限值minsup时，称X为频繁项集&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;Property&quot;&gt;&lt;a href=&quot;#Property&quot; class=&quot;headerlink&quot; title=&quot;Property&quot;&gt;&lt;/a&gt;Property&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;支持单调性
(Support Monotonicity Property)： $\sup(J) \ge \sup(I), \forall J \in I$, 这就意味着一个itemset I 包含在一个交易中，那么它的所有子集也包含在这个交易中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;向下闭包属性(Downward Closure Property)：每一个频繁项的子集也是一个频繁项。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Association-Rule&quot;&gt;&lt;a href=&quot;#Association-Rule&quot; class=&quot;headerlink&quot; title=&quot;Association Rule&quot;&gt;&lt;/a&gt;Association Rule&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;置信度：$conf(X \Rightarrow Y) = \frac{sup(X \bigcup Y)}{sup(X)}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;关联规则：同时满足最小支持度阈值和最小置信度阈值的规则称为关联规则。这分别保证了有效数量的交易是相关的和在条件概率方面有足够的强度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;一般框架：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;找出所有频繁项集，即候选规则&lt;/li&gt;
&lt;li&gt;对所有候选规则计算置信度，找出其中的关联规则&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;一个直观的实现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;给定一个频繁集I&lt;/li&gt;
&lt;li&gt;产生所有可能的划分X,Y:Y=I-X&lt;/li&gt;
&lt;li&gt;检测置信度：$X \Rightarrow Y$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;置信度单调性：$X_1 \subset X_2 \subset I, conf(X_2 \Rightarrow I - X_2) \ge conf(X_1 \Rightarrow I - X_1)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&quot;Apriori-Algorithm&quot;&gt;&lt;a href=&quot;#Apriori-Algorithm&quot; class=&quot;headerlink&quot; title=&quot;Apriori Algorithm&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://blog.csdn.net/golden1314521/article/details/41457019&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Apriori Algorithm&lt;/a&gt;&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;基本思想：为了减少频繁项集的生成时间，我们应该尽早的消除一些完全不可能是频繁项集的集合，Apriori的两条定律如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果一个集合是频繁项集，则它的所有子集都是频繁项集。举例：假设一个集合{A,B}是频繁项集，即A、B同时出现在一条记录的次数大于等于最小支持度minsup，则它的子集{A},{B}出现次数必定大于等于minsup，即它的子集都是频繁项集。&lt;/li&gt;
&lt;li&gt;如果一个集合不是频繁项集，则它的所有超集都不是频繁项集。举例：假设集合{A}不是频繁项集，即A出现的次数小于minsup，则它的任何超集如{A,B}出现的次数必定小于minsup，因此其超集必定也不是频繁项集。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;算法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用k-itemsets频繁项产生(k+1)-itemsets候选集&lt;/li&gt;
&lt;li&gt;在计数前对候选集剪枝&lt;/li&gt;
&lt;li&gt;对余下的(k+1)-candidates计算支持度&lt;/li&gt;
&lt;li&gt;当(k+1)-candidates中没有频繁项时停止，否则循环
&lt;img src=&quot;/resource/blog/2016-12/apriori.png&quot; alt=&quot;Apriori算法&quot;&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;优化1： Candidates Generation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;思路：U中的Item使用字典序排列，Itemsets按字符串排序&lt;/li&gt;
&lt;li&gt;方法：&lt;ul&gt;
&lt;li&gt;对k-itemsets频繁项排序&lt;/li&gt;
&lt;li&gt;如果两个itemset的前k-1 items相同则合并&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;优化2：Level-wise Pruning Trick&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;令$F_k$为k-itemsets频繁项，$C_{k+1}$为(k+1)-candidates 集&lt;/li&gt;
&lt;li&gt;对于一个集合I:$I \in C_{k+1}$为频繁集当且仅当I中的所有的k-subsets都是频繁项&lt;/li&gt;
&lt;li&gt;剪枝：&lt;ul&gt;
&lt;li&gt;产生I的所有的k-subsets&lt;/li&gt;
&lt;li&gt;如果它们当中的一个不属于$F_k$，那么移除I&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;上述优化后的算法&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将数据库中的事务的数据排序，首先将每条事务记录中多个元素排序，然后将事务整体排序。&lt;/li&gt;
&lt;li&gt;令k=1，扫描数据库，得到候选的1-项集并统计其出现次数，由此得到各个1-项集的支持度，然后根据最小支持度来提出掉非频繁的1-项集进而得到频繁的1-项集。&lt;/li&gt;
&lt;li&gt;令k=k+1.通过频繁（k-1）-项集产生k-项集候选集的方法（也称“连接步”）：如果两个（k-1）-项集，如果只有最后一个元素不同，其他都相同，那么这两个(k-1)-项集项集可以“连接”为一个k-项集。不能连接的就不用考虑了，不会频繁的。&lt;/li&gt;
&lt;li&gt;从候选集中剔除非频繁项集的方法（也称“剪枝步”）：对任一候选集，看其所有子项集(其实只需要对k-2个子项集进行判别即可)是否存在于频繁的(k-1)-项集中，如果不在，直接剔除；扫描数据库，计数，最终确认得到的频繁的k-项集。&lt;/li&gt;
&lt;li&gt;如果得到的频繁的k-项集的数目&amp;lt;=1，则搜索频繁项集的过程结束；否则转到第3步。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;优化3：Support Counting&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;朴素的方法：对于每一个候选集:$I_i \in C_{k+1}$,对于每一个交易$T_j$,检查$T_i$是否在$T_j$出现。&lt;/li&gt;
&lt;li&gt;优化方法：将$C_{k+1}$中的候选集模式组织成一个Hash tree，使用Hash tree加速计数:先将所有的k-阶候选集存储在哈希树的结构的叶节点上，然后对每个transaction记录找到其包含的所有k-阶候选集，所以这个过程只需要浏览一遍数据库。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hash Tree&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1.用k表示插入进行到第几层，初始值为1.&lt;/li&gt;
&lt;li&gt;2.对输入项集的第k项进行hash，得到n&lt;/li&gt;
&lt;li&gt;3.判断当前层的根节点的第n个子节点是否为叶节点，若非则跳到1继续&lt;/li&gt;
&lt;li&gt;4.将项集插入到该叶子节点是否&lt;/li&gt;
&lt;li&gt;5.判断该叶子节点是否已满，若是则进行分裂，否则插入结束，分裂过程是将该节点原有的项集和新项集按第level项进行hash，然后分别插入到下一层的新叶节点中，而原叶节点变为非叶节点&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;基于Hash Tree的计数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于每一个$T_j$，识别那些在Hash Tree中可能包含子集项的叶子节点&lt;/li&gt;
&lt;li&gt;过程：&lt;ul&gt;
&lt;li&gt;根节点：对$T_j$中的所有项进行Hash&lt;/li&gt;
&lt;li&gt;如果在叶子节点上，则寻找$T_j$中所有的子集项&lt;/li&gt;
&lt;li&gt;如果在内部节点，则在给定位置之后Hash每一个项&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;聚类&quot;&gt;&lt;a href=&quot;#聚类&quot; class=&quot;headerlink&quot; title=&quot;聚类&quot;&gt;&lt;/a&gt;聚类&lt;/h2&gt;&lt;h3 id=&quot;K-Means&quot;&gt;&lt;a href=&quot;#K-Means&quot; class=&quot;headerlink&quot; title=&quot;K-Means&quot;&gt;&lt;/a&gt;K-Means&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;思想&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于代表的算法，错误平方和,即类间平方和最小：$\min_{\bar{Y_1},\ldots, \bar{Y_k}} O = \sum_{i=1}^n[\min_j \ Dist(\bar{X_i},\bar{Y_j})]$&lt;/li&gt;
&lt;li&gt;距离度量：$Dist(\bar{X_i}, \bar{Y_j}) = \Vert \bar{X_i} - \bar{Y_j} \Vert_2^2$&lt;/li&gt;
&lt;li&gt;指定聚类：$C_1,\ldots, C_k$&lt;/li&gt;
&lt;li&gt;优化步骤：$\bar{Y_j} = argmin_{\bar{Y}} \sum_{\bar{X_i} \in C_j} \Vert \bar{X_i} - \bar{Y} \Vert_2^2 = \frac{1}{C_j}\sum_{\bar{X_i} \in C_j} \bar{X_i}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;算法流程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为每个聚类确定一个初始聚类中心，这样就有K 个初始聚类中心。&lt;/li&gt;
&lt;li&gt;将样本集中的样本按照最小距离原则分配到最邻近聚类。&lt;/li&gt;
&lt;li&gt;使用每个聚类中的样本均值作为新的聚类中心。&lt;/li&gt;
&lt;li&gt;重复步骤2.3直到聚类中心不再变化。&lt;/li&gt;
&lt;li&gt;结束，得到K个聚类。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;本地马氏距离度量：$Dist(\bar{X_i}, \bar{Y_j}) = (\bar{X_i} - \bar{Y_j} ) \Sigma_r^{-1}(\bar{X_i} - \bar{Y_j})^T$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;基于核的方法&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;k-Medians 算法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用曼哈顿距离：$Dist(\bar{X_i}, \bar{Y_j}) = \Vert \bar{X_i} - \bar{Y_j} \Vert_1 = \sum_{p=1}^d \vert X_i^p - Y_j^p \vert$&lt;/li&gt;
&lt;li&gt;优化步骤：$Y_j^p = argmin_Y \sum_{\bar{X_i} \in C_j} |X_i^p -Y| = median{X_i^p | \bar{X_i} \in C_j}$&lt;/li&gt;
&lt;li&gt;缺点：$\bar{Y} = [Y_j^1,\ldots,Y_j^d]$可能不在数据集合中。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;K-Medoids&quot;&gt;&lt;a href=&quot;#K-Medoids&quot; class=&quot;headerlink&quot; title=&quot;K-Medoids&quot;&gt;&lt;/a&gt;K-Medoids&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;核心：代表点是从数据中选出来的：$\min_{\bar{Y_1},\ldots, \bar{Y_k} \in D} O = \sum_{i=1}^n[\min_j \ Dist(\bar{X_i},\bar{Y_j})]$&lt;/li&gt;
&lt;li&gt;基于爬山法的优化：代表点S初始化为D中的k个点，S通过不断迭代地与D中的点交换来改善。&lt;/li&gt;
&lt;li&gt;交换方法：所有点|S||D|交换；随机选择r对$(\bar{X_i},\bar{Y_j}$点然后选择最好的。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Spectral-Clustering&quot;&gt;&lt;a href=&quot;#Spectral-Clustering&quot; class=&quot;headerlink&quot; title=&quot;Spectral Clustering&quot;&gt;&lt;/a&gt;Spectral Clustering&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;思想：把数据映射到一个新空间,该空间里具有约化的维度,使得相似性更加显而易见,然后对新空间的数据进行聚类。&lt;/li&gt;
&lt;li&gt;优化问题：(k=1)&lt;ul&gt;
&lt;li&gt;$\min_{y \in \mathcal{R}^n} \ y^TLy$&lt;/li&gt;
&lt;li&gt;s.t. $y^TDy = 1$&lt;/li&gt;
&lt;li&gt;解决方案：$Ly = \lambda Dy$, 最小值为$y^1=1$,使用第二最小值$y^2$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;拉普拉斯特征值映设(k&amp;gt;1):&lt;ul&gt;
&lt;li&gt;向量形式：$O = \sum_{i=1}^n\sum_{j=1}^nw_{ij} \Vert y_i - y_j \Vert_2^2 = 2 trace(Y^TLY)$&lt;/li&gt;
&lt;li&gt;$Y = [y_1,\ldots,y_n]^T \in \mathcal{R}^{n*k}$&lt;/li&gt;
&lt;li&gt;$L = D -W \in \mathcal{R}^{n*n}$:图的拉普拉斯表示&lt;/li&gt;
&lt;li&gt;$W=[w_{ij}] \in \mathcal{R}^{n*n}$:相似度矩阵&lt;/li&gt;
&lt;li&gt;$D \in \mathcal{R}^{n*n}$是一个对角矩阵，$D_{ii} = \sum_{j=1}^nw_{ij}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;优化问题：(k&amp;gt;1)&lt;ul&gt;
&lt;li&gt;$\min_{y \in \mathcal{R}^{n*k}} \ trace(Y^TLY)$&lt;/li&gt;
&lt;li&gt;s.t. $Y^TDY = I$&lt;/li&gt;
&lt;li&gt;解决方案：$Ly = \lambda Dy$, 最小值为$y^1=1$,$Y = [y^2,\ldots,y^{k+1}] \in \mathcal{R}^{n*k}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;步骤：&lt;ul&gt;
&lt;li&gt;构建带权图相似度矩阵 W:使用 k 近邻算法,对每个点选取前 k 个邻居为 1,其余为 0,并对称化该矩阵;&lt;/li&gt;
&lt;li&gt;构建拉普拉斯图矩阵 L=D-W,并归一化:构建 W 的对角元素矩阵 D($d_i = \sum_iW_{ij}$), $L = D^{-0.5}LD^{-0.5} = I - D^{-0.5}WD^{-0.5}$；&lt;/li&gt;
&lt;li&gt;特征值分解,得到新的数据:eig_values, eig_vectors = np.linalg.eigh(L),将特征值排序,选取特征值最小的 k 个对应的特征向量列组成显得数据;&lt;/li&gt;
&lt;li&gt;调用 KMedoids 算法&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Non-negative-Matrix-Factorization&quot;&gt;&lt;a href=&quot;#Non-negative-Matrix-Factorization&quot; class=&quot;headerlink&quot; title=&quot;Non-negative Matrix Factorization&quot;&gt;&lt;/a&gt;Non-negative Matrix Factorization&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;优化问题：&lt;ul&gt;
&lt;li&gt;$\min_{U \in \mathcal{R}^{d*k}, V \in \mathcal{R}^{v*k}} \ \Vert X - UV^T \Vert_F^2 $&lt;/li&gt;
&lt;li&gt;$s.t. \ U \ge 0, V \ge 0$&lt;/li&gt;
&lt;li&gt;非负矩阵分解是非凸的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;解释&lt;ul&gt;
&lt;li&gt;矩阵近似：$X  \approx UV^T， x_i \approx Uv_i = \sum_{j=1}^ku_jv_{ij}$&lt;/li&gt;
&lt;li&gt;向量近似：$x_i \approx Uv_i = \sum_{j=1}^ku_jv_{ij}, u_1,\ldots,u_k \in \mathcal{R}^d$可视为基向量。$v_i = [v_{i1},\ldots,v_{ik}]^T$可视为原始$x_i$的新的k维表示。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;分类&quot;&gt;&lt;a href=&quot;#分类&quot; class=&quot;headerlink&quot; title=&quot;分类&quot;&gt;&lt;/a&gt;分类&lt;/h2&gt;&lt;h3 id=&quot;LDA&quot;&gt;&lt;a href=&quot;#LDA&quot; class=&quot;headerlink&quot; title=&quot;LDA&quot;&gt;&lt;/a&gt;LDA&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;两分类问题&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Naive-Bayes&quot;&gt;&lt;a href=&quot;#Naive-Bayes&quot; class=&quot;headerlink&quot; title=&quot;Naive Bayes&quot;&gt;&lt;/a&gt;Naive Bayes&lt;/h3&gt;&lt;h3 id=&quot;SVM&quot;&gt;&lt;a href=&quot;#SVM&quot; class=&quot;headerlink&quot; title=&quot;SVM&quot;&gt;&lt;/a&gt;SVM&lt;/h3&gt;&lt;h3 id=&quot;Logistic-Regression&quot;&gt;&lt;a href=&quot;#Logistic-Regression&quot; class=&quot;headerlink&quot; title=&quot;Logistic Regression&quot;&gt;&lt;/a&gt;Logistic Regression&lt;/h3&gt;&lt;h2 id=&quot;凸优化&quot;&gt;&lt;a href=&quot;#凸优化&quot; class=&quot;headerlink&quot; title=&quot;凸优化&quot;&gt;&lt;/a&gt;凸优化&lt;/h2&gt;&lt;h3 id=&quot;The-Dual-Problem&quot;&gt;&lt;a href=&quot;#The-Dual-Problem&quot; class=&quot;headerlink&quot; title=&quot;The Dual Problem&quot;&gt;&lt;/a&gt;The Dual Problem&lt;/h3&gt;&lt;h2 id=&quot;高级分类方法&quot;&gt;&lt;a href=&quot;#高级分类方法&quot; class=&quot;headerlink&quot; title=&quot;高级分类方法&quot;&gt;&lt;/a&gt;高级分类方法&lt;/h2&gt;&lt;h3 id=&quot;Semisupervised-Learning&quot;&gt;&lt;a href=&quot;#Semisupervised-Learning&quot; class=&quot;headerlink&quot; title=&quot;Semisupervised Learning&quot;&gt;&lt;/a&gt;Semisupervised Learning&lt;/h3&gt;&lt;h3 id=&quot;Active-Learning&quot;&gt;&lt;a href=&quot;#Active-Learning&quot; class=&quot;headerlink&quot; title=&quot;Active Learning&quot;&gt;&lt;/a&gt;Active Learning&lt;/h3&gt;&lt;h3 id=&quot;Ensemble-Methods&quot;&gt;&lt;a href=&quot;#Ensemble-Methods&quot; class=&quot;headerlink&quot; title=&quot;Ensemble Methods&quot;&gt;&lt;/a&gt;Ensemble Methods&lt;/h3&gt;&lt;h2 id=&quot;线性回归&quot;&gt;&lt;a href=&quot;#线性回归&quot; class=&quot;headerlink&quot; title=&quot;线性回归&quot;&gt;&lt;/a&gt;线性回归&lt;/h2&gt;&lt;h3 id=&quot;Least-Square&quot;&gt;&lt;a href=&quot;#Least-Square&quot; class=&quot;headerlink&quot; title=&quot;Least Square&quot;&gt;&lt;/a&gt;Least Square&lt;/h3&gt;&lt;h3 id=&quot;Ridge-Regression&quot;&gt;&lt;a href=&quot;#Ridge-Regression&quot; class=&quot;headerlink&quot; title=&quot;Ridge Regression&quot;&gt;&lt;/a&gt;Ridge Regression&lt;/h3&gt;&lt;h2 id=&quot;Web挖掘&quot;&gt;&lt;a href=&quot;#Web挖掘&quot; class=&quot;headerlink&quot; title=&quot;Web挖掘&quot;&gt;&lt;/a&gt;Web挖掘&lt;/h2&gt;&lt;h3 id=&quot;Page-Ranking&quot;&gt;&lt;a href=&quot;#Page-Ranking&quot; class=&quot;headerlink&quot; title=&quot;Page Ranking&quot;&gt;&lt;/a&gt;Page Ranking&lt;/h3&gt;&lt;h3 id=&quot;Collaborative-Filtering&quot;&gt;&lt;a href=&quot;#Collaborative-Filtering&quot; class=&quot;headerlink&quot; title=&quot;Collaborative Filtering&quot;&gt;&lt;/a&gt;Collaborative Filtering&lt;/h3&gt;</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;2016年秋南京大学计算机系数据挖掘课程学期总结。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Course" scheme="http://keyunluo.github.io/categories/Course/"/>
    
    
      <category term="数据挖掘" scheme="http://keyunluo.github.io/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title>SkipList 跳跃表(1) ——基本介绍</title>
    <link href="http://keyunluo.github.io/2016/11/16/Storage/skiplist1.html"/>
    <id>http://keyunluo.github.io/2016/11/16/Storage/skiplist1.html</id>
    <published>2016-11-16T13:25:02.000Z</published>
    <updated>2017-07-06T08:08:07.154Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;跳跃列表（也称跳表）是一种随机化数据结构，基于并联的链表，其效率可比拟于二叉查找树（对于大多数操作需要$O(log\ n)$平均时间）。
基本上，跳跃列表是对有序的链表增加上附加的前进链接，增加是以随机化的方式进行的，所以在列表中的查找可以快速的跳过部分列表，因此得名,所有操作都以对数随机化的时间进行。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;SkipList主要思想&quot;&gt;&lt;a href=&quot;#SkipList主要思想&quot; class=&quot;headerlink&quot; title=&quot;SkipList主要思想&quot;&gt;&lt;/a&gt;SkipList主要思想&lt;/h2&gt;&lt;p&gt;对于我们熟悉的binary search来说，我们需要能够做到random access才行。但是在普通的link这种数据结构中却不能做到。而这种情况下我们有很多类似的工具比如heap，tree，b tree，red－black tree等等类似的都是来自AVL的变种。它们实现起来复杂，需要各种旋转，调整来保持平衡，此外，跳表在当前热门的开源项目中也有很多应用，比如LevelDB的核心数据结构memtable是用跳表实现的，redis的sorted set数据
结构也是有跳表实现的。&lt;/p&gt;
&lt;p&gt;SkipList主要思想是，对于一个给定的有序链表，每隔一段数据对其进行索引，这样的索引可以建立多次，这是一种通过“空间来换取时间”的一个算法，通过在每个节点中增加了向前的指针(即层)，从而提升查找的效率。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-11/skip_list1.png&quot; alt=&quot;图1&quot;&gt;&lt;/p&gt;
&lt;p&gt;如图1， 原始的链表L1是已经排好序的，但我们不能随机查找某一项，因而增加一个链表L2，这个链表跳过一些元素，减少不必要的比较。具体来说，比较次数为L2中数据的个数加上跳过的数据个数，即$L = L2 + L1/L2$, 要让L最小，需要$L2=L1/L2$,设原始链表长度为n,则总的访问次数为$2\sqrt n$.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-11/skip_list2.png&quot; alt=&quot;图2&quot;&gt;&lt;/p&gt;
&lt;p&gt;如果想继续优化，则可以在L2上再添加一个L3链表，同理可推导出$L3=L2/L3=L1/L2$时，比较次数最少$3\sqrt[3]{n}$.&lt;/p&gt;
&lt;p&gt;假设链表有k层，访问次数为$k\sqrt[k]{n}$,取$k=\log_2 n$,得访问次数$\log_2{n} \cdot n^{\frac{1}{\log_2 n}} = 2\log_2{n}$, 相当于一颗二叉搜索树。&lt;/p&gt;
&lt;h2 id=&quot;SkipList基本结构&quot;&gt;&lt;a href=&quot;#SkipList基本结构&quot; class=&quot;headerlink&quot; title=&quot;SkipList基本结构&quot;&gt;&lt;/a&gt;SkipList基本结构&lt;/h2&gt;&lt;h3 id=&quot;基本结构&quot;&gt;&lt;a href=&quot;#基本结构&quot; class=&quot;headerlink&quot; title=&quot;基本结构&quot;&gt;&lt;/a&gt;基本结构&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-11/skiplist.png&quot; alt=&quot;SkipList&quot;&gt;&lt;/p&gt;
&lt;p&gt;从上图可知，跳跃表主要由以下几部分组成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;表头（head）：负责维护跳跃表的节点指针。&lt;/li&gt;
&lt;li&gt;跳跃表节点：保存着元素值，以及多个层。&lt;/li&gt;
&lt;li&gt;层：保存着指向其他元素的指针。高层的指针越过的元素数量大于等于低层的指针，为了提高查找的效率，程序总是从高层先开始访问，然后随着元素值范围的缩小，慢慢降低层次。&lt;/li&gt;
&lt;li&gt;表尾：全部由 NULL 组成，表示跳跃表的末尾。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;建立算法&quot;&gt;&lt;a href=&quot;#建立算法&quot; class=&quot;headerlink&quot; title=&quot;建立算法&quot;&gt;&lt;/a&gt;建立算法&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;给定一个有序的链表&lt;/li&gt;
&lt;li&gt;选择连表中最大和最小的元素，然后从其他元素中按照一定算法（随机）随即选出一些元素，将这些元素组成有序链表。这个新的链表称为一层，原链表称为其下一层&lt;/li&gt;
&lt;li&gt;为刚选出的每个元素添加一个指针域，这个指针指向下一层中值同自己相等的元素，Top指针指向该层首元素&lt;/li&gt;
&lt;li&gt;重复2、3步，直到不再能选择出除最大最小元素以外的元素。&lt;/li&gt;
&lt;/ol&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;跳跃列表（也称跳表）是一种随机化数据结构，基于并联的链表，其效率可比拟于二叉查找树（对于大多数操作需要$O(log\ n)$平均时间）。
基本上，跳跃列表是对有序的链表增加上附加的前进链接，增加是以随机化的方式进行的，所以在列表中的查找可以快速的跳过部分列表，因此得名,所有操作都以对数随机化的时间进行。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Storage" scheme="http://keyunluo.github.io/categories/Storage/"/>
    
    
      <category term="存储引擎" scheme="http://keyunluo.github.io/tags/%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/"/>
    
  </entry>
  
  <entry>
    <title>LSM Tree (1)</title>
    <link href="http://keyunluo.github.io/2016/11/16/Storage/lsm1.html"/>
    <id>http://keyunluo.github.io/2016/11/16/Storage/lsm1.html</id>
    <published>2016-11-16T06:25:02.000Z</published>
    <updated>2017-07-06T08:08:07.154Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;LSM树的基本思想是将修改的数据保存在内存中，达到一定数量后在将修改的数据批量写入磁盘，在写入的过程中与之前已经存在的数据做合并。同B树存储模型一样，LSM存储模型也支持增、删、读、改以及顺序扫描操作。LSM模型利用批量写入解决了随机写入的问题，虽然牺牲了部分读的性能，但是大大提高了写的性能。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;特点&quot;&gt;&lt;a href=&quot;#特点&quot; class=&quot;headerlink&quot; title=&quot;特点&quot;&gt;&lt;/a&gt;特点&lt;/h2&gt;&lt;p&gt;通过将大量的随机写转换为顺序写，从而极大地提升了数据写入的性能，虽然与此同时牺牲了部分读的性能。只适合存储 key 值有序且写入大于读取的数据，或者读取操作通常是 key 值连续的数据。&lt;/p&gt;
&lt;h2 id=&quot;存储模型&quot;&gt;&lt;a href=&quot;#存储模型&quot; class=&quot;headerlink&quot; title=&quot;存储模型&quot;&gt;&lt;/a&gt;存储模型&lt;/h2&gt;&lt;h3 id=&quot;WAL&quot;&gt;&lt;a href=&quot;#WAL&quot; class=&quot;headerlink&quot; title=&quot;WAL&quot;&gt;&lt;/a&gt;WAL&lt;/h3&gt;&lt;p&gt;在设计数据库的时候经常被使用，当插入一条数据时，数据先顺序写入 WAL 文件中，之后插入到内存中的 MemTable 中。这样就保证了数据的持久化，不会丢失数据，并且都是顺序写，速度很快。当程序挂掉重启时，可以从 WAL 文件中重新恢复内存中的 MemTable。&lt;/p&gt;
&lt;h3 id=&quot;MemTable&quot;&gt;&lt;a href=&quot;#MemTable&quot; class=&quot;headerlink&quot; title=&quot;MemTable&quot;&gt;&lt;/a&gt;MemTable&lt;/h3&gt;&lt;p&gt;MemTable 对应的就是 WAL 文件，是该文件内容在内存中的存储结构，通常用 SkipList 来实现。MemTable 提供了 k-v 数据的写入、删除以及读取的操作接口。其内部将 k-v 对按照 key 值有序存储，这样方便之后快速序列化到 SSTable 文件中，仍然保持数据的有序性。&lt;/p&gt;
&lt;h3 id=&quot;Immutable-Memtable&quot;&gt;&lt;a href=&quot;#Immutable-Memtable&quot; class=&quot;headerlink&quot; title=&quot;Immutable Memtable&quot;&gt;&lt;/a&gt;Immutable Memtable&lt;/h3&gt;&lt;p&gt;顾名思义，Immutable Memtable 就是在内存中只读的 MemTable，由于内存是有限的，通常我们会设置一个阀值，当 MemTable 占用的内存达到阀值后就自动转换为 Immutable Memtable，Immutable Memtable 和 MemTable 的区别就是它是只读的，系统此时会生成新的 MemTable 供写操作继续写入。&lt;/p&gt;
&lt;p&gt;之所以要使用 Immutable Memtable，就是为了避免将 MemTable 中的内容序列化到磁盘中时会阻塞写操作。&lt;/p&gt;
&lt;h3 id=&quot;SSTable&quot;&gt;&lt;a href=&quot;#SSTable&quot; class=&quot;headerlink&quot; title=&quot;SSTable&quot;&gt;&lt;/a&gt;SSTable&lt;/h3&gt;&lt;p&gt;SSTable 就是 MemTable 中的数据在磁盘上的有序存储，其内部数据是根据 key 从小到大排列的。通常为了加快查找的速度，需要在 SSTable 中加入数据索引，可以快读定位到指定的 k-v 数据。&lt;/p&gt;
&lt;p&gt;SSTable 通常采用的分级的结构，例如 LevelDB 中就是如此。MemTable 中的数据达到指定阀值后会在 Level 0 层创建一个新的 SSTable。当某个 Level 下的文件数超过一定值后，就会将这个 Level 下的一个 SSTable 文件和更高一级的 SSTable 文件合并，由于 SSTable 中的 k-v 数据都是有序的，相当于是一个多路归并排序，所以合并操作相当快速，最终生成一个新的 SSTable 文件，将旧的文件删除，这样就完成了一次合并过程。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-11/lsm-tree-sstable.png&quot; alt=&quot;SSTable&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;常用操作的实现&quot;&gt;&lt;a href=&quot;#常用操作的实现&quot; class=&quot;headerlink&quot; title=&quot;常用操作的实现&quot;&gt;&lt;/a&gt;常用操作的实现&lt;/h2&gt;&lt;h3 id=&quot;写入&quot;&gt;&lt;a href=&quot;#写入&quot; class=&quot;headerlink&quot; title=&quot;写入&quot;&gt;&lt;/a&gt;写入&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-11/lsm-tree-write.png&quot; alt=&quot;lsm-tree-write.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;在 LSM Tree 中，写入操作是相当快速的，只需要在 WAL 文件中顺序写入当次操作的内容，成功之后将该 k-v 数据写入 MemTable 中即可。尽管做了一次磁盘 IO，但是由于是顺序追加写入操作，效率相对来说很高，并不会导致写入速度的降低。数据写入 MemTable 中其实就是往 SkipList 中插入一条数据，过程也相当简单快速。&lt;/p&gt;
&lt;h3 id=&quot;更新&quot;&gt;&lt;a href=&quot;#更新&quot; class=&quot;headerlink&quot; title=&quot;更新&quot;&gt;&lt;/a&gt;更新&lt;/h3&gt;&lt;p&gt;更新操作其实并不真正存在，和写入一个 k-v 数据没有什么不同，只是在读取的时候，会从 Level0 层的 SSTable 文件开始查找数据，数据在低层的 SSTable 文件中必然比高层的文件中要新，所以总能读取到最新的那条数据。也就是说此时在整个 LSM Tree 中可能会同时存在多个 key 值相同的数据，只有在之后合并 SSTable 文件的时候，才会将旧的值删除。&lt;/p&gt;
&lt;h3 id=&quot;删除&quot;&gt;&lt;a href=&quot;#删除&quot; class=&quot;headerlink&quot; title=&quot;删除&quot;&gt;&lt;/a&gt;删除&lt;/h3&gt;&lt;p&gt;删除一条记录的操作比较特殊，并不立即将数据从文件中删除，而是记录下对这个 key 的删除操作标记，同插入操作相同，插入操作插入的是 k-v 值，而删除操作插入的是 k-del 标记，只有当合并 SSTable 文件时才会真正的删除。&lt;/p&gt;
&lt;h3 id=&quot;Compaction&quot;&gt;&lt;a href=&quot;#Compaction&quot; class=&quot;headerlink&quot; title=&quot;Compaction&quot;&gt;&lt;/a&gt;Compaction&lt;/h3&gt;&lt;p&gt;当数据不断从 Immutable Memtable 序列化到磁盘上的 SSTable 文件中时，SSTable 文件的数量就不断增加，而且其中可能有很多更新和删除操作并不立即对文件进行操作，而只是存储一个操作记录，这就造成了整个 LSM Tree 中可能有大量相同 key 值的数据，占据了磁盘空间。&lt;/p&gt;
&lt;p&gt;为了节省磁盘空间占用，控制 SSTable 文件数量，需要将多个 SSTable 文件进行合并，生成一个新的 SSTable 文件。比如说有 5 个 10 行的 SSTable 文件要合并成 1 个 50 行的 SSTable 文件，但是其中可能有 key 值重复的数据，我们只需要保留其中最新的一条即可，这个时候新生成的 SSTable 可能只有 40 行记录。&lt;/p&gt;
&lt;p&gt;通常在使用过程中我们采用分级合并的方法，其特点如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每一层都包含大量 SSTable 文件，key 值范围不重复，这样查询操作只需要查询这一层的一个文件即可。(第一层比较特殊，key 值可能落在多个文件中，并不适用于此特性）&lt;/li&gt;
&lt;li&gt;当一层的文件达到指定数量后，其中的一个文件会被合并进入上一层的文件中。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;读取&quot;&gt;&lt;a href=&quot;#读取&quot; class=&quot;headerlink&quot; title=&quot;读取&quot;&gt;&lt;/a&gt;读取&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-11/lsm-tree-read.png&quot; alt=&quot;lsm-tree-read.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;LSM Tree 的读取效率并不高，当需要读取指定 key 的数据时，先在内存中的 MemTable 和 Immutable MemTable 中查找，如果没有找到，则继续从 Level 0 层开始，找不到就从更高层的 SSTable 文件中查找，如果查找失败，说明整个 LSM Tree 中都不存在这个 key 的数据。如果中间在任何一个地方找到这个 key 的数据，那么按照这个路径找到的数据都是最新的。&lt;/p&gt;
&lt;p&gt;在每一层的 SSTable 文件的 key 值范围是不重复的，所以只需要查找其中一个 SSTable 文件即可确定指定 key 的数据是否存在于这一层中。Level 0 层比较特殊，因为数据是 Immutable MemTable 直接写入此层的，所以 Level 0 层的 SSTable 文件的 key 值范围可能存在重复，查找数据时有可能需要查找多个文件。&lt;/p&gt;
&lt;h3 id=&quot;优化读取&quot;&gt;&lt;a href=&quot;#优化读取&quot; class=&quot;headerlink&quot; title=&quot;优化读取&quot;&gt;&lt;/a&gt;优化读取&lt;/h3&gt;&lt;p&gt;因为这样的读取效率非常差，通常会进行一些优化，例如 LevelDB 中的 Mainfest 文件，这个文件记录了 SSTable 文件的一些关键信息，例如 Level 层数，文件名，最小 key 值，最大 key 值等，这个文件通常不会太大，可以放入内存中，可以帮助快速定位到要查询的 SSTable 文件，避免频繁读取。&lt;/p&gt;
&lt;p&gt;另外一个经常使用的方法是布隆解析器(Bloom filter)，布隆解析器是一个使用内存判断文件是否包含一个关键字的有效方法。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;LSM树的基本思想是将修改的数据保存在内存中，达到一定数量后在将修改的数据批量写入磁盘，在写入的过程中与之前已经存在的数据做合并。同B树存储模型一样，LSM存储模型也支持增、删、读、改以及顺序扫描操作。LSM模型利用批量写入解决了随机写入的问题，虽然牺牲了部分读的性能，但是大大提高了写的性能。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Storage" scheme="http://keyunluo.github.io/categories/Storage/"/>
    
    
      <category term="存储引擎" scheme="http://keyunluo.github.io/tags/%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/"/>
    
  </entry>
  
  <entry>
    <title>Hash存储引擎</title>
    <link href="http://keyunluo.github.io/2016/11/15/Storage/hash.html"/>
    <id>http://keyunluo.github.io/2016/11/15/Storage/hash.html</id>
    <published>2016-11-15T06:25:02.000Z</published>
    <updated>2017-07-06T08:08:07.154Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;存储系统的基本功能包括：增删读改，读操作又分为&lt;strong&gt;随机读取&lt;/strong&gt;和&lt;strong&gt;顺序扫描&lt;/strong&gt;, 目前单机系统的存储引擎主要有：哈希存储、B树存储、LSM树存储。Hash存储是基于哈希表结构 ：数组+链表的，支持增删改以及随机读。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;哈希存储引擎&quot;&gt;&lt;a href=&quot;#哈希存储引擎&quot; class=&quot;headerlink&quot; title=&quot;哈希存储引擎&quot;&gt;&lt;/a&gt;哈希存储引擎&lt;/h2&gt;&lt;p&gt;哈希存储引擎哈希表的持久化实现，支持增、删、改以及随机读取操作，但不支持顺序扫描，对应的存储系统为key-value存储系统。对于key-value的插入以及查询，哈希表的复杂度都是O(1)，明显比树的操作O(n)快,如果不需要有序的遍历数据，哈希表就非常适合。代表性的数据库有：Redis，Memcache，以及存储系统Bitcask。&lt;/p&gt;
&lt;p&gt;Bitcask是一个基于哈希表结构的键值存储系统，它仅支持追加操作(Append-only)，所有的写操作只追加不修改老的数据。每个文件有一定的大小限制，当文件增加到相应大小，就会产生一个新文件，老的文件只读不写。在任意时刻，只有一个文件是可写的，用于数据追加，称为活跃文件，而其他已经达到大小限制的文件，称为老数据文件。&lt;/p&gt;
&lt;h2 id=&quot;缺点&quot;&gt;&lt;a href=&quot;#缺点&quot; class=&quot;headerlink&quot; title=&quot;缺点&quot;&gt;&lt;/a&gt;缺点&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hash索引仅仅能满足“=”，“IN”和“&amp;lt;=&amp;gt;”查询，不能使用范围查询&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hash索引无法被用来避免数据排序操作&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hash索引不能利用部分索引键查询&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hash索引在任何时候都不能避免表扫描：Hash碰撞，链式扫描&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Hash索引遇到大量Hash值相等的情况后性能就不一定会比BTree索引好&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;数据结构&quot;&gt;&lt;a href=&quot;#数据结构&quot; class=&quot;headerlink&quot; title=&quot;数据结构&quot;&gt;&lt;/a&gt;数据结构&lt;/h2&gt;&lt;p&gt;Bitcask数据文件中的数据是一条一条写入操作，记录包含，key，value，主键长度，value长度，时间戳(timestamp)以及crc校验值。(删除操作不会删除旧的条目，而是将value设定为一个特殊的标识值)。&lt;/p&gt;
&lt;p&gt;内存中采用基于哈希表的索引数据结构，哈希表的作用是通过主键快速地定位到value的位置。哈希表结构中的每一项包含了三个用于定位数据的信息，分别是文件编号（file id），value在文件中的位置（value_pos），value长度（value_sz），通过读取file_id对应文件的value_pos开始的value_sz个字节，这就得到了最终的value值。写入时首先将Key-Value记录追加到活跃数据文件的末尾，接着更新内存哈希表，因此，每个写操作总共需要进行一次顺序的磁盘写入和一次内存操作。&lt;/p&gt;
&lt;p&gt;Bitcask在内存中存储了主键和value的索引信息，磁盘文件中存储了主键和value的实际内容。系统基于一个假设，value的长度远大于主键的长度。假如value的平均长度为1KB，每条记录在内存中的索引信息为32字节，那么，磁盘内存比为32 : 1。这样，32GB内存索引的数据量为32GB×32 = 1TB。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-11/bitcask.bmp&quot; alt=&quot;BitCask数据结构&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;定期合并&quot;&gt;&lt;a href=&quot;#定期合并&quot; class=&quot;headerlink&quot; title=&quot;定期合并&quot;&gt;&lt;/a&gt;定期合并&lt;/h2&gt;&lt;p&gt;Bitcask系统中的记录删除或者更新后，原来的记录成为垃圾数据。如果这些数据一直保存下去，文件会无限膨胀下去，为了解决这个问题，Bitcask需要定期执行合并（Compaction）操作以实现垃圾回收。所谓合并操作，即将所有老数据文件中的数据扫描一遍并生成新的数据文件，这里的合并其实就是对同一个key的多个操作以只保留最新一个的原则进行删除，每次合并后，新生成的数据文件就不再有冗余数据了。&lt;/p&gt;
&lt;h2 id=&quot;快速恢复&quot;&gt;&lt;a href=&quot;#快速恢复&quot; class=&quot;headerlink&quot; title=&quot;快速恢复&quot;&gt;&lt;/a&gt;快速恢复&lt;/h2&gt;&lt;p&gt;Bitcask系统中的哈希索引存储在内存中，如果不做额外的工作，服务器断电重启重建哈希表需要扫描一遍数据文件，如果数据文件很大，这是一个非常耗时的过程。Bitcask通过索引文件（hint file）来提高重建哈希表的速度。&lt;/p&gt;
&lt;p&gt;简单来说，索引文件就是将内存中的哈希索引表转储到磁盘生成的结果文件。Bitcask对老数据文件进行合并操作时，会产生新的数据文件，这个过程中还会产生一个索引文件，这个索引文件记录每一条记录的哈希索引信息。与数据文件不同的是，索引文件并不存储具体的value值，只存储value的位置（与内存哈希表一样）。这样，在重建哈希表时，就不需要扫描所有数据文件，而仅仅需要将索引文件中的数据一行行读取并重建即可，大大减少了重启后的恢复时间。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;存储系统的基本功能包括：增删读改，读操作又分为&lt;strong&gt;随机读取&lt;/strong&gt;和&lt;strong&gt;顺序扫描&lt;/strong&gt;, 目前单机系统的存储引擎主要有：哈希存储、B树存储、LSM树存储。Hash存储是基于哈希表结构 ：数组+链表的，支持增删改以及随机读。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Storage" scheme="http://keyunluo.github.io/categories/Storage/"/>
    
    
      <category term="存储引擎" scheme="http://keyunluo.github.io/tags/%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E/"/>
    
  </entry>
  
  <entry>
    <title>高级算法--作业2</title>
    <link href="http://keyunluo.github.io/2016/11/03/Course/advanced-algorithm-assignment2.html"/>
    <id>http://keyunluo.github.io/2016/11/03/Course/advanced-algorithm-assignment2.html</id>
    <published>2016-11-03T07:41:02.000Z</published>
    <updated>2017-07-06T08:08:06.889Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;高级算法：第二次作业。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;Problem-1&quot;&gt;&lt;a href=&quot;#Problem-1&quot; class=&quot;headerlink&quot; title=&quot;Problem 1&quot;&gt;&lt;/a&gt;Problem 1&lt;/h2&gt;&lt;p&gt;Consider the following optimization problem.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Instance&lt;/em&gt;: &lt;strong&gt;n&lt;/strong&gt; positive integers $x_1&amp;lt;x_2&amp;lt;\cdots &amp;lt;x_n$.
Find two &lt;em&gt;disjoint&lt;/em&gt; nonempty subsets $A,B\subset{1,2,\ldots,n}$ with $\sum_{i\in A}x_i\ge \sum_{i\in B}x_i$, such that the ratio $\frac{\sum_{i\in A}x_i}{\sum_{i\in B}x_i}$ is minimized.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Give a pseudo-polynomial time algorithm for the problem, and then give an FPTAS for the problem based on the pseudo-polynomial time algorithm.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;My-Solution&quot;&gt;&lt;a href=&quot;#My-Solution&quot; class=&quot;headerlink&quot; title=&quot;My Solution&quot;&gt;&lt;/a&gt;My Solution&lt;/h3&gt;&lt;p&gt;1&amp;gt;. &lt;strong&gt;PTAS Algorithm&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; Let $X =\sum_{i=1}^{n}x_i$ , and table $T = [1, \ldots,X][ 1, \ldots,X]$, each value in $T_i[a][b]$ is a boolean value which indicates if there are disjoint subset &lt;strong&gt;A&lt;/strong&gt; and &lt;strong&gt;B&lt;/strong&gt; that the sum of element in $X_A$ is &lt;strong&gt;a&lt;/strong&gt; and the sum of element in $X_B$ is &lt;strong&gt;b&lt;/strong&gt; . For any $x_i$ can be in $X_A$ or $X_B$ or neither of them.&lt;/p&gt;
&lt;p&gt; The table can be computed as follows:&lt;/p&gt;
&lt;p&gt;$$T_i[a][b]=
\begin{cases}
1 &amp;amp; \text{if} \  T_{i-1}[a][b]=1  \\
1 &amp;amp; \text{if} \  T_{i-1}[a-x_i][b]=1  \\
1 &amp;amp; \text{if} \  T_{i-1}[a][b-x_i]=1  \\
0 &amp;amp; otherwise
\end{cases}$$&lt;/p&gt;
&lt;p&gt;  After filling the table,  it takes overall $O(nX^2)$ time, the optimal result can be found by choosing the minimum ratio $a/b$ with the constraint that $T[a][b]=1\  \text{and} \  a \ge b$.  This can be done in $O(X^2)$ time, after get the optimal value, we can re-construct the set $A,B$ by the following algorithm:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Algorithm: &lt;strong&gt;Re-Construct-Set&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Init&lt;/strong&gt;: A,B = $\emptyset$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Input&lt;/strong&gt;: a, b, i&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;: set A, B&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Steps:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp; if i == 1:&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp; if a == $x_1$:&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp; &amp;emsp;&amp;emsp;  return ({a}, $\emptyset$)&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp; if b == $x_1$:&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp; &amp;emsp;&amp;emsp;  return ($\emptyset$,{b})&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp; if $T_{i-1}[a][b]$ == 1:&lt;/p&gt;
&lt;p&gt; &amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp; return Re-Construct-Set(a,b,i-1):&lt;/p&gt;
&lt;p&gt; &amp;emsp;&amp;emsp; if $T_{i-1}[a-x_i][b]$ == 1:&lt;/p&gt;
&lt;p&gt; &amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp; (A,B)$\Leftarrow$ Re-Construct-Set(a,b,i-1):&lt;/p&gt;
&lt;p&gt; &amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp; return ($A \bigcup {x_i}$, B) :&lt;/p&gt;
&lt;p&gt; &amp;emsp;&amp;emsp; if $T_{i-1}[a][b-x_i]$ == 1:&lt;/p&gt;
&lt;p&gt; &amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp; (A,B)$\Leftarrow$ Re-Construct-Set(a,b,i-1):&lt;/p&gt;
&lt;p&gt; &amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp; return (A, $B \bigcup {x_i}$) :&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This recursive algorithm takes $O(n)$ time,  so all in all, the running time is $O(nX^2)$, it is a  pseudo-polynomial time algorithm.&lt;/p&gt;
&lt;p&gt;2&amp;gt;. &lt;strong&gt;FPTAS Algorithm&lt;/strong&gt; (unsolved)
 According to the algorithm above, we know that the  input  of $x_1, x_2, \ldots, x_n$ is  ascending.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
&lt;p&gt;


&lt;hr&gt;
&lt;/p&gt;&lt;h2 id=&quot;Problem-2&quot;&gt;&lt;a href=&quot;#Problem-2&quot; class=&quot;headerlink&quot; title=&quot;Problem 2&quot;&gt;&lt;/a&gt;Problem 2&lt;/h2&gt;&lt;p&gt;In the &lt;em&gt;maximum directed cut&lt;/em&gt; (MAX-DICUT) problem, we are given as input a directed graph &lt;em&gt;G(V,E)&lt;/em&gt;. The goal is to partition &lt;em&gt;V&lt;/em&gt; into disjoint &lt;em&gt;S&lt;/em&gt; and &lt;em&gt;T&lt;/em&gt; so that the number of edges in $E(S,T)={(u,v)\in E\mid u\in S, v\in T}$ is maximized. The following is the integer program for MAX-DICUT:&lt;/p&gt;
&lt;p&gt;\begin{align}
\text{maximize} &amp;amp;&amp;amp;&amp;amp; \sum_{(u,v)\in E}y_{u,v}\\
\text{subject to} &amp;amp;&amp;amp; y_{u,v} &amp;amp;\le x_u, &amp;amp; \forall (u,v)&amp;amp;\in E,\\
&amp;amp;&amp;amp; y_{u,v} &amp;amp;\le 1-x_v, &amp;amp; \forall (u,v)&amp;amp;\in E,\\
&amp;amp;&amp;amp; x_v &amp;amp;\in{0,1}, &amp;amp; \forall v&amp;amp;\in V,\\
&amp;amp;&amp;amp;  y_{u,v} &amp;amp;\in{0,1}, &amp;amp; \forall (u,v)&amp;amp;\in E.
\end{align}&lt;/p&gt;
&lt;p&gt;Let $x_v^*,y_{u,v}^*$ denote the optimal solution to the &lt;strong&gt;LP-relaxation&lt;/strong&gt; of the above integer program.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Apply the randomized rounding such that for every $v\in V, \hat{x}_v=1$ independently with probability $x_v^*$. Analyze the approximation ratio (between the expected size of the random cut and OPT).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Apply another randomized rounding such that for every $v\in V, \hat{x}_v=1$ independently with probability $1/4+x_v^*/2$. Analyze the approximation ratio for this algorithm.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&quot;My-Solution-1&quot;&gt;&lt;a href=&quot;#My-Solution-1&quot; class=&quot;headerlink&quot; title=&quot;My Solution&quot;&gt;&lt;/a&gt;My Solution&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;let &lt;strong&gt;OPT&lt;/strong&gt; be the maximum weight of &lt;em&gt;MAX-DICUT&lt;/em&gt;, and it equals the value of given &lt;em&gt;ILP&lt;/em&gt; algorithm,  and the result of the optimal  LP-relaxation is $OPT_{LP}$, the probability of points(u,v) in cut is:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{array} {lcl}
\Pr((u,v)\ in\ cut) &amp;amp; = &amp;amp; \Pr(u \in S \ and \ v \in T) \\
        &amp;amp; = &amp;amp; \Pr(u \in S) \cdot \Pr(v \in T)\\
        &amp;amp; = &amp;amp; x_u(1 -x_v) \\
        &amp;amp; = &amp;amp;\frac{1}{2}x_u+x_u(\frac{1}{2}-x_v)\\
        &amp;amp; \ge &amp;amp; \frac{1}{2}y_{u,v} + x_u(\frac{1}{2}-x_v)
\end{array}$$&lt;/p&gt;
&lt;p&gt;since that $\sum_{(u,v \in E)}x_u(\frac{1}{2}-x_v) \ge 0 $, so the total number of cuts W is as follows:&lt;/p&gt;
&lt;p&gt;  $$E(W)= \sum_{(u,v \in E)} \Pr((u,v)\ in\ cut) \ge \sum_{(u,v \in E)}\frac{y_{u,v}}{2} \ge \frac{OPT_{LP}}{2} \ge \frac{OPT}{2}$$&lt;/p&gt;
&lt;p&gt;   The approximation ratio for this algorithm is 0.5.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;let &lt;strong&gt;OPT&lt;/strong&gt; be the maximum weight of &lt;em&gt;MAX-DICUT&lt;/em&gt;, and it equals the value of given &lt;em&gt;ILP&lt;/em&gt; algorithm,  and the result of the optimal  LP-relaxation is $OPT_{LP}$, the probability of points(u,v) in cut is:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{array} {lcl}
\Pr((u,v)\ in\ cut) &amp;amp; = &amp;amp; \Pr(u \in S \ and \ v \in T) \\
        &amp;amp; = &amp;amp; \Pr(u \in S) \cdot \Pr(v \in T)\\
        &amp;amp; = &amp;amp; (\frac{1}{4}+\frac{x_u}{2})(1 - (\frac{1}{4}+\frac{x_v}{2})) \\
        &amp;amp; = &amp;amp; (\frac{1}{4}+\frac{x_u}{2})(\frac{1}{4}+\frac{1-x_v}{2})\\
        &amp;amp; \ge &amp;amp; (\frac{1}{4}+\frac{y_{u,v}}{2})(\frac{1}{4}+\frac{y_{u,v}}{2})  \\
        &amp;amp; = &amp;amp; (\frac{1}{4}-\frac{y_{u,v}}{2})^2 + \frac{y_{u,v}}{2}  \\
        &amp;amp; \ge &amp;amp; \frac{y_{u,v}}{2}
\end{array}$$&lt;/p&gt;
&lt;p&gt;So, the total number of cuts W is as follows:&lt;/p&gt;
&lt;p&gt;$$E(W)= \sum_{(u,v \in E)} \Pr((u,v)\ in\ cut) \ge \sum_{(u,v \in E)}\frac{y_{u,v}}{2} \ge \frac{OPT_{LP}}{2} \ge \frac{OPT}{2}$$&lt;/p&gt;
&lt;p&gt; The approximation ratio for this algorithm is 0.5.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Problem-3&quot;&gt;&lt;a href=&quot;#Problem-3&quot; class=&quot;headerlink&quot; title=&quot;Problem 3&quot;&gt;&lt;/a&gt;Problem 3&lt;/h2&gt;&lt;p&gt;Recall the MAX-SAT problem and its integer program:&lt;/p&gt;
&lt;p&gt;\begin{align}
\text{maximize} &amp;amp;&amp;amp;&amp;amp; \sum_{j=1}^my_j\\
\text{subject to} &amp;amp;&amp;amp;&amp;amp; \sum_{i\in S_j^+}x_i+\sum_{i\in S_j^-}(1-x_i)\ge y_j, &amp;amp;&amp;amp; 1\le j\le m,\\
&amp;amp;&amp;amp;&amp;amp; x_i\in{0,1}, &amp;amp;&amp;amp; 1\le i\le n,\\
&amp;amp;&amp;amp;&amp;amp; y_j\in{0,1}, &amp;amp;&amp;amp; 1\le j\le m.
\end{align}&lt;/p&gt;
&lt;p&gt;Recall that $S_j^+,S_j^-\subseteq{1,2,\ldots,n}$ are the respective sets of variables appearing positively and negatively in clause $j$.&lt;/p&gt;
&lt;p&gt;Let $x_i^*,y_j^*$ denote the optimal solution to the &lt;strong&gt;LP-relaxation&lt;/strong&gt; of the above integer program. In our class we learnt that if $\hat{x}_i$ is round to 1 independently with probability $x_i^*$, we have approximation ratio $1 − 1 / e$.&lt;/p&gt;
&lt;p&gt;We consider a generalized rounding scheme such that every $\hat{x}_i$ is round to 1 independently with probability $f(x_i^*)$ for some function $f:[0,1]\to[0,1]$ to be specified.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Suppose $f(x)$ is an arbitrary function satisfying that $1-4^{-x}\le f(x)\le 4^{x-1}$ for any $x\in[0,1]$. Show that with this rounding scheme, the approximation ratio (between the expected number of satisfied clauses and OPT is at least $3 / 4$.&lt;/li&gt;
&lt;li&gt;Is it possible that for some more clever $f$ we can do better than this? Try to justify your argument.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&quot;My-Solution-2&quot;&gt;&lt;a href=&quot;#My-Solution-2&quot; class=&quot;headerlink&quot; title=&quot;My Solution&quot;&gt;&lt;/a&gt;My Solution&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;let $g(x) = 1 -4^{-x}, x \in[0,1], g(x) \le f(x) \le 1 - g(1-x), g\prime\prime(x)&lt;0$, then=&quot;&quot; *g(x)*=&quot;&quot; is=&quot;&quot; &lt;u=&quot;&quot;&gt;monotonically increasing and concavity, $0 \le g(x) \le \frac{3}{4}, g(x) \ge  \frac{3}{4}x$, let $X= \sum_{i=1}^k x_i\prime, x_i\prime= x_i\ for\ i \in [0,l],\  x_i\prime= 1 -  x_i\ for\ i \in (l,k]$, then $g(X) \ge \frac{3}{4}X, X \in [0,1], g(X) \ge \frac{3}{4}, X \in [1,\infty]$&lt;/0$,&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\begin{array} {lcl}
\Pr(S_j\ is \ satisfied) &amp;amp; = &amp;amp; 1 - \prod_{i \in S_j^+}(1 - f(x_i))\prod_{i \in S_j^-}f(x_i)\\
        &amp;amp; \ge &amp;amp;1 - \prod_{i =1}^l(1 - g(x_i))\prod_{i =l+1}^k(1 - g(1 - x_i))\\
        &amp;amp; = &amp;amp; 1 - \prod_{i=1}^k(1- g(x_i^\prime)) = 1 - \prod_{i=1}^k(4^{-x_i\prime}) =  1 - \prod_{i=1}^k(4^{-X}) \\
        &amp;amp; = &amp;amp; g(X) \\
        &amp;amp; \ge &amp;amp; \frac{3}{4} min(1, \sum_{i=1}^k x_i^\prime) =  \frac{3}{4} min(1, \sum_{i=1}^l x_i + \sum_{i = l+1}^k(1-x_i))\\
        &amp;amp; \ge &amp;amp; \frac{3}{4} min(1, y_i) \ge \frac{3}{4} y_i
\end{array}$$&lt;/p&gt;
&lt;p&gt;   So,  $OPT \ge OPT_{LP} = \sum_{j=1}^my_j^*$, the approximation ratio  = $\frac{3}{4}$&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;It is impossible that for some more clever f we can do better than this.&lt;/p&gt;
&lt;p&gt;Considering the symmetric property of $1- f(x_i)$ and $f(x_i)$, the best result is that they are equal, and the expectation of $S_j$ is not satisfied is $\frac{1}{4^{y_j^\ast} }$, so $\Pr(S_j\ is \ satisfied) \ge 1 - \frac{1}{4^{y_j^\ast} } \ge \frac{3}{4}{y_j^\ast}$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Problem-4&quot;&gt;&lt;a href=&quot;#Problem-4&quot; class=&quot;headerlink&quot; title=&quot;Problem 4&quot;&gt;&lt;/a&gt;Problem 4&lt;/h2&gt;&lt;p&gt;Recall that the instance of &lt;strong&gt;set cover&lt;/strong&gt; problem is a collection of &lt;em&gt;m&lt;/em&gt; subsets $S_1,S_2,\ldots,S_m\subseteq U$, where $U$ is a universe of size $n = | U | $. The goal is to find the smallest $C\subseteq{1,2,\ldots,m}$ such that $U=\bigcup_{i\in C}S_i$. The frequency &lt;em&gt;f&lt;/em&gt; is defined to be $\max_{x\in U}|{i\mid x\in S_i}|$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Give the primal integer program for set cover, its LP-relaxation and the dual LP.&lt;/li&gt;
&lt;li&gt;Describe the complementary slackness conditions for the problem.&lt;/li&gt;
&lt;li&gt;Give a primal-dual algorithm for the problem. Present the algorithm in the language of primal-dual scheme (alternatively raising variables for the LPs). Analyze the approximation ratio in terms of the frequency &lt;em&gt;f&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&quot;My-Solution-3&quot;&gt;&lt;a href=&quot;#My-Solution-3&quot; class=&quot;headerlink&quot; title=&quot;My Solution&quot;&gt;&lt;/a&gt;My Solution&lt;/h3&gt;&lt;p&gt;1&amp;gt;.&lt;/p&gt;
&lt;p&gt;(1)&lt;strong&gt;Primal Integer Program&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Minimize:  $\sum_{j=1}^m c_j x_j$&lt;/p&gt;
&lt;p&gt;Subject to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\sum_{j:e_i \in S_j}x_j \ge 1, i = 1,2,\ldots, n$, &lt;em&gt;or&lt;/em&gt; :$\sum_{j=1}^ma_{i,j}x_j \ge 1, i = 1,2,\ldots, n$&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$x_j={0,1}, j = 1,2,\ldots, m$&lt;/p&gt;
&lt;p&gt;  (2)&lt;strong&gt;LP-relaxation&lt;/strong&gt;
  Minimize:  $\sum_{j =1}^mc_jx_j$
  Subject to:&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;$\sum_{j:e_i \in S_j}x_j \ge 1, i = 1,2,\ldots, n$, &lt;em&gt;or&lt;/em&gt; :$\sum_{j=1}^ma_{i,j}x_j \ge 1, i = 1,2,\ldots, n$&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$x_j \ge 0, j = 1,2,\ldots, m$&lt;/p&gt;
&lt;p&gt;  (3) &lt;strong&gt;Dual LP&lt;/strong&gt;
  Maximize: $\sum_{i=1}^n y_i$
  Subject to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;$\sum_{j:e_i \in S_j}y_j \le c_j, j=1, 2,\ldots, m$, &lt;em&gt;or&lt;/em&gt; :$\sum_{i=1}^na_{i,j}y_j \le c_j, j = 1,2,\ldots, m$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$y_i \ge 0, i =1,2,\ldots,n$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;2&amp;gt;.Complementary Slackness Conditions&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$\forall$ feasible primal solution x and dual solution y, x and y are both optimal if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For each $1 \le i \le n\ ,either\ \sum_{j=1}^m a_{i,j}x_j =1\ or\ y_i\ =\ 0\ $&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each $1 \le j \le m\ ,either \ \sum_{i=1}^na_{i,j}y_j=c_j\ or\ x_j=\ 0 $&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\forall$ feasible primal solution x and dual solution y, for $\alpha,\beta \ge 1$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;For each $1 \le i \le n\ ,either\ 1 \le \sum_{j=1}^m a_{i,j}x_j \le \beta \ or\ y_i\ =\ 0\ $&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each $1 \le j \le m\ ,either \ c_j \ge  \sum_{i=1}^na_{i,j}y_j \ge \frac{c_j}{\alpha}\ or\ x_j=\ 0 $&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;3&amp;gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Initially $x=0,y=0$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;While $E \neq \emptyset:$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&amp;emsp;&amp;emsp;Pick an $e$ uncovered and raise $y_e$ until some set goes tight&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&amp;emsp;&amp;emsp;Pick all tight sets in the cover and update $x$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&amp;emsp;&amp;emsp;Delete these sets from $E$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Output the set cover $x$&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;here, $cx \le \alpha\beta y \le \alpha\beta OPT$, in primal conditions, the variables will be incremented integrally, $x_j \neq 0 \Rightarrow \sum_{j:e_i \in S_j}y_j  = c_j, \alpha =1$, in the dual conditions, each element having a nonzero dual value can be covered at most f times, $y_i \neq 0 \Rightarrow \sum_{j:e_i \in S_j}x_j  \le f, \beta =f$.
So, the approximation ratio is $f$.&lt;/p&gt;
&lt;hr&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;高级算法：第二次作业。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Course" scheme="http://keyunluo.github.io/categories/Course/"/>
    
    
      <category term="高级算法" scheme="http://keyunluo.github.io/tags/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>高级算法--作业1</title>
    <link href="http://keyunluo.github.io/2016/10/13/Course/advanced-algorithm-assignment1.html"/>
    <id>http://keyunluo.github.io/2016/10/13/Course/advanced-algorithm-assignment1.html</id>
    <published>2016-10-13T12:41:02.000Z</published>
    <updated>2017-07-06T08:08:06.888Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;高级算法：第一次作业。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;Problem-1&quot;&gt;&lt;a href=&quot;#Problem-1&quot; class=&quot;headerlink&quot; title=&quot;Problem 1&quot;&gt;&lt;/a&gt;Problem 1&lt;/h2&gt;&lt;p&gt;For any $\alpha\ge 1$, a cut $C$ in an undirected (multi)graph $G(V,E)$ is called an &lt;strong&gt;α-min-cut&lt;/strong&gt; if $|C|\le\alpha|C^*|$ where $C^*$ is a min-cut in $G$.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Give a lower bound to the probability that Karger’s Random Contraction algorithm returns an &lt;strong&gt;α-min-cut&lt;/strong&gt; in a graph $G(V,E) $ of n vertices.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the above bound to estimate the number of distinct &lt;strong&gt;α-min&lt;/strong&gt; cuts in $G$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h3 id=&quot;My-Solution&quot;&gt;&lt;a href=&quot;#My-Solution&quot; class=&quot;headerlink&quot; title=&quot;My Solution&quot;&gt;&lt;/a&gt;My Solution&lt;/h3&gt;&lt;p&gt;1.From Karger’s algorithm we know that the maximum size of α-min-cut &lt;strong&gt;C&lt;/strong&gt; is &lt;strong&gt;α|C*|&lt;/strong&gt;, the degree of multi-graph $G_i:degree(G_i) = 2|E_i| \geq |V_i| |C^*| \ge |V_i|\cdot|C|/ \alpha $ ,so
the probability $p_i =\Pr[e_i \notin C \vert \forall j &amp;lt; i,e_i \notin C]$ can be computed as:&lt;/p&gt;
&lt;p&gt;$$\begin{array} {lcl}
p_i  &amp;amp; = &amp;amp; 1 - \frac{\vert C \vert}{\vert E_i \vert} \\
&amp;amp; \geq &amp;amp; 1 - \frac{2\alpha}{|V_i|} \\
&amp;amp; = &amp;amp; 1- \frac{2\alpha}{n-i+1}
\end{array}$$&lt;/p&gt;
&lt;p&gt;and further, apply the Karger’s algorithm until &lt;strong&gt;2α&lt;/strong&gt; vertices remain:&lt;/p&gt;
&lt;p&gt;$$\begin{array} {lcl}
p_{\text{survives until 2α vertices remain}} &amp;amp; \geq &amp;amp; \Pr[\,C_{2α}\mbox{ is returned by }{RandomContract}\,]\\
&amp;amp; = &amp;amp; \Pr[\,e_i\not\in C\mbox{ for all }i=1,2,\ldots,n-2α\,]\\
&amp;amp; = &amp;amp; \prod_{i=1}^{n-2α}\Pr[e_i\not\in C\mid \forall j&amp;lt;i, e_j\not\in C]\\
&amp;amp; \geq &amp;amp; \prod_{i=1}^{n-2α}\left(1-\frac{2α}{n-i+1}\right)\\
&amp;amp; = &amp;amp; \frac{(n-2α)!}{n!/(2α)!} \\
&amp;amp; = &amp;amp; \frac{(n-2α)!(2α)!}{n!} \\
&amp;amp; = &amp;amp; \frac{1}{n \choose 2α}
\end{array}$$&lt;/p&gt;
&lt;p&gt;then, choose a random cut in the remaining multi-graph:G&lt;/p&gt;
&lt;p&gt;$$\begin{array} {lcl}
p_{\text{correct}} &amp;amp; = &amp;amp; \Pr[\text{C survives until 2α vertices remain}] \times \Pr[\text{C survives random cut}]\\
&amp;amp; \geq &amp;amp; \frac{1}{n \choose 2α} \times \frac{2}{2^{2α}}\\
&amp;amp; = &amp;amp; \frac{(2α)!}{2^{2α-1}} \times \frac{(n-2α)!}{n!}\\
&amp;amp; \geq &amp;amp; \frac{(2α)!}{2^{2α-1}} \times \frac{1}{n^{2α}}\\
&amp;amp; \geq &amp;amp; \frac{1}{n^{2α}}
\end{array}$$&lt;/p&gt;
&lt;p&gt;So, a lower bound to the probability that Karger’s Random Contraction algorithm returns an α-min-cut in a graph G(V,E) of n vertices can be $n^{-2α}$.&lt;/p&gt;
&lt;p&gt;2.By the analysis of Karger’s algorithm, we know $p_C\ge\frac{1}{n^{2α}}$. And since $p_{correct}$ is a well defined probability, due to the unitarity of probability, it must hold that $p_{\text{correct}}\le 1$. Therefore:&lt;/p&gt;
&lt;p&gt;$$1 \ge p_{correct} = \sum_{C \in \sf{C}}p_C \ge |\sf{C}| \frac{1}{n^{2α}}$$&lt;/p&gt;
&lt;p&gt;and this means that $ |\sf{C}| \le n^{2α}$, So  the up bound of the number of distinct α-min cuts in G is $ n^{2α}$.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Problem-2&quot;&gt;&lt;a href=&quot;#Problem-2&quot; class=&quot;headerlink&quot; title=&quot;Problem 2&quot;&gt;&lt;/a&gt;Problem 2&lt;/h2&gt;&lt;p&gt;Let $G(V,E) $ be an undirected graph with positive edge weights $w:E\to\mathbb{Z}^+$. Given a partition of $V$ into $k$ disjoint subsets $S_1,S_2,\ldots,S_k$, we define
$w(S_1,S_2,\ldots,S_k)=\sum_{uv\in E\atop \exists i\neq j: u\in S_i,v\in S_j}w(uv)$
as the cost of the k-cut ${S_1,S_2,\ldots,S_k}$. Our goal is to find a &lt;strong&gt;k-cut&lt;/strong&gt; with maximum cost.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Give a poly-time greedy algorithm for finding the weighted max $k-cut$. Prove that the approximation ratio is $(1 − 1 / k)$.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Consider the following local search algorithm for the weighted max cut &lt;strong&gt;(max 2-cut)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt; Fill in the blank parenthesis. Give an analysis of the running time of the algorithm. And prove that the approximation ratio is &lt;em&gt;0.5&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;start with an arbitrary bipartition of $V$ into disjoint $S_0,S_1$;&lt;/p&gt;
&lt;p&gt; while (true) do&lt;/p&gt;
&lt;p&gt;  &amp;emsp;&amp;emsp;if $\exists i\in{0,1}$ and $v\in S_i$ such that (&lt;strong&gt;&lt;strong&gt;&lt;strong&gt;__&lt;/strong&gt;&lt;/strong&gt;&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;  &amp;emsp;&amp;emsp;&amp;emsp;then $v$ leaves $S_i$ and joins $S_{1 − i}$;&lt;/p&gt;
&lt;p&gt;  &amp;emsp;&amp;emsp;&amp;emsp;continue;&lt;/p&gt;
&lt;p&gt;  &amp;emsp;&amp;emsp;end if&lt;/p&gt;
&lt;p&gt;  &amp;emsp;&amp;emsp;break;&lt;/p&gt;
&lt;p&gt;end&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr&gt;
&lt;h3 id=&quot;My-Solution-1&quot;&gt;&lt;a href=&quot;#My-Solution-1&quot; class=&quot;headerlink&quot; title=&quot;My Solution&quot;&gt;&lt;/a&gt;My Solution&lt;/h3&gt;&lt;p&gt;1.(1) &lt;strong&gt;algorithm&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$S_1={v_1},S_2={v_2},\ldots,S_k={v_k}$,$V =V - {v_1,v_2,\ldots,v_k} $  ;&lt;/p&gt;
&lt;p&gt;while ( V is not empty) do:&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp; Randomly select  a vertic $v$ in V;&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp; $V = V- v$&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp; put $v$ to $S_j$ for the biggest increased value&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(2) &lt;strong&gt;proof&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$\begin{array} {lcl}
E(w) &amp;amp; = &amp;amp; \sum_{(i,j) \in E} w_{i,j} \Pr[i \in V_m,j\in V_n, \forall m \neq n] \\
        &amp;amp; = &amp;amp; \sum_{(i,j) \in E} w_{i,j}  (1 - \Pr[i \in V_m,j\in V_m, \forall m ]) \\
        &amp;amp; = &amp;amp; \sum_{(i,j) \in E} w_{i,j}  (1 - \sum_{m=1}^k\Pr[i \in V_m,j\in V_m]) \\
        &amp;amp; = &amp;amp; \sum_{(i,j) \in E} w_{i,j}  (1 - \sum_{m=1}^k\frac{1}{k^2}) \\
        &amp;amp; = &amp;amp; (1-\frac{1}{k})\sum_{(i,j) \in E} w_{i,j}  \\
        &amp;amp; \ge &amp;amp; (1-\frac{1}{k})OPT
\end{array}$$&lt;/p&gt;
&lt;p&gt;2.(1) the &lt;strong&gt;blank&lt;/strong&gt;: $\underline{v \in S_{1-i} \ has\ bigger\ cut \ result}$&lt;/p&gt;
&lt;p&gt;(2) &lt;strong&gt;analysis&lt;/strong&gt;: calculate the cut  between $S_0,S_1$ is $O(n^2)$, and in each loop the value is increaed at least 1, thus the maximum possible cut value is $\sum_{e \in E}w_e$,then the loop size is $\sum_{e \in E}w_e$ at most. finally the total running time is $O(n^2 \cdot \sum_{e \in E}w_e)$.&lt;/p&gt;
&lt;p&gt;(3) &lt;strong&gt;proof&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;the &lt;em&gt;OPT&lt;/em&gt; can’t be larger than the sum of all egde weights, then : $\sum_{e \in E} w_e\geq OPT $&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;if we move $v$ from $S_i \  to \ S_{1-i}$, then $\sum_{v \in S_{1-i}, (u,v) \in E}w(u,v) \geq \sum_{v \in S_{i}, (u,v) \in E}w(u,v) $,
according to the mean value theorem, we have $\sum_{v \in S_{1-i}, (u,v) \in E}w(u,v) \geq \frac{1}{2}\sum_{v :(u,v) \in E}w(u,v)$, and  for any $u’ \in S_{1-i}$ , we can get $\sum_{v \in S_i, (u’,v) \in E}w(u’,v) \geq \frac{1}{2}\sum_{v :(u’,v) \in E}w(u’,v)$ similarly.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, for all vertices in V, the max-cut is :&lt;/p&gt;
&lt;p&gt;$$\begin{array} {lcl}
V(S) &amp;amp;= &amp;amp; \sum_{u \in S_i,v \in S_{1-i},(u,v) \in E } \\
        &amp;amp; = &amp;amp; \sum_{v \in S_i, (u,v) \in E}w(u,v) + \sum_{v \in S_{1-i}, (u,v) \in E}w(u,v)  \\
        &amp;amp; \ge &amp;amp; \frac{1}{2}\sum_{v :(u’,v) \in E}w(u,v) + \frac{1}{2}\sum_{v :(u,v) \in E}w(u,v) \\
        &amp;amp; = &amp;amp; \frac{1}{2} \sum_{e \in E}w_e \\
        &amp;amp; \ge &amp;amp;\frac{1}{2}OPT
 \end{array}$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Problem-3&quot;&gt;&lt;a href=&quot;#Problem-3&quot; class=&quot;headerlink&quot; title=&quot;Problem 3&quot;&gt;&lt;/a&gt;Problem 3&lt;/h2&gt;&lt;p&gt;Given $m$ subsets $S_1,S_2,\ldots, S_m\subseteq U $of a universe $U$ of size $n$, we want to find a $C\subseteq{1,2,\ldots, n}$ of fixed size $k = | C |$ with the &lt;em&gt;maximum coverage&lt;/em&gt; $\left|\bigcup_{i\in C}S_i\right|$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Give a poly-time greedy algorithm for the problem. Prove that the approximation ratio is $1 − (1 − 1 / k)k &amp;gt; 1 − 1 / e$.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&quot;My-Solution-2&quot;&gt;&lt;a href=&quot;#My-Solution-2&quot; class=&quot;headerlink&quot; title=&quot;My Solution&quot;&gt;&lt;/a&gt;My Solution&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;algorithm&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;GreedyCover&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Input: sets $S_1,S_2,\ldots,S_m$;&lt;/p&gt;
&lt;p&gt;initially, $U=\bigcup_{i=1}^mS_i$, and $C=\emptyset$, count = 0;&lt;/p&gt;
&lt;p&gt;while $U\neq\emptyset$ and count &amp;lt; k do&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;find $i\in{1,2,\ldots, m}$ with the largest $|S_i\cap U|$;&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;let $C=C\cup{i}$and $U=U-  S_i$;&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;count = count + 1&lt;/p&gt;
&lt;p&gt;return C;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;proof&lt;/strong&gt;:
let $c_i$ denote the elements of selected $S_j$ in the i-th round , and $C_i=\sum_{i=1}^ic_i$, the optimum solution is $OPT$, the remaining elements in $OPT$ is $R_i$, we have: $R_i = OPT - C_i,R_0 = OPT, C_0 =0$.&lt;/p&gt;
&lt;p&gt;At each round , the Greedy algorithm selects the subset $S_j$ with maximum size of uncoverd elements yet,  there exists one set that cover at least $\frac{1}{k}$ fraction of the remaning uncovered elements $R_i$, so we have $c_{i+1} \ge \frac{R_i}{k}$.&lt;/p&gt;
&lt;p&gt;On the other hand, consider  the  conclusion that we want to prove:$C_k  = OPT - R_k \ge (1- (1-\frac{1}{k})^k)\cdot OPT$, that equals to: $R_k \leq (1- \frac{1}{k})^k \cdot OPT$,it is hard to prove it directly, we use  the technique of mathematical induction: $R_i \leq (1- \frac{1}{k})^i \cdot OPT$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for i =0, $R_0 \le OPT$, it is true&lt;/li&gt;
&lt;li&gt;suppose  i, it is true: $R_i \le (1 - \frac{1}{k})^i\cdot OPT$&lt;/li&gt;
&lt;li&gt;when it comes to i+1: $R_{i+1} \leq R_{i} - c_{i+1} \leq R_i - \frac{R_i}{k} = R_i(1- \frac{1}{k}) \le (1- \frac{1}{k})^{i+1}\cdot OPT$&lt;/li&gt;
&lt;li&gt;we have proved it!&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Problem-4&quot;&gt;&lt;a href=&quot;#Problem-4&quot; class=&quot;headerlink&quot; title=&quot;Problem 4&quot;&gt;&lt;/a&gt;Problem 4&lt;/h2&gt;&lt;p&gt;We consider minimum makespan scheduling on parallel identical machines when jobs are subject to &lt;strong&gt;precedence constraints&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We still want to schedule $n$ jobs $j=1,2,\ldots, n$ on $m$ identical machines, where job $j$ has processing time $p_j$. But now a partial order $\preceq$ is defined on jobs, so that if $j\prec k $ then job $j$ must be completely finished before job $k$ begins. The following is a variant of the &lt;em&gt;List algorithm&lt;/em&gt; for this problem: we still assume that the input is a list of $n$ jobs with processing times $p_1,p_2,\ldots, p_n$.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;whenever a machine becomes idle&lt;/p&gt;
&lt;p&gt; &amp;emsp;&amp;emsp;assign the next &lt;em&gt;available job&lt;/em&gt; on the list to the machine;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here a job $k$ is available if all jobs $j\prec k$ have already been completely processed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prove that the approximation ratio is 2.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&quot;My-Solution-3&quot;&gt;&lt;a href=&quot;#My-Solution-3&quot; class=&quot;headerlink&quot; title=&quot;My Solution&quot;&gt;&lt;/a&gt;My Solution&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;proof&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;with &lt;strong&gt;precedence constraints&lt;/strong&gt;,it is known that:
 $$OPT \ge \max_{\mathcal{A}:i \preceq j}p_{\mathcal{A}}, OPT\ge\frac{1}{m}\sum_{j=1}^np_j$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;with &lt;strong&gt;greedy stratege&lt;/strong&gt;,it is known that the max  completion time is less than the sum of the max processing time $p_{max}$ in  precedence constraints and the average processing time $p_{mean}$ among m identical machines, which is:
$$C_{max} \le \max_{\mathcal{A}:i \preceq j}p(\mathcal{A}) + \frac{1}{m}\sum_{j=1}^np_j = 2\cdot OPT$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&quot;Problem-5&quot;&gt;&lt;a href=&quot;#Problem-5&quot; class=&quot;headerlink&quot; title=&quot;Problem 5&quot;&gt;&lt;/a&gt;Problem 5&lt;/h2&gt;&lt;p&gt;For a &lt;strong&gt;hypergraph&lt;/strong&gt; $H(V,E)$ with vertex set $V$, every &lt;em&gt;hyperedge&lt;/em&gt; $e\in E$ is a subset $e\subset V$ of vertices, not necessarily of size 2. A hypergraph $H(V,E)$ is &lt;strong&gt;k-uniform&lt;/strong&gt; if every hyperedge $e\in V$ is of size $k = | e | $.&lt;/p&gt;
&lt;p&gt;A &lt;strong&gt;hypergraph&lt;/strong&gt; $H(V,E)$ is said to have &lt;strong&gt;property B&lt;/strong&gt; (named after Bernstein) if $H$ is 2-coloable; that is, if there is a proper 2-coloring $f:V \to \{ {\color{red}{R},\color{blue}{B}} \}$ which assigns each vertex one of the two colors $\color{red}{Red}$ or $\color{blue}{Blue}$, such that none of the hyperedge is monochromatic.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Let $H(V,E)$ be a &lt;strong&gt;k-uniform hypergraph&lt;/strong&gt; in which every hyperedge $e\in E$ shares vertices with at most d other hyperedges.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Prove that if $2\mathrm{e}\cdot (d+1)\le 2^{k}$, then $H$ has &lt;strong&gt;property B&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Describe how to use &lt;strong&gt;Moser’s recursive Fix&lt;/strong&gt; algorithm to find a proper &lt;strong&gt;2-coloring&lt;/strong&gt; of $H$. Give the pseudocode. Prove the condition in in terms of $d$ and $k$ under which the algorithm can find a 2-coloring of $H$ with high probability.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Describe how to use Moser-Tardos random solver to find a proper 2-coloring of H. Give the pseudocode. Prove the condition in in terms of d and k under which the algorithm can find a 2-coloring of $H$ within bounded expected time. Give an upper bound on the expected running time.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Let $H(V,E)$ be a hypergraph (not necessarily uniform) with at least $n\ge 2 $ vertices satisfying that
$\forall v\in V, \sum_{e\ni v}(1-1/n)^{-|e|}2^{-|e|+1}\le \frac{1}{n}$.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Prove that $H$ has &lt;strong&gt;property B&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Describe how to use Moser-Tardos random solver to find a proper 2-coloring of $H$. Give an &lt;u&gt; upper bound &lt;/u&gt; on the expected running time.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&quot;My-Solution-4&quot;&gt;&lt;a href=&quot;#My-Solution-4&quot; class=&quot;headerlink&quot; title=&quot;My Solution&quot;&gt;&lt;/a&gt;My Solution&lt;/h3&gt;&lt;p&gt;1.&lt;strong&gt;(1)proof&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;if $2\mathrm{e}\cdot (d+1)\le 2^{k}$, it equal to $\mathrm{e}\cdot \frac{1}{2^{k-1}} (d+1)\le 1 \qquad(1)$, on the other hand,  the size of each edge $|e| = k$,  let the probability of one edge cannot be  2-coloable in edge $e_i$ is $Pr[e_i]= 2 / 2^k = 1/2^{k-1} \qquad (2)$, it is a bad thing. From the two formulas above, we know that it  meets &lt;code&gt;Lovász Local Lemma&lt;/code&gt;  condition . So the probability that not all bad things happen is greater than 0. In our problem, it means H has property B.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;(2)algorithm&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;$\phi$: k-CNF of max degree d with m clauses on n variables, $|e| = m$, for clause $C_i$, bad event $A_i: C_i$ is not satisfied, which is the vertices in edge is colored the same color.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$n$ variables: $x_1,x_2,\ldots,x_n, \ n$ is the total number of vertices, $x_i=\{0,1\}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;For each clause C in φ, we denote by $\mathsf{vbl}(C)\subseteq\mathcal{X}$ the set of variables on which C is defined.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We also abuse the notation and denote by $\Gamma(C)=\{\text{clause }D\text{ in }\phi\mid D\neq C, \mathsf{vbl}(C)\cap\mathsf{vbl}(D)\neq\phi\} $ the neighborhood of &lt;strong&gt;C&lt;/strong&gt;, i.e. the set of other clauses in φ that shares variables with C, and $\Gamma^+(C)=\Gamma(C)\cup\{C\}$ the inclusive neighborhood of &lt;strong&gt;C&lt;/strong&gt;,&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Solve($\phi$)&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Pick values of $x_1,x_2\ldots,x_n$ uniformly and independently at random;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;while ∃ unsatisfied clause C  in φ :&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;Fix ($C$);&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fix(C):&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Replace the values of variables in $\mathsf{vbl}(C)$ with uniform and independent random values;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;while ∃ unsatisfied clause D overlapping with C:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;Fix(D);&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;proof&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;first, for the edges number m, we have $m \le 2^{k-1}$, that is $k \ge \log_2^m +1$, the proof is as follows:
the probability of one edge cannot be  2-coloable in edge $e_i$ is $Pr[e_i]= 2 / 2^k = 1/2^{k-1} $, therefore the probability of the bad 2-coloring is:$Pr(\bigwedge_{i=1}^m e_i)\le \sum_{i=1}^mPr(e_i)=\frac{m}{2^{k-1}} \le 1$, so $k \ge \log_2^m +1 \qquad(1)$.&lt;/li&gt;
&lt;li&gt;second,  using Moser’s recursive Fix algorithm, there are &lt;strong&gt;m&lt;/strong&gt; recursion trees, &lt;strong&gt;t&lt;/strong&gt; total nodes, total of random bits is: $n+tk$, the sequence of random bits can be recoverd from &lt;u&gt;final assignment+ reciursion trees, for each recursion tree, the root uses $\left \lceil \log_2m\right \rceil$ bits, each internal node uses at most : $\log_2d + c $ bits&lt;/u&gt;,  which is $m\left \lceil \log_2m\right \rceil + t(\log_2d + c)$, with the &lt;strong&gt;Incompressibility Theorem&lt;/strong&gt;(N uniform random bits cannot be encoded to substantially less than N bits.), we have $n+tk \le m\left \lceil \log_2m\right \rceil + t(\log_2d + c)$,  and it is equal to $t(k-c-\log_2d \le m\left \lceil \log_2m\right \rceil + \log_2n)$, and $k-c-\log_2d &amp;gt;0$, we get : $d\le 2^{k-c} \qquad(2)$,  with &lt;code&gt;Lovász Local Lemma&lt;/code&gt;, we know that $ep(d+1) \leq 1$, and  here is $p = 1/2^{k-1}$, so $d \le \frac{2^{k-1}}{e} - 1\qquad(3)$  ,and the probability the algorithm can find a 2-coloring of H is $n+tk=O(n+km\log_2m)$ .&lt;/li&gt;
&lt;li&gt;using (1) ,(2) and (3), we can get the conditions of d and k: $k \ge \log_2^m +1$,$d \le \frac{2^{k-1}}{e} - 1$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;(3)algorithm&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{X}$ is a set of mutually independent random variables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;RandomSolver:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;sample all $X \in \mathcal{X}$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;while there is non-violated bad event $A \in \mathcal{A}$:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;resample all $X \in vbl(A)$;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;proof&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;from above problem, we know that  $k \ge \log_2^m +1 \qquad(1)$.&lt;/li&gt;
&lt;li&gt;with &lt;code&gt;Lovász Local Lemma&lt;/code&gt;, we know that $d \le \frac{2^{k-1}}{e} - 1\qquad(2)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;(1) proof(unsolved)&lt;/strong&gt;:&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;let $k= |e|$, &lt;strong&gt;m&lt;/strong&gt; edges, &lt;strong&gt;n&lt;/strong&gt; vertices, $n \ge 2$, max degree of dependency graph: &lt;strong&gt;d&lt;/strong&gt;, then we have $2(1-1/n) \ge 1$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$\forall v\in V, \frac{1}{n} \geq \sum_{e \ni v}(1-\frac{1}{n})^{-|e|}2^{-|e|+1} = \sum_{e \ni v}2(2(1-1/n))^{-|e|}= 2 \sum_{i=1}^n \frac{k_i}{2(2-1/n)^{k_i}}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$\forall i, p = \Pr[A_i] \le \frac{2}{2^{\min(k)}} \le \frac{1}{2}, d \le \max(k_i)$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;(2) algorithm(unsolved)&lt;/strong&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;高级算法：第一次作业。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Course" scheme="http://keyunluo.github.io/categories/Course/"/>
    
    
      <category term="高级算法" scheme="http://keyunluo.github.io/tags/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>高级算法(3)--Min-Cut-Max-Flow(2)-近似算法</title>
    <link href="http://keyunluo.github.io/2016/10/06/Course/advanced-algorithm-3.html"/>
    <id>http://keyunluo.github.io/2016/10/06/Course/advanced-algorithm-3.html</id>
    <published>2016-10-06T06:41:02.000Z</published>
    <updated>2017-07-06T08:08:06.888Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;本节重点讨论Karger’s Contraction algorithm解决最小割问题及其运行复杂度分析。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;收缩操作&quot;&gt;&lt;a href=&quot;#收缩操作&quot; class=&quot;headerlink&quot; title=&quot;收缩操作&quot;&gt;&lt;/a&gt;收缩操作&lt;/h2&gt;&lt;p&gt;在上一博客的基础上，讨论收缩操作：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The contraction operator Contract(G, e)&lt;/p&gt;
&lt;p&gt; say e = uv:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;replace $\{u,v\}$ by a new vertex x;&lt;/li&gt;
&lt;li&gt;for every edge (no matter parallel or not) in the form of $uw$ or $vw$ that connects one of $\{u,v\}$ to a vertex $w\in V\setminus\{u,v\}$ in the graph other than $u,v$, replace it by a new edge $xw$;&lt;/li&gt;
&lt;li&gt;the reset of the graph does not change.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;换句话说，$Contract(G, uv)$就是将和$u,v$合并成一个新的点$x$,其他与新点$x$相邻的点所形成的边都将保存下来，因此，即使原图没有并行边，收缩操作也会产生并行边，所以我们把条件放宽至multi-graph。&lt;/p&gt;
&lt;h2 id=&quot;Karger’s-algorithm&quot;&gt;&lt;a href=&quot;#Karger’s-algorithm&quot; class=&quot;headerlink&quot; title=&quot;Karger’s algorithm&quot;&gt;&lt;/a&gt;Karger’s algorithm&lt;/h2&gt;&lt;h3 id=&quot;简单的idea&quot;&gt;&lt;a href=&quot;#简单的idea&quot; class=&quot;headerlink&quot; title=&quot;简单的idea&quot;&gt;&lt;/a&gt;简单的idea&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;在当前的multi-graph中每一步随机选择一条边来收缩直到最终仅剩下两个点为止&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;这两个点之间的并行边一定是这个原始图的一个Cut&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;我们返回这个Cut并且希望有很大几率这个Cut是minimum-Cut&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;伪代码&quot;&gt;&lt;a href=&quot;#伪代码&quot; class=&quot;headerlink&quot; title=&quot;伪代码&quot;&gt;&lt;/a&gt;伪代码&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;RandomContract (Karger 1993)&lt;/p&gt;
&lt;p&gt;Input: multi-graph G(V,E);&lt;/p&gt;
&lt;p&gt;while | V | &amp;gt; 2 do&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;choose an edge ${uv}\in E$ uniformly at random;&lt;/p&gt;
&lt;p&gt;&amp;emsp;&amp;emsp;G = $Contract(G,uv)$;&lt;/p&gt;
&lt;p&gt;return C = E (the parallel edges between the only two vertices in V);&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;从另一个方面说，顶点集$V=\{v_1,v_2,\ldots,v_n\}$,我们从n个顶点类$ S_1,S_2,\ldots, S_n ，其中每一个S_i = {v_i} $仅包含一个顶点开始，通过调用收缩操作$Contract(G,uv),  u \in S_i and v\in S_j , i \neq j$, 我们使用$S_i \bigcup S_j$操作。&lt;/p&gt;
&lt;h3 id=&quot;简单分析&quot;&gt;&lt;a href=&quot;#简单分析&quot; class=&quot;headerlink&quot; title=&quot;简单分析&quot;&gt;&lt;/a&gt;简单分析&lt;/h3&gt;&lt;p&gt;在上述RandomContract算法中，一共有$n-2$个收缩操作，每一个收缩操作可以在$O(n)$时间内完成，因此，时间上限是：$O(n^2)$&lt;/p&gt;
&lt;h2 id=&quot;精度分析&quot;&gt;&lt;a href=&quot;#精度分析&quot; class=&quot;headerlink&quot; title=&quot;精度分析&quot;&gt;&lt;/a&gt;精度分析&lt;/h2&gt;&lt;h3 id=&quot;分析&quot;&gt;&lt;a href=&quot;#分析&quot; class=&quot;headerlink&quot; title=&quot;分析&quot;&gt;&lt;/a&gt;分析&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;约定&lt;/strong&gt;：$ e_1,e_2,\ldots,e_{n-2} $ 为运行&lt;em&gt;RandomContract Algorithm&lt;/em&gt;随机选择的边，$G_1 = G$为原始的输入multi-graph,$G_{i + 1} = Contract(G_i,e_i) ,i = 1,2, \ldots, n-2$, $C$是multi-graph $G$ 中的最小割， $p_C=Pr[C\ is\  returned \  by \  RandomContract]$。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;命题1&lt;/strong&gt;：如果：$e \notin C$，那么$C$仍然是multi-graph $G^{‘} = contract(G,e)$的一个最小割&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;基于上述命题，事件$ \mbox{“}C\mbox{ is returned by }RandomContract\mbox{”}\,$等价于事件$“ e_i \notin C , for\ all\ i=1,2,\ldots,n-2 ”$，因此，根据条件概率中的链式规则，可以得到如下等式：&lt;/p&gt;
&lt;p&gt;$$\begin{array} {lcl}
p_C  &amp;amp; = &amp;amp; Pr[ C\ is\ returned\ by\ RandomContract] \\
      &amp;amp; = &amp;amp; Pr[e_i \notin C for\ all\ i = 1,2,\ldots, n-2] \\
      &amp;amp; = &amp;amp; \prod_{i=1}^{n-2}Pr[ e_i \notin C \vert \forall j &amp;lt; i, e_j \notin C]
\end{array}$$&lt;/p&gt;
&lt;p&gt;即前i-1个边都不在C中，第i个边在上述条件发生的情况下也不在C中，下面考虑上式的上界：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;命题2&lt;/strong&gt;:如果 C 是multi-graph $G(V,E)$中的一个最小割，那么其满足如下约束条件：$degree(G)=2|E|/|V| \geq |C|$，即C的度数小于等于平均每个点的度数。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;已知，$|V_i| = n -i +1， p_i = Pr[e_i \notin C \vert \forall j &amp;lt; i, e_j \notin C]$,根据命题2，可得：&lt;/p&gt;
&lt;p&gt;$$\begin{array} {lcl}
p_i  &amp;amp; = &amp;amp; 1 - \frac{\vert C \vert}{\vert E_i \vert} \\
      &amp;amp; \geq &amp;amp; 1 - \frac{2}{|V_i|} \\
      &amp;amp; = &amp;amp; 1- \frac{2}{n-i+1}
\end{array}$$&lt;/p&gt;
&lt;p&gt;因此，得到最终结果：&lt;/p&gt;
&lt;p&gt;$$\begin{array} {lcl}
p_{\text{correct}} &amp;amp; = &amp;amp; \Pr[\,\text{a minimum cut is returned by }RandomContract\,]\\
                    &amp;amp; \geq &amp;amp; \Pr[\,C\mbox{ is returned by }{RandomContract}\,]\\
                    &amp;amp; = &amp;amp; \Pr[\,e_i\not\in C\mbox{ for all }i=1,2,\ldots,n-2\,]\\
                    &amp;amp; = &amp;amp; \prod_{i=1}^{n-2}\Pr[e_i\not\in C\mid \forall j&amp;lt;i, e_j\not\in C]\\
                    &amp;amp; \geq &amp;amp; \prod_{i=1}^{n-2}\left(1-\frac{2}{n-i+1}\right)\\
                    &amp;amp; = &amp;amp; \prod_{k=3}^{n}\frac{k-2}{k}\\
                    &amp;amp; = &amp;amp; \frac{2}{n(n-1)}.
\end{array}$$&lt;/p&gt;
&lt;h3 id=&quot;定理&quot;&gt;&lt;a href=&quot;#定理&quot; class=&quot;headerlink&quot; title=&quot;定理&quot;&gt;&lt;/a&gt;定理&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;定理&lt;/strong&gt;：对于任意具有n个顶点的多图，随机收缩算法返回一个最小割的概率至少是$\frac{2}{n(n-1)}$,当我们独立地运行$t=\frac{n(n-1)\ln n}{2}$次时，一个最小割被找到的概率为：
$$\begin{array} {lcl}
&amp;amp;\quad &amp;amp; 1-\Pr[\,\mbox{all }t\mbox{ independent runnings of } RandomContract\mbox{ fails to find a min-cut}\,] \\
&amp;amp; = &amp;amp; 1-\Pr[\,\mbox{a single running of }{RandomContract}\mbox{ fails}\,]^{t} \\
&amp;amp; \geq &amp;amp; 1- \left(1-\frac{2}{n(n-1)}\right)^{\frac{n(n-1)\ln n}{2}} \\
&amp;amp; \geq &amp;amp; 1-\frac{1}{n}.
\end{array}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;随机收缩算法时间复杂度为$O(n^2)$,所以总的时间复杂度为：$O(n^4\log n)$&lt;/p&gt;
&lt;h3 id=&quot;推论&quot;&gt;&lt;a href=&quot;#推论&quot; class=&quot;headerlink&quot; title=&quot;推论&quot;&gt;&lt;/a&gt;推论&lt;/h3&gt;&lt;p&gt;根据上述定理，一个随机收缩算法找到一个正确的最小割的概率至少是$\frac{2}{n(n-1)}$，则最小割至多有$\frac{n(n-1)}{2}$个。&lt;/p&gt;
&lt;h2 id=&quot;Fast-Min-Cut&quot;&gt;&lt;a href=&quot;#Fast-Min-Cut&quot; class=&quot;headerlink&quot; title=&quot;Fast Min-Cut&quot;&gt;&lt;/a&gt;Fast Min-Cut&lt;/h2&gt;&lt;p&gt;在随机收缩算法中，$p_C \ge\prod_{i=1}^{n-2}\left(1-\frac{2}{n-i+1}\right).$,随着i增加$\left(1-\frac{2}{n-i+1}\right)$值减小，这就意味着成功的概率会随着收缩程度的增加而不断减小，此时剩余的点变少。&lt;/p&gt;
&lt;p&gt;因此，我们考虑如下的分段算法：首先使用随机收缩算法减少顶点的个数到一个相当小的程度，然后在每一个小的实例里寻找最小割，这样的算法称为FastCut。&lt;/p&gt;
&lt;h3 id=&quot;FastCut算法&quot;&gt;&lt;a href=&quot;#FastCut算法&quot; class=&quot;headerlink&quot; title=&quot;FastCut算法&quot;&gt;&lt;/a&gt;FastCut算法&lt;/h3&gt;&lt;h3 id=&quot;算法&quot;&gt;&lt;a href=&quot;#算法&quot; class=&quot;headerlink&quot; title=&quot;算法&quot;&gt;&lt;/a&gt;算法&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;预处理算法：RandomContract&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;RandomContract(G,t)&lt;/p&gt;
&lt;p&gt;Input: multi-graph $G(V,E)$, and integer $t \ge 2$;
while $| V | &amp;gt; t$ do&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;choose an edge ${uv}\in E$ uniformly at random;&lt;/li&gt;
&lt;li&gt;$G = Contract(G,uv)$;
return $G$;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;FastCut算法&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;FastCut(G)&lt;/p&gt;
&lt;p&gt;Input: multi-graph $G(V,E)$;&lt;/p&gt;
&lt;p&gt;if $|V|\le 6$ then return a mincut by brute force;&lt;/p&gt;
&lt;p&gt;else let $t=\left\lceil1+|V|/\sqrt{2}\right\rceil$;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$G_1$ = $RandomContract(G,t)$;&lt;/li&gt;
&lt;li&gt;$G_2$ = $RandomContract(G,t)$;&lt;/li&gt;
&lt;li&gt;return the smaller one of $FastCut(G_1)$ and $FastCut(G_2)$;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;分析-1&quot;&gt;&lt;a href=&quot;#分析-1&quot; class=&quot;headerlink&quot; title=&quot;分析&quot;&gt;&lt;/a&gt;分析&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;随机收缩过程&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;$$\begin{array} {lcl}
&amp;amp;\quad &amp;amp; \Pr[C\text{ survives all contractions in }RandomContract(G,t)]\\
&amp;amp; = &amp;amp; \prod_{i=1}^{n-t}\Pr[C\text{ survives the }i\text{-th contraction}\mid C\text{ survives the first }(i-1)\text{-th contractions}]\\
&amp;amp; \ge &amp;amp; \prod_{i=1}^{n-t}\left(1-\frac{2}{n-i+1}\right)\\
&amp;amp; = &amp;amp; \prod_{k=t+1}^{n}\frac{k-2}{k}\\
&amp;amp; = &amp;amp; \frac{t(t-1)}{n(n-1)}.
\end{array}$$&lt;/p&gt;
&lt;p&gt;当$t=\left\lceil1+n/\sqrt{2}\right\rceil$, 上式概率至少是$ 1 / 2$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FastCut过程&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;p(n)为C被FastCut(G)返回的概率：&lt;/p&gt;
&lt;p&gt;$$\begin{array} {lcl}
p(n) &amp;amp; = &amp;amp; \Pr[C\text{ is returned by }\textit{FastCut}(G)]\\
     &amp;amp; = &amp;amp; 1-\left(1-\Pr[C\text{ survives in }G_1\wedge C=\textit{FastCut}(G_1)]\right)^2\\
     &amp;amp; = &amp;amp; 1-\left(1-\Pr[C\text{ survives in }G_1]\Pr[C=\textit{FastCut}(G_1)\mid C\text{ survives in }G_1]\right)^2\\
     &amp;amp; \ge &amp;amp; 1-\left(1-\frac{1}{2}p\left(\left\lceil1+n/\sqrt{2}\right\rceil\right)\right)^2,
\end{array}$$&lt;/p&gt;
&lt;p&gt;基本案例是$ p(n) = 1\ for\ n\le 6.$&lt;/p&gt;
&lt;p&gt;$$p(n)=\Omega\left(\frac{1}{\log n}\right).$$&lt;/p&gt;
&lt;p&gt;时间复杂度为：&lt;/p&gt;
&lt;p&gt;$$T(n)=2T\left(\left\lceil1+n/\sqrt{2}\right\rceil\right)+O(n^2),$$&lt;/p&gt;
&lt;p&gt;$T(n) = O(1)\ for\ n\le 6$, 因此$ T(n) = O(n^2logn)$.&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;本节重点讨论Karger’s Contraction algorithm解决最小割问题及其运行复杂度分析。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Course" scheme="http://keyunluo.github.io/categories/Course/"/>
    
    
      <category term="高级算法" scheme="http://keyunluo.github.io/tags/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>高级算法(2)--Min-Cut-Max-Flow(1)-确定性算法</title>
    <link href="http://keyunluo.github.io/2016/10/05/Course/advanced-algorithm-2.html"/>
    <id>http://keyunluo.github.io/2016/10/05/Course/advanced-algorithm-2.html</id>
    <published>2016-10-05T05:41:02.000Z</published>
    <updated>2017-07-06T08:08:06.888Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;对于一个图中的两个节点来说，如果把图中的一些边去掉，刚好让他们之间无法连通的话，这些被去掉的边组成的集合就叫做割了，最小割就是指所有割中权重之和最小的一个割。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;图的割&quot;&gt;&lt;a href=&quot;#图的割&quot; class=&quot;headerlink&quot; title=&quot;图的割&quot;&gt;&lt;/a&gt;图的割&lt;/h2&gt;&lt;h3 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h3&gt;&lt;p&gt;对于无向图 $G(V,E)$ 以及边的子集 $C \subseteq E$, 如果把C中所有的边都删除后图G不再连通则称边的子集C是图G的一个割。更加严格地，我们考虑割集断开了图的剩余部分的顶点的一个子集S。&lt;/p&gt;
&lt;p&gt;一对不相交集合： $S,T \subseteq V$ 如果S和T不为空，并且$S \bigcup T = V$成立，那么S,T称作V的二划分。&lt;/p&gt;
&lt;p&gt;给定一个顶点V的二划分 $\{S,T\}$，一个割集C定义如下：&lt;/p&gt;
&lt;p&gt;$$C = E(S,T), 其中 E(S,T) = \{ uv \in E \vert u \in S , v \in T \}$$&lt;/p&gt;
&lt;p&gt;对于一个图G，割可能有很多个，我们对找出最小的割集和最大的割集感兴趣。&lt;/p&gt;
&lt;h3 id=&quot;最小割&quot;&gt;&lt;a href=&quot;#最小割&quot; class=&quot;headerlink&quot; title=&quot;最小割&quot;&gt;&lt;/a&gt;最小割&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;输入：无向图 $G(V,E)$ , 输出：一个G中边数最小的割C。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;也就是说，将顶点集合$V$二划分成两个不相交的非空集合$\{S，T\}$，使得 $E|(S,T)|$ 最小。&lt;/p&gt;
&lt;p&gt;更一般地，允许点$(u,v)$之间存在不止一条边，即并行边，这样的图称为&lt;strong&gt;multi-graphs&lt;/strong&gt;,此时割C包括并行边。&lt;/p&gt;
&lt;h2 id=&quot;流网络&quot;&gt;&lt;a href=&quot;#流网络&quot; class=&quot;headerlink&quot; title=&quot;流网络&quot;&gt;&lt;/a&gt;流网络&lt;/h2&gt;&lt;h3 id=&quot;定义-1&quot;&gt;&lt;a href=&quot;#定义-1&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h3&gt;&lt;p&gt;流网络（Flow Networks）指的是一个有向图 $G = (V, E)$，其中每条边 $(u, v) ∈ E$ 均有一非负容量 $c(u, v) ≥ 0$。如果 $(u, v) ∉ E$ 则可以规定 $c(u, v) = 0$。流网络中有两个特殊的顶点：源点 s （source）和汇点 t（sink）。为方便起见，假定每个顶点均处于从源点到汇点的某条路径上，就是说，对每个顶点 $v ∈ E$，存在一条路径 s –&amp;gt; v –&amp;gt; t。因此，图 G 为连通图，且 $|E| ≥ |V| - 1$。&lt;/p&gt;
&lt;h3 id=&quot;流-Flow-的基本性质&quot;&gt;&lt;a href=&quot;#流-Flow-的基本性质&quot; class=&quot;headerlink&quot; title=&quot;流(Flow)的基本性质&quot;&gt;&lt;/a&gt;流(Flow)的基本性质&lt;/h3&gt;&lt;p&gt;设$C_{uv}$代表边u到v最大允许流量(Capacity), $f_{uv}$代表u到v的当前流量, 那么有一下两个性质:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;$(u, v)$为有向图边, $0 \leq f_{uv} \leq C_{uv}$, 即对于所有的边, 当前流量不允许超过其Capacity&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;除了$s, t$之外, 对所有节点有 $\sum\limits_{(v, u)}f_{wu} = \sum\limits_{(u, v)}f_{uv}$, 即对于任何一点, 流入该点的流量等于留出该点的流量, 流量守恒原则(类似与能量守恒的概念)。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;相关概念&quot;&gt;&lt;a href=&quot;#相关概念&quot; class=&quot;headerlink&quot; title=&quot;相关概念&quot;&gt;&lt;/a&gt;相关概念&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;残存网络 ：给定网络G和流量f, 残存网络$G_f$由那些仍有空间对流量进行调整的边构成.&lt;strong&gt;残存网络 = 容量网络capacity - 流量网络flow&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;增广路径：增广路径p是残存网络中一条从源节点s到汇点t的简单路径,在一条增广路径p上能够为每条边增加的流量的最大值为路径p的残存容量$c_f(p) = min {c_f(u,v):(u,v) \in p }$。 在一条增广路径p上, 要增加整条增广路径的水流量, 则必须看最小能承受水流量的管道, 不然水管会爆掉, 这最小承受水流量就是&lt;strong&gt;残存容量&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;最大流&quot;&gt;&lt;a href=&quot;#最大流&quot; class=&quot;headerlink&quot; title=&quot;最大流&quot;&gt;&lt;/a&gt;最大流&lt;/h3&gt;&lt;p&gt;流的值定义为：$|f| =Σ_{v∈V}f(s, v)$，即从源点 s 出发的总流。&lt;/p&gt;
&lt;p&gt;最大流问题（Maximum-flow problem）中，给出源点 s 和汇点 t 的流网络 G，希望找出从 s 到 t 的最大值流。&lt;/p&gt;
&lt;h2 id=&quot;最小割最大流定理&quot;&gt;&lt;a href=&quot;#最小割最大流定理&quot; class=&quot;headerlink&quot; title=&quot;最小割最大流定理&quot;&gt;&lt;/a&gt;最小割最大流定理&lt;/h2&gt;&lt;h3 id=&quot;定理&quot;&gt;&lt;a href=&quot;#定理&quot; class=&quot;headerlink&quot; title=&quot;定理&quot;&gt;&lt;/a&gt;定理&lt;/h3&gt;&lt;p&gt;如果 f 是具有源点 s 和汇点 t 的流网络 $G = (V, E)$ 中的一个流，则下列条件是等价的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;f 是 G 的一个最大流。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;残留网络 $G_f$ 不包含增广路径。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;对 G 的某个割 (S, T)，有 |f| = c(S, T)。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;算法&quot;&gt;&lt;a href=&quot;#算法&quot; class=&quot;headerlink&quot; title=&quot;算法&quot;&gt;&lt;/a&gt;算法&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;min=MAXINT,确定一个源点&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;枚举汇点&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;计算最大流，并确定当前源汇的最小割集，若比min小更新min&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;转到2直到枚举完毕&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;min即为所求，输出min&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不难看出复杂度很高：枚举汇点要$O(n)$，最短增广路最大流算法求最大流是$O((n^2)m)$复杂度，在复杂网络中$O(m)=O(n^2)$，算法总复杂度就是$O(n^5)$；哪怕采用最高标号预进流算法求最大流$O((n^2)(m^{0.5}))$，算法总复杂度也要$O(n^4)$&lt;/p&gt;
&lt;h2 id=&quot;最小割的确定性算法&quot;&gt;&lt;a href=&quot;#最小割的确定性算法&quot; class=&quot;headerlink&quot; title=&quot;最小割的确定性算法&quot;&gt;&lt;/a&gt;最小割的确定性算法&lt;/h2&gt;&lt;p&gt;目前已知最好的求最小割的确定性算法是&lt;strong&gt; Stoer–Wagner algorithm&lt;/strong&gt; $(O(mn + n^2\log n), m=|E|)$,包含并行边&lt;/p&gt;
&lt;h3 id=&quot;算法思路：&quot;&gt;&lt;a href=&quot;#算法思路：&quot; class=&quot;headerlink&quot; title=&quot;算法思路：&quot;&gt;&lt;/a&gt;算法思路：&lt;/h3&gt;&lt;p&gt;1.min=MAXINT，固定一个顶点P&lt;/p&gt;
&lt;p&gt;2.从点P用“类似”prim的s算法扩展出“最大生成树”，记录最后扩展的顶点和最后扩展的边&lt;/p&gt;
&lt;p&gt;3.计算最后扩展到的顶点的切割值（即与此顶点相连的所有边权和），若比min小更新min&lt;/p&gt;
&lt;p&gt;4.合并最后扩展的那条边的两个端点为一个顶点（当然他们的边也要合并，这个好理解吧？）&lt;/p&gt;
&lt;p&gt;5.转到2，合并N-1次后结束&lt;/p&gt;
&lt;p&gt;6.min即为所求，输出min&lt;/p&gt;
&lt;p&gt;prim本身复杂度是$O(n^2)$，合并n-1次，算法复杂度即为$O(n^3)$&lt;/p&gt;
&lt;p&gt;如果在prim中加堆优化，复杂度会降为$O((n^2)logn)$&lt;/p&gt;
&lt;p&gt;其核心思想是迭代缩小规模, 算法基于这样一个事实:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对于图中任意两点s和t, 它们要么属于最小割的两个不同集中, 要么属于同一个集.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果是后者, 那么合并s和t后并不影响最小割. 基于这么个思想, 如果每次能求出图中某两点之间的最小割, 然后更新答案后合并它们再继续求最小割, 就得到最终答案了. 算法步骤如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;设最小割cut=INF, 任选一个点s到集合A中, 定义W(A, p)为A中的所有点到A外一点p的权总和.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;对刚才选定的s, 更新W(A,p)(该值递增).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;选出A外一点p, 且W(A,p)最大的作为新的s, 若A!=G(V), 则继续2.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;把最后进入A的两点记为s和t, 用W(A,t)更新cut.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;新建顶点u, 边权w(u, v)=w(s, v)+w(t, v), 删除顶点s和t, 以及与它们相连的边.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;若|V|!=1则继续1.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一个很详细的博客&lt;a href=&quot;http://blog.coolstack.cc/2016/01/08/%E6%9C%9D%E8%8A%B1%E5%A4%95%E6%8B%BE%EF%BC%9A%E6%97%A0%E5%90%91%E5%9B%BE%E5%85%A8%E5%B1%80%E6%9C%80%E5%B0%8F%E5%89%B2/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;朝花夕拾：无向图全局最小割&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;基本步骤：&quot;&gt;&lt;a href=&quot;#基本步骤：&quot; class=&quot;headerlink&quot; title=&quot;基本步骤：&quot;&gt;&lt;/a&gt;基本步骤：&lt;/h3&gt;&lt;p&gt;用wage数组记录点的连通度，vis数组标记点是否在集合里面，In数组表示点被其它点合并。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;一：找到S - T的最小割Mincut，其中S 和 T为最后并入集合的两个点。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1，初始化数组vis 和 wage；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2，遍历所有不在集合且没有被合并的点，找到最大wage值的点Next，并记录Mincut、S和T；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;3，Next并入集合，叠加与Next相连的所有点（不在集合 且 没有被合并），更新这些点的wage值；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;4，重复操作2和3一共N次 或者 找不到新的Next值时 跳出，返回Mincut；&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;二、找全局最小割ans，需要重复第一步N-1次，因为每次合并一个点，最多合并N-1个点；&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;1，每次对返回的Mincut，更新ans = min(ans, Mincut)，当然ans为0时，说明图不连通；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;2，把点T合并到S点，操作有：对 所有没有被合并的点j，Map[S][j] += Map[T][j]。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;例子&quot;&gt;&lt;a href=&quot;#例子&quot; class=&quot;headerlink&quot; title=&quot;例子&quot;&gt;&lt;/a&gt;例子&lt;/h3&gt;&lt;p&gt;来自&lt;a href=&quot;http://blog.csdn.net/i_love_home/article/details/9698791&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;CSDN博客&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;步骤: &lt;img src=&quot;/resource/blog/2016-10/mincut.jpg&quot; alt=&quot;寻找 s, t 两点，然后合并于 s 点&quot;&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;66&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;68&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;69&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;70&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;71&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;72&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;73&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;74&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;75&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;76&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;77&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;78&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;79&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;80&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;include&lt;/span&gt; &lt;span class=&quot;meta-string&quot;&gt;&amp;lt;cstdio&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;include&lt;/span&gt; &lt;span class=&quot;meta-string&quot;&gt;&amp;lt;cstring&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;include&lt;/span&gt; &lt;span class=&quot;meta-string&quot;&gt;&amp;lt;algorithm&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;define&lt;/span&gt; MAXN 500+10&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;define&lt;/span&gt; INF 0x3f3f3f3f&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;namespace&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;std&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; Map[MAXN][MAXN];&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt; vis[MAXN];&lt;span class=&quot;comment&quot;&gt;//是否已并入集合&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; wage[MAXN];&lt;span class=&quot;comment&quot;&gt;//记录每个点的连通度&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt; In[MAXN];&lt;span class=&quot;comment&quot;&gt;//该点是否已经合并到其它点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; N, M;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;getMap&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;memset&lt;/span&gt;(Map, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;keyword&quot;&gt;sizeof&lt;/span&gt;(Map));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; a, b, c;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;; i &amp;lt; M; i++)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;built_in&quot;&gt;scanf&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&quot;%d%d%d&quot;&lt;/span&gt;, &amp;amp;a, &amp;amp;b, &amp;amp;c);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        a++, b++;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        Map[a][b] += c;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        Map[b][a] += c;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; S, T;&lt;span class=&quot;comment&quot;&gt;//记录每次找s-t割  所遍历的最后两个点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;work&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; Mincut;&lt;span class=&quot;comment&quot;&gt;//每一步找到的s-t割&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;memset&lt;/span&gt;(wage, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;keyword&quot;&gt;sizeof&lt;/span&gt;(wage));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;memset&lt;/span&gt;(vis, &lt;span class=&quot;literal&quot;&gt;false&lt;/span&gt;, &lt;span class=&quot;keyword&quot;&gt;sizeof&lt;/span&gt;(vis));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; Next;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;; i &amp;lt;= N; i++)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; Max = -INF;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; j = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;; j &amp;lt;= N; j++)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(!In[j] &amp;amp;&amp;amp; !vis[j] &amp;amp;&amp;amp; Max &amp;lt; wage[j])&lt;span class=&quot;comment&quot;&gt;//找最大的wage值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                Next = j;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                Max = wage[j];&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(Next == T) &lt;span class=&quot;keyword&quot;&gt;break&lt;/span&gt;;&lt;span class=&quot;comment&quot;&gt;//找不到点 图本身不连通&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        vis[Next] = &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;;&lt;span class=&quot;comment&quot;&gt;//标记 已经并入集合&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        Mincut = Max;&lt;span class=&quot;comment&quot;&gt;//每次更新&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        S = T, T = Next;&lt;span class=&quot;comment&quot;&gt;// 记录前、后点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; j = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;; j &amp;lt;= N; j++)&lt;span class=&quot;comment&quot;&gt;//继续找不在集合 且 没有被合并过的点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(In[j] || vis[j]) &lt;span class=&quot;keyword&quot;&gt;continue&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            wage[j] += Map[Next][j];&lt;span class=&quot;comment&quot;&gt;//累加 连通度&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; Mincut;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Stoer_wagner&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;memset&lt;/span&gt;(In, &lt;span class=&quot;literal&quot;&gt;false&lt;/span&gt;, &lt;span class=&quot;keyword&quot;&gt;sizeof&lt;/span&gt;(In));&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; ans = INF;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; i = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;; i &amp;lt; N&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;; i++)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ans = min(ans, work());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(ans == &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;) &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;&lt;span class=&quot;comment&quot;&gt;//本身不连通&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        In[T] = &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt;(&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; j = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;; j &amp;lt;= N; j++)&lt;span class=&quot;comment&quot;&gt;//把T点合并到S点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt;(In[j]) &lt;span class=&quot;keyword&quot;&gt;continue&lt;/span&gt;;&lt;span class=&quot;comment&quot;&gt;//已经合并&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            Map[S][j] += Map[T][j];&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            Map[j][S] += Map[j][T];&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; ans;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;while&lt;/span&gt;(&lt;span class=&quot;built_in&quot;&gt;scanf&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&quot;%d%d&quot;&lt;/span&gt;, &amp;amp;N, &amp;amp;M) != EOF)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        getMap();&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;built_in&quot;&gt;printf&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&quot;%d\n&quot;&lt;/span&gt;, Stoer_wagner());&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;对于一个图中的两个节点来说，如果把图中的一些边去掉，刚好让他们之间无法连通的话，这些被去掉的边组成的集合就叫做割了，最小割就是指所有割中权重之和最小的一个割。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Course" scheme="http://keyunluo.github.io/categories/Course/"/>
    
    
      <category term="高级算法" scheme="http://keyunluo.github.io/tags/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>高级算法(1)--NP完全性与近似算法</title>
    <link href="http://keyunluo.github.io/2016/10/04/Course/advanced-algorithm-1.html"/>
    <id>http://keyunluo.github.io/2016/10/04/Course/advanced-algorithm-1.html</id>
    <published>2016-10-04T12:41:02.000Z</published>
    <updated>2017-07-06T08:08:06.888Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;本学期修读了尹一通老师的高级算法课，感觉略有困难，故将学习过程中的知识点以博客的形式记录下来，温故而知新，本节总结NP完全性问题和近似算法的概念。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;NP完全理论&quot;&gt;&lt;a href=&quot;#NP完全理论&quot; class=&quot;headerlink&quot; title=&quot;NP完全理论&quot;&gt;&lt;/a&gt;NP完全理论&lt;/h2&gt;&lt;p&gt;NP理论如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;多项式时间&lt;/strong&gt;：一个问题的计算时间m(n)不大于问题大小n的多项式倍数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;判定问题&lt;/strong&gt;：判定某类问题是否具有算法解，或者是否存在能行性的方法使得对该问题类中的每一个特例能在有限步内机械地判定它是否具有某种性质的问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;P类问题&lt;/strong&gt;：在多项式时间内求解的判定问题构成P类问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;非确定性算法&lt;/strong&gt;：非确定性算法将问题分解成猜测和验证两个阶段。算法的猜测阶段是非确定性的，算法的验证阶段是确定性的，它验证猜测阶段给出解的正确性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;NP类问题&lt;/strong&gt;：非确定性多项式时间可解的判定问题构成NP类问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;NPC问题&lt;/strong&gt;：NP中的某些问题的复杂性与整个类的复杂性相关联。这些问题中任何一个如果存在多项式时间的算法,那么所有NP问题都是多项式时间可解的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;NPH问题&lt;/strong&gt;：某个问题被称作NP困难，当所有NP问题可以在多项式时间归约到这个问题。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;它们之间的关系：&lt;/p&gt;
&lt;p&gt;$$P \subseteq NP, NPC \subseteq NP, NPC \subseteq NPH$$&lt;/p&gt;
&lt;h2 id=&quot;近似算法&quot;&gt;&lt;a href=&quot;#近似算法&quot; class=&quot;headerlink&quot; title=&quot;近似算法&quot;&gt;&lt;/a&gt;近似算法&lt;/h2&gt;&lt;p&gt;在计算复杂性理论中的某些假设下，比如最著名的 $ P\neq NP  $假设下，对于一些可已被证明为NP完全的优化问题，无法在多项式时间内精确求到最优解，然而在现实或理论研究中，这类问题都有广泛的应用，在精确解无法得到的情况下，转而依靠高效的近似算法求可以接受的近似解。&lt;/p&gt;
&lt;h3 id=&quot;近似比&quot;&gt;&lt;a href=&quot;#近似比&quot; class=&quot;headerlink&quot; title=&quot;近似比&quot;&gt;&lt;/a&gt;近似比&lt;/h3&gt;&lt;p&gt;对于一个最大化问题的实例，设其最优解是 $ OPT $，某个近似算法的解是 $x$，若下式成立，&lt;/p&gt;
&lt;p&gt;$$\alpha \leq \frac{x}{OPT} \leq 1 $$&lt;/p&gt;
&lt;p&gt;其中 $ 0&amp;lt;\alpha &amp;lt;1 $ 则定义此近似算法的近似比为 $ \alpha $。&lt;/p&gt;
&lt;p&gt;相应的，对于一个最小化问题的实例，&lt;/p&gt;
&lt;p&gt;$$\alpha \geq \frac{x}{OPT} \geq 1 $$&lt;/p&gt;
&lt;p&gt;其中 $\alpha &amp;gt;1$则定义此近似算法的近似比为 $ \alpha $。&lt;/p&gt;
&lt;h3 id=&quot;分类&quot;&gt;&lt;a href=&quot;#分类&quot; class=&quot;headerlink&quot; title=&quot;分类&quot;&gt;&lt;/a&gt;分类&lt;/h3&gt;&lt;p&gt;按照可以达到近似比的不同，可以将近似算法大致按以下分类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;FPTAS(Fully Polynomial-Time Approximation Scheme):FPTAS要求算法对问题规模n和1/ε都是多项式时间复杂度的，称为完全多项式时间近似模式，如$O((1/ε)^2n^3)$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;PTAS(Polynomial-Time Approximation Scheme):该算法的输入为问题的实例以及一个任意小的值ε &amp;gt; 0，而算法的结果是一个近似度为 1 + ε的可行解；并且对于每一个固定的ε，算法运行时间复杂度对于实例的规模n是多项式时间的,如$O(n^{2/ε})$。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;常数近似&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;对数的多项式&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;多项式&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中对数的多项式和多项式都是对应于输入规模的。&lt;/p&gt;
&lt;h3 id=&quot;设计方法&quot;&gt;&lt;a href=&quot;#设计方法&quot; class=&quot;headerlink&quot; title=&quot;设计方法&quot;&gt;&lt;/a&gt;设计方法&lt;/h3&gt;&lt;p&gt;近似算法的常用设计方法有贪心法，线性规划、半正定规划的松弛和取整，随机算法等。&lt;/p&gt;
&lt;h2 id=&quot;课程网站&quot;&gt;&lt;a href=&quot;#课程网站&quot; class=&quot;headerlink&quot; title=&quot;课程网站&quot;&gt;&lt;/a&gt;课程网站&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;http://tcs.nju.edu.cn/wiki/index.php/%E9%9A%8F%E6%9C%BA%E7%AE%97%E6%B3%95_%5C_%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95_(Fall_2016%29&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;随机算法/高级算法&lt;/a&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;本学期修读了尹一通老师的高级算法课，感觉略有困难，故将学习过程中的知识点以博客的形式记录下来，温故而知新，本节总结NP完全性问题和近似算法的概念。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Course" scheme="http://keyunluo.github.io/categories/Course/"/>
    
    
      <category term="高级算法" scheme="http://keyunluo.github.io/tags/%E9%AB%98%E7%BA%A7%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04上编译OpenCV的Python3包</title>
    <link href="http://keyunluo.github.io/2016/10/04/Tools/ubuntu16.04-opencv3-python3.html"/>
    <id>http://keyunluo.github.io/2016/10/04/Tools/ubuntu16.04-opencv3-python3.html</id>
    <published>2016-10-04T05:41:02.000Z</published>
    <updated>2017-07-06T08:08:07.312Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;本篇博文记录在Ubuntu16.04上安装OpenCV。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;安装依赖软件&quot;&gt;&lt;a href=&quot;#安装依赖软件&quot; class=&quot;headerlink&quot; title=&quot;安装依赖软件&quot;&gt;&lt;/a&gt;安装依赖软件&lt;/h2&gt;&lt;p&gt;从源代码编译时需要很多第三方软件，因此首先需要在系统中安装这些库，具体如下:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sudo apt-get install build-essential cmake cmake-gui git libjpeg-dev libpng-dev       \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;libtiff5-dev libjasper-dev libavcodec-dev libavformat-dev libswscale-dev pkg-config   \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;libgtk2&lt;span class=&quot;number&quot;&gt;.0&lt;/span&gt;-dev libeigen3-dev libtheora-dev libvorbis-dev libxvidcore-dev libx264-dev   \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sphinx-common libtbb-dev yasm libfaac-dev libopencore-amrnb-dev libopencore-amrwb-dev \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;libopenexr-dev libgstreamer-plugins-base1&lt;span class=&quot;number&quot;&gt;.0&lt;/span&gt;-dev libavcodec-dev libavutil-dev \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;libavfilter-dev libavformat-dev libavresample-dev libgphoto2-dev libhdf5-mpi-dev \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;libhdf5-openmpi-dev libhdf5-mpich-dev&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;安装好这些库后便可以从&lt;code&gt;github&lt;/code&gt;上&lt;code&gt;clone&lt;/code&gt;最新的代码:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;mkdir build &amp;amp;&amp;amp; cd build&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git clone --branch 3.1.0 --depth 1 https://github.com/Itseez/opencv.git&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git clone --branch 3.1.0 --depth 1 https://github.com/itseez/opencv_contrib&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;建立编译目录：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cd opencv &amp;amp;&amp;amp; mkdir release&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cd release&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;编译&quot;&gt;&lt;a href=&quot;#编译&quot; class=&quot;headerlink&quot; title=&quot;编译&quot;&gt;&lt;/a&gt;编译&lt;/h2&gt;&lt;p&gt;接上，若使用Anaconda Python，则指定Python库所在的位置，这里是&lt;code&gt;/usr/local/anaconda3/lib/libpython3.5m.so&lt;/code&gt;：在终端下运行：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cmake -D OPENCV_EXTRA_MODULES_PATH=../../opencv_contrib/modules/  -DBUILD_TIFF=ON \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-DBUILD_opencv_java=ON -DWITH_CUDA=OFF -DENABLE_AVX=ON -DWITH_OPENGL=ON -DWITH_OPENCL=ON \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-DWITH_IPP=ON -DWITH_TBB=ON -DWITH_EIGEN=ON -DWITH_V4L=ON -DBUILD_TESTS=OFF \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-DBUILD_PERF_TESTS=OFF -DCMAKE_BUILD_TYPE=RELEASE -DCMAKE_INSTALL_PREFIX=$(python3 -c &lt;span class=&quot;string&quot;&gt;&quot;import sys; print(sys.prefix)&quot;&lt;/span&gt;) \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-DPYTHON_EXECUTABLE=$(which python3) \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-DPYTHON_INCLUDE_DIR=$(python3 -c &lt;span class=&quot;string&quot;&gt;&quot;from distutils.sysconfig import get_python_inc; print(get_python_inc())&quot;&lt;/span&gt;) \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-DPYTHON_PACKAGES_PATH=$(python3 -c &lt;span class=&quot;string&quot;&gt;&quot;from distutils.sysconfig import get_python_lib; print(get_python_lib())&quot;&lt;/span&gt;) \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;-DPYTHON_LIBRARY=/usr/local/anaconda3/lib/libpython3&lt;span class=&quot;number&quot;&gt;.5&lt;/span&gt;m.so ..&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;没有报错后，接着便&lt;code&gt;make&lt;/code&gt;和&lt;code&gt;make install&lt;/code&gt; :&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;make &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo make install&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;使用&quot;&gt;&lt;a href=&quot;#使用&quot; class=&quot;headerlink&quot; title=&quot;使用&quot;&gt;&lt;/a&gt;使用&lt;/h2&gt;&lt;p&gt;安装完成后，测试是否安装成功:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; cv2 &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;x = cv2.bgsegm.createBackgroundSubtractorGMG()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;本篇博文记录在Ubuntu16.04上安装OpenCV。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="MachineLearning" scheme="http://keyunluo.github.io/categories/MachineLearning/"/>
    
    
      <category term="OpenCV" scheme="http://keyunluo.github.io/tags/OpenCV/"/>
    
  </entry>
  
  <entry>
    <title>数据压缩(1) —— 概述</title>
    <link href="http://keyunluo.github.io/2016/09/24/Algorithm/compress-1.html"/>
    <id>http://keyunluo.github.io/2016/09/24/Algorithm/compress-1.html</id>
    <published>2016-09-24T06:25:02.000Z</published>
    <updated>2017-07-06T08:08:06.869Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt; 数据压缩，是指在不丢失信息的前提下，缩减数据量以减少存储空间，提高传输、存储和处理效率的一种技术方法。或者是按照一定的算法对数据进行重新组织，减少数据的冗余和存储的空间。本系列博文具体学习经典的压缩算法以及最近流行的算法，并应用到FASTQ下一代基因数据的压缩过程中。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;压缩技术&quot;&gt;&lt;a href=&quot;#压缩技术&quot; class=&quot;headerlink&quot; title=&quot;压缩技术&quot;&gt;&lt;/a&gt;压缩技术&lt;/h2&gt;&lt;p&gt;压缩，是为了减少存储空间而把数据转换成比原始格式更紧凑形式的过程。能实现数据压缩的本质原因就是数据的冗余性。&lt;/p&gt;
&lt;h3 id=&quot;无损压缩&quot;&gt;&lt;a href=&quot;#无损压缩&quot; class=&quot;headerlink&quot; title=&quot;无损压缩&quot;&gt;&lt;/a&gt;无损压缩&lt;/h3&gt;&lt;p&gt;无损压缩是指使用压缩后的数据进行重构(或者叫做还原，解压缩)，重构后的数据与原来的数据完全相同；无损压缩用于要求重构的信号与原始信号完全一致的场合。一个很常见的例子是磁盘文件的压缩。根据目前的技术水平，无损压缩算法一般可以把普通文件的数据压缩到原来的1/2～1/4。一些常用的无损压缩算法有霍夫曼(Huffman)算法和LZW(Lenpel-Ziv &amp;amp; Welch)压缩算法。&lt;/p&gt;
&lt;h3 id=&quot;有损压缩&quot;&gt;&lt;a href=&quot;#有损压缩&quot; class=&quot;headerlink&quot; title=&quot;有损压缩&quot;&gt;&lt;/a&gt;有损压缩&lt;/h3&gt;&lt;p&gt;有损压缩是指使用压缩后的数据进行重构，重构后的数据与原来的数据有所不同，但不会让人对原始资料表达的信息造成误解。有损压缩适用于重构信号不一定非要和原始信号完全相同的场合。例如，图像和声音的压缩就可以采用有损压缩，因为其中包含的数据往往多于我们的视觉系统和听觉系统所能接收的信息，丢掉一些数据而不至于对声音或者图像所表达的意思产生误解，但可大大提高压缩比。&lt;/p&gt;
&lt;h3 id=&quot;性能测量&quot;&gt;&lt;a href=&quot;#性能测量&quot; class=&quot;headerlink&quot; title=&quot;性能测量&quot;&gt;&lt;/a&gt;性能测量&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;压缩比：压缩前后数据文件的大小比较&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;压缩/解压时间：压缩/解压缩文件所需要的时间&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;并行化：利用多核CPU或多线程实现加速&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;失真： 有损压缩中原数据与重构结果之间的差别&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;信息论与熵编码&quot;&gt;&lt;a href=&quot;#信息论与熵编码&quot; class=&quot;headerlink&quot; title=&quot;信息论与熵编码&quot;&gt;&lt;/a&gt;信息论与熵编码&lt;/h2&gt;&lt;h3 id=&quot;数据模型&quot;&gt;&lt;a href=&quot;#数据模型&quot; class=&quot;headerlink&quot; title=&quot;数据模型&quot;&gt;&lt;/a&gt;数据模型&lt;/h3&gt;&lt;p&gt;信息的定义是度量一个数据片段复杂度的量。一个数据集拥有越多的信息，它就越难被压缩。稀有的概念和信息的概念是相关的，因为稀有符号的出现比常见符号的出现提供了更多的信息。&lt;/p&gt;
&lt;p&gt;我们把压缩算法降低信息负载的有效性，称为它的效率。一个效率更高的压缩算法相比效率低的压缩算法，能够更多地降低特定数据集的大小。&lt;/p&gt;
&lt;h3 id=&quot;概率模型&quot;&gt;&lt;a href=&quot;#概率模型&quot; class=&quot;headerlink&quot; title=&quot;概率模型&quot;&gt;&lt;/a&gt;概率模型&lt;/h3&gt;&lt;p&gt;设计一个压缩方案的最重要一步，是为数据创建一个概率模型。这个模型允许我们测量数据的特征，达到有效的适应压缩算法的目的。&lt;/p&gt;
&lt;h3 id=&quot;熵&quot;&gt;&lt;a href=&quot;#熵&quot; class=&quot;headerlink&quot; title=&quot;熵&quot;&gt;&lt;/a&gt;熵&lt;/h3&gt;&lt;p&gt;熵定义为给定模型的最小编码率，它建立在字母表和它的概率模型之上：&lt;/p&gt;
&lt;p&gt;$$H(G,P) =  - \sum_{i=0}^nP(X_i)\log_2(P(X_i))  X_i \in G $$&lt;/p&gt;
&lt;p&gt;拥有更多罕见符号的模型，比拥有较少并且常见符号的模型的熵要高。更进一步，熵值更高的模型比熵值低的模型更难压缩。&lt;/p&gt;
&lt;h2 id=&quot;编码实例&quot;&gt;&lt;a href=&quot;#编码实例&quot; class=&quot;headerlink&quot; title=&quot;编码实例&quot;&gt;&lt;/a&gt;编码实例&lt;/h2&gt;&lt;h3 id=&quot;Huffman压缩算法&quot;&gt;&lt;a href=&quot;#Huffman压缩算法&quot; class=&quot;headerlink&quot; title=&quot;Huffman压缩算法&quot;&gt;&lt;/a&gt;Huffman压缩算法&lt;/h3&gt;&lt;p&gt;huffman压缩算法可以说是无损压缩中最优秀的算法。它使用预先二进制描述来替换每个符号，长度由特殊符号出现的频率决定。其中出现次数比较多的符号需要很少的位来表示，而出现次数较少的符号则需要较多的位来表示。&lt;/p&gt;
&lt;p&gt;huffman压缩算法的原理：利用数据出现的次数构造Huffman二叉树，并且出现次数较多的数据在树的上层，出现次数较少的数据在树的下层。于是，我们就可以从根节点到每个数据的路径来进行编码并实现压缩。&lt;/p&gt;
&lt;p&gt;如：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;符号&lt;/th&gt;
&lt;th&gt;概率&lt;/th&gt;
&lt;th&gt;码子&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;B&lt;/td&gt;
&lt;td&gt;0.4&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;D&lt;/td&gt;
&lt;td&gt;0.3&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;A&lt;/td&gt;
&lt;td&gt;0.2&lt;/td&gt;
&lt;td&gt;110&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;C&lt;/td&gt;
&lt;td&gt;0.1&lt;/td&gt;
&lt;td&gt;111&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;平均编码率显著地降低到了1.9比特/符号.&lt;/p&gt;
&lt;h3 id=&quot;字典方法&quot;&gt;&lt;a href=&quot;#字典方法&quot; class=&quot;headerlink&quot; title=&quot;字典方法&quot;&gt;&lt;/a&gt;字典方法&lt;/h3&gt;&lt;p&gt;这种类型的编码器使用一个字典来保存最近发现的符号。当遇到一个符号时，首先会在字典中查找它，检查是否已经存储过了。如果是，那么输出将只包含字典入口的引用（通常是一个偏移量），而不是整个符号。&lt;/p&gt;
&lt;p&gt;使用字典方法的压缩方案包括LZ77 and LZ78，它们是很多不同的无损压缩方案的基础。&lt;/p&gt;
&lt;p&gt;在一些情况下，会使用一个滑动窗口来自适应地追踪最近发现的符号。这种情况下，一个符号只在相对较近发现时才会保存在字典中。否则，符号被剔除（之后再出现可能会重新加入字典）。这个过程防止符号字典变得过大，并利用了一个事实，即序列中的符号会在相对短的窗口内重复出现。&lt;/p&gt;
&lt;h3 id=&quot;算术编码&quot;&gt;&lt;a href=&quot;#算术编码&quot; class=&quot;headerlink&quot; title=&quot;算术编码&quot;&gt;&lt;/a&gt;算术编码&lt;/h3&gt;&lt;p&gt;算术编码是一种无损数据压缩方法，也是一种熵编码的方法。和其它熵编码方法不同的地方在于，其他的熵编码方法通常是把输入的消息分区为符号，然后对每个符号进行编码，而算术编码是直接把整个输入的消息编码为一个数，一个满足(0.0 ≤ n &amp;lt; 1.0)的小数n。&lt;/p&gt;
&lt;h3 id=&quot;游程编码&quot;&gt;&lt;a href=&quot;#游程编码&quot; class=&quot;headerlink&quot; title=&quot;游程编码&quot;&gt;&lt;/a&gt;游程编码&lt;/h3&gt;&lt;p&gt;统计某一节字节在整个字节表中出现的次数，并以该字节和出现的次数作为编码的依据。&lt;/p&gt;
&lt;p&gt;如：输入字符&lt;code&gt;ASSDDRRZZZZZO&lt;/code&gt; =&amp;gt; &lt;code&gt;A1S2D2R2Z5O1&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;对于上述方法，有人提出：如果字节表中出现连续不重复的数据，就会因为设置太多的字节次数为而达不到压缩的效果。
再想的坏一点，如果字符表中的各个字符只出现一次，也就是全部不重复，那么经过RLE编码之后，不仅没有实现压缩，
反倒是增加了一倍。比如“ABCDEFG”，如果用上述方法，那么经过压缩后应为：A1B1C1D1E1F1G1，针对上述问题，
人们对算法做了一些改进。&lt;/p&gt;
&lt;p&gt;改进算法的核心思想是：我们知道一个字节是八位，那么用最高位来当做一个标志位，这个标志位如果为1，
则表示后面跟的是重复的数据，如果为0则表示后面跟的是非重复的数据；我们用低七位来表示这个数据重复的次数。
用下一个字节来表示这个字节数据。&lt;/p&gt;
&lt;p&gt;如输入&lt;code&gt;AAABBBBBCABCDDD&lt;/code&gt;=&amp;gt;&lt;code&gt;0X83 A 0X85 B 0X41 C A B C 0X83 D&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&quot;增量编码&quot;&gt;&lt;a href=&quot;#增量编码&quot; class=&quot;headerlink&quot; title=&quot;增量编码&quot;&gt;&lt;/a&gt;增量编码&lt;/h3&gt;&lt;p&gt;对于给定一系列增量数字，如&lt;code&gt;[0,1,2,3,4,5,6,7]&lt;/code&gt; =&amp;gt; &lt;code&gt;[0,1,1,1,1,1,1,1]&lt;/code&gt;,熵减为1&lt;/p&gt;
&lt;h3 id=&quot;符号分组&quot;&gt;&lt;a href=&quot;#符号分组&quot; class=&quot;headerlink&quot; title=&quot;符号分组&quot;&gt;&lt;/a&gt;符号分组&lt;/h3&gt;&lt;p&gt;S = &lt;code&gt;&amp;quot;TOBEORNOTTOBEORTOBEORNOT&amp;quot;&lt;/code&gt; =&amp;gt; &lt;code&gt;[TO,BE,OR,NOT]&lt;/code&gt; , H(S)=1.98&lt;/p&gt;
&lt;h3 id=&quot;排列&quot;&gt;&lt;a href=&quot;#排列&quot; class=&quot;headerlink&quot; title=&quot;排列&quot;&gt;&lt;/a&gt;排列&lt;/h3&gt;&lt;p&gt;B &lt;code&gt;[5, 7, 1, 4, 6, 3, 2, 0]&lt;/code&gt;是&lt;code&gt;[0,1,2,3,4,5,6,7]&lt;/code&gt;的一个排列，对其采用擦除码(Elimination Coding)编码，结果如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-09/escode.png&quot; alt=&quot;擦除码实例&quot;&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt; 数据压缩，是指在不丢失信息的前提下，缩减数据量以减少存储空间，提高传输、存储和处理效率的一种技术方法。或者是按照一定的算法对数据进行重新组织，减少数据的冗余和存储的空间。本系列博文具体学习经典的压缩算法以及最近流行的算法，并应用到FASTQ下一代基因数据的压缩过程中。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Algorithm" scheme="http://keyunluo.github.io/categories/Algorithm/"/>
    
    
      <category term="压缩" scheme="http://keyunluo.github.io/tags/%E5%8E%8B%E7%BC%A9/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法——决策树</title>
    <link href="http://keyunluo.github.io/2016/09/23/MachineLearning/statical-learning-5.html"/>
    <id>http://keyunluo.github.io/2016/09/23/MachineLearning/statical-learning-5.html</id>
    <published>2016-09-23T01:25:02.000Z</published>
    <updated>2017-07-06T08:08:06.962Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;决策树(decision tree)是一种基本的分类与回归方法。决策树模型呈树形结构，其主要优点是模型具有可读性，分类速度快，学习时，利用训练数据根据损失函数最小化的原则建立决策树模型，分类速度快。决策树的学习通常包括3个步骤：特征选择、决策树的生成和决策树的修建。本节对应于统计学习方法第五章的内容，主要学习ID3、C4.5和CART算法。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h1 id=&quot;决策树模型与学习&quot;&gt;&lt;a href=&quot;#决策树模型与学习&quot; class=&quot;headerlink&quot; title=&quot;决策树模型与学习&quot;&gt;&lt;/a&gt;决策树模型与学习&lt;/h1&gt;&lt;h2 id=&quot;决策树模型&quot;&gt;&lt;a href=&quot;#决策树模型&quot; class=&quot;headerlink&quot; title=&quot;决策树模型&quot;&gt;&lt;/a&gt;决策树模型&lt;/h2&gt;&lt;p&gt;分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示一个类。&lt;/p&gt;
&lt;p&gt;分类的时候，从根节点开始，当前节点设为根节点，当前节点必定是一种特征，根据实例的该特征的取值，向下移动，直到到达叶节点，将实例分到叶节点对应的类中。&lt;/p&gt;
&lt;h2 id=&quot;决策树与if-then规则&quot;&gt;&lt;a href=&quot;#决策树与if-then规则&quot; class=&quot;headerlink&quot; title=&quot;决策树与if-then规则&quot;&gt;&lt;/a&gt;决策树与if-then规则&lt;/h2&gt;&lt;p&gt;决策树的属性结构其实对应着一个规则集合：由决策树的根节点到叶节点的每条路径构成的规则组成；路径上的内部特征对应着if条件，叶节点对应着then结论。决策树和规则集合是等效的，都具有一个重要的性质：互斥且完备。也就是说任何实例都被且仅被一条路径或规则覆盖。&lt;/p&gt;
&lt;h2 id=&quot;决策树与条件概率分布&quot;&gt;&lt;a href=&quot;#决策树与条件概率分布&quot; class=&quot;headerlink&quot; title=&quot;决策树与条件概率分布&quot;&gt;&lt;/a&gt;决策树与条件概率分布&lt;/h2&gt;&lt;p&gt;决策树还是给定特征条件下类的条件概率分布的一种退化表示（非等效，个人理解）。该条件分布定义在特征空间的划分上，特征空间被花费为互不相交的单元，每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的每条路径对应于划分中的一个单元。给定实例的特征X，一定落入某个划分，决策树选取该划分里最大概率的类作为结果输出。如图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-08/决策树与条件概率密度.jpg&quot; alt=&quot;决策树与条件概率密度&quot;&gt;&lt;/p&gt;
&lt;p&gt;关于b图，我是这么理解的，将a图的基础上增加一个条件概率的维度P，代表在当前特征X的情况下，分类为+的后验概率。图中的方块有些地方完全没有，比如x2轴上[a2,1]这个区间，说明只要X落在这里，Y就一定是-的，同理对于[0,a1]和[0,a2]围起来的一定是+的。有些地方只有一半，比如x1轴上[a1,1]这个区间，说明决策树认为X落在这里，Y只有一半概率是+的，根据选择条件概率大的类别的原则，就认为Y是-的（因为不满足P(+)&amp;gt;0.5)。&lt;/p&gt;
&lt;h2 id=&quot;决策树学习&quot;&gt;&lt;a href=&quot;#决策树学习&quot; class=&quot;headerlink&quot; title=&quot;决策树学习&quot;&gt;&lt;/a&gt;决策树学习&lt;/h2&gt;&lt;p&gt;决策树学习算法包含特征选择、决策树的生成与剪枝过程。决策树的学习算法一般是递归地选择最优特征，并用最优特征对数据集进行分割。开始时，构建根节点，选择最优特征，该特征有几种值就分割为几个子集，每个子集分别递归调用此方法，返回节点，返回的节点就是上一层的子节点。直到数据集为空，或者数据集只有一维特征为止。&lt;/p&gt;
&lt;p&gt;基本骨架的Python实现：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;majorityCnt&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(classList)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    返回出现次数最多的分类名称&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param classList: 类列表&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :return: 出现次数最多的类名称&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    classCount = &amp;#123;&amp;#125;  &lt;span class=&quot;comment&quot;&gt;# 这是一个字典&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; vote &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; classList:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; vote &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; classCount.keys(): classCount[vote] = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        classCount[vote] += &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;), reverse=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; sortedClassCount[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;createTree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(dataSet, labels, chooseBestFeatureToSplitFunc=chooseBestFeatureToSplitByID3)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    创建决策树&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param dataSet:数据集&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param labels:数据集每一维的名称&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :return:决策树&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    classList = [example[&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;] &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; example &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; dataSet]  &lt;span class=&quot;comment&quot;&gt;# 类别列表&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; classList.count(classList[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]) == len(classList):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; classList[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]  &lt;span class=&quot;comment&quot;&gt;# 当类别完全相同则停止继续划分&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; len(dataSet[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]) == &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;:  &lt;span class=&quot;comment&quot;&gt;# 当只有一个特征的时候，遍历完所有实例返回出现次数最多的类别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; majorityCnt(classList)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    bestFeat = chooseBestFeatureToSplitFunc(dataSet)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    bestFeatLabel = labels[bestFeat]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    myTree = &amp;#123;bestFeatLabel: &amp;#123;&amp;#125;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;del&lt;/span&gt; (labels[bestFeat])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    featValues = [example[bestFeat] &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; example &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; dataSet]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    uniqueVals = set(featValues)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; value &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; uniqueVals:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        subLabels = labels[:]  &lt;span class=&quot;comment&quot;&gt;# 复制操作&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; myTree&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;由于决策树表示条件概率分布，所以高度不同的决策树对应不同复杂度的概率模型。最优决策树的生成是个NP问题，能实现的生成算法都是局部最优的，剪枝则是既定决策树下的全局最优。&lt;/p&gt;
&lt;h1 id=&quot;特征选择&quot;&gt;&lt;a href=&quot;#特征选择&quot; class=&quot;headerlink&quot; title=&quot;特征选择&quot;&gt;&lt;/a&gt;特征选择&lt;/h1&gt;&lt;h2 id=&quot;特征选择-1&quot;&gt;&lt;a href=&quot;#特征选择-1&quot; class=&quot;headerlink&quot; title=&quot;特征选择&quot;&gt;&lt;/a&gt;特征选择&lt;/h2&gt;&lt;p&gt;样本通常有很多维特征，希望选择具有分类能力的特征。比如下表：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-08/贷款申请.jpg&quot; alt=&quot;贷款申请&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以用Python建立数据集：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;createDataSet&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    创建数据集&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :return:数据集和每个维度的名称&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    dataSet = [[&lt;span class=&quot;string&quot;&gt;u&#39;青年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;一般&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;拒绝&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               [&lt;span class=&quot;string&quot;&gt;u&#39;青年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;好&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;拒绝&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               [&lt;span class=&quot;string&quot;&gt;u&#39;青年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;是&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;好&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;同意&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               [&lt;span class=&quot;string&quot;&gt;u&#39;青年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;是&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;是&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;一般&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;同意&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               [&lt;span class=&quot;string&quot;&gt;u&#39;青年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;一般&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;拒绝&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               [&lt;span class=&quot;string&quot;&gt;u&#39;中年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;一般&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;拒绝&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               [&lt;span class=&quot;string&quot;&gt;u&#39;中年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;好&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;拒绝&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               [&lt;span class=&quot;string&quot;&gt;u&#39;中年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;是&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;是&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;好&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;同意&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               [&lt;span class=&quot;string&quot;&gt;u&#39;中年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;是&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;非常好&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;同意&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               [&lt;span class=&quot;string&quot;&gt;u&#39;中年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;是&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;非常好&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;同意&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               [&lt;span class=&quot;string&quot;&gt;u&#39;老年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;是&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;非常好&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;同意&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               [&lt;span class=&quot;string&quot;&gt;u&#39;老年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;是&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;好&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;同意&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               [&lt;span class=&quot;string&quot;&gt;u&#39;老年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;是&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;好&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;同意&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               [&lt;span class=&quot;string&quot;&gt;u&#39;老年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;是&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;非常好&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;同意&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               [&lt;span class=&quot;string&quot;&gt;u&#39;老年&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;否&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;一般&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;拒绝&#39;&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;               ]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    labels = [&lt;span class=&quot;string&quot;&gt;u&#39;年龄&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;有工作&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;有房子&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;u&#39;信贷情况&#39;&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;# 返回数据集和每个维度的名称&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; dataSet, labels&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;也可以根据特征分割数据集：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;splitDataSet&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(dataSet, axis, value)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    按照给定特征划分数据集&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param dataSet: 待划分的数据集&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param axis: 划分数据集的特征的维度&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param value: 特征的值&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :return: 符合该特征的所有实例（并且自动移除掉这维特征）&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    retDataSet = []&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; featVec &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; dataSet:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; featVec[axis] == value:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            reducedFeatVec = featVec[:axis]  &lt;span class=&quot;comment&quot;&gt;# 删掉这一维特征&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            reducedFeatVec.extend(featVec[axis + &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;:])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            retDataSet.append(reducedFeatVec)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; retDataSet&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;信息增益&quot;&gt;&lt;a href=&quot;#信息增益&quot; class=&quot;headerlink&quot; title=&quot;信息增益&quot;&gt;&lt;/a&gt;信息增益&lt;/h2&gt;&lt;p&gt;对于一个可能有n种取值的随机变量：$P(X=x_i)=p_i$,其熵为：$H(X)=-\sum_{i=1}^np_i\log p_i$ ,另外定义0log0=0,当对数的底为2时，熵的单位是bit，为自然对数时，单位是nat。&lt;/p&gt;
&lt;p&gt;用Python实现信息熵（香农熵）：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;calcShannonEnt&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(dataSet)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    计算训练数据集中的Y随机变量的香农熵&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param dataSet:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :return:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    numEntries = len(dataSet)  &lt;span class=&quot;comment&quot;&gt;# 实例的个数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    labelCounts = &amp;#123;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; featVec &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; dataSet:  &lt;span class=&quot;comment&quot;&gt;# 遍历每个实例，统计标签的频次&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        currentLabel = featVec[&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; currentLabel &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; labelCounts.keys(): labelCounts[currentLabel] = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        labelCounts[currentLabel] += &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    shannonEnt = &lt;span class=&quot;number&quot;&gt;0.0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; labelCounts:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        prob = float(labelCounts[key]) / numEntries&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        shannonEnt -= prob * log(prob, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)  &lt;span class=&quot;comment&quot;&gt;# log base 2&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; shannonEnt&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;由定义知，X的熵与X的值无关，只与分布有关，所以也可以将X的熵记作H(p),即：&lt;/p&gt;
&lt;p&gt;$$H(p)=-\sum_{i=1}^np_i\log p_i$$&lt;/p&gt;
&lt;p&gt;熵其实就是X的不确定性，从定义可以验证$0 \leq H(p) \leq \log n$&lt;/p&gt;
&lt;p&gt;设随机变量(X,Y)，其联合分布为：&lt;/p&gt;
&lt;p&gt;$$P(X=x_i,Y=y_i)=p_{ij},i=1,2,\cdots,n;j=1,2,\cdots,m$$&lt;/p&gt;
&lt;p&gt;条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性，定义为在X给定的条件下，Y的概率分布对X的数学期望：&lt;/p&gt;
&lt;p&gt;$$H(Y|X)=\sum_{i=1}^np_iH(Y|X=x_i),p_i=P(X=x_i),i=1,2,\cdots,n$$&lt;/p&gt;
&lt;p&gt;当上述定义式中的概率由数据估计（比如上一章提到的极大似然估计）得到时，所对应的熵和条件熵分别称为经验熵和经验条件熵。&lt;/p&gt;
&lt;p&gt;Python实现条件熵的计算：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;calcConditionalEntropy&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(dataSet, i, featList, uniqueVals)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&#39;&#39;&#39;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    计算X_i给定的条件下，Y的条件熵&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param dataSet:数据集&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param i:维度i&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param featList: 数据集特征列表&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param uniqueVals: 数据集特征集合&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :return:条件熵&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &#39;&#39;&#39;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    ce = &lt;span class=&quot;number&quot;&gt;0.0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; value &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; uniqueVals:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        subDataSet = splitDataSet(dataSet, i, value)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        prob = len(subDataSet) / float(len(dataSet))  &lt;span class=&quot;comment&quot;&gt;# 极大似然估计概率&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ce += prob * calcShannonEnt(subDataSet)  &lt;span class=&quot;comment&quot;&gt;# ∑pH(Y|X=xi) 条件熵的计算&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; ce&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;有了上述知识，就可以一句话说明什么叫信息增益了：信息增益表示得知特征X的信息而使类Y的信息的熵减少的程度。形式化的定义如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即$g(D|A)=H(D)-H(D|A)$,这个差又称为互信息，决策树学习中的信息增益等价于训练数据集中类与特征的互信息。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;用Python计算信息增益：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;calcInformationGain&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(dataSet, baseEntropy, i)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    计算信息增益&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param dataSet:数据集&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param baseEntropy:数据集中Y的信息熵&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param i: 特征维度i&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :return: 特征i对数据集的信息增益g(dataSet|X_i)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    featList = [example[i] &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; example &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; dataSet]  &lt;span class=&quot;comment&quot;&gt;# 第i维特征列表&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    uniqueVals = set(featList)  &lt;span class=&quot;comment&quot;&gt;# 转换成集合&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    newEntropy = calcConditionalEntropy(dataSet, i, featList, uniqueVals)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    infoGain = baseEntropy - newEntropy  &lt;span class=&quot;comment&quot;&gt;# 信息增益，就是熵的减少，也就是不确定性的减少&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; infoGain&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;回到最初的问题，如何判断一个特征的分类能力呢？信息增益大的特征具有更强的分类能力。只要计算出各个特征的信息增益，找出最大的那一个就行。&lt;/p&gt;
&lt;h2 id=&quot;信息增益的算法&quot;&gt;&lt;a href=&quot;#信息增益的算法&quot; class=&quot;headerlink&quot; title=&quot;信息增益的算法&quot;&gt;&lt;/a&gt;信息增益的算法&lt;/h2&gt;&lt;p&gt;输入：训练数据集D和特征A；&lt;/p&gt;
&lt;p&gt;输出：特征A对训练数据集D的信息增益g(D,A);&lt;/p&gt;
&lt;p&gt;(1) 计算数据集D的经验熵H(D)&lt;/p&gt;
&lt;p&gt;$$H(D)=-\sum_{k=1}^K\frac{\vert C_k \vert}{\vert D \vert}\log_2\frac{\vert C_k \vert}{\vert D \vert}$$&lt;/p&gt;
&lt;p&gt;(2) 计算特征A对数据集D的经验条件熵H(D|A)&lt;/p&gt;
&lt;p&gt;$$H(D \vert A)=\sum_{i=1}^n \frac{\vert D_i \vert}{\vert D \vert} H(D_i) = -\sum_{i=1}^n \frac{\vert D_i \vert}{\vert D \vert} \sum_{k=1}^K \frac{\vert D_{ik} \vert}{\vert D_i \vert} \log_2 \frac{\vert D_{ik} \vert}{\vert D_i \vert}$$&lt;/p&gt;
&lt;p&gt;(3) 计算信息增益&lt;/p&gt;
&lt;p&gt;$$g(D,A)=H(D)-H(D|A)$$&lt;/p&gt;
&lt;h2 id=&quot;信息增益比&quot;&gt;&lt;a href=&quot;#信息增益比&quot; class=&quot;headerlink&quot; title=&quot;信息增益比&quot;&gt;&lt;/a&gt;信息增益比&lt;/h2&gt;&lt;p&gt;信息增益算法有个缺点，信息增益的值是相对于训练数据集而言的，当H(D)大的时候，信息增益值往往会偏大，这样对H(D)小的特征不公平。改进的方法是信息增益比：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;特征增益比：特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益g(D,A)与训练数据集D的经验熵H(D)之比：$$g_R(D,A)=\frac{g(D,A)}{H(D)}$$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Python代码：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;calcInformationGainRate&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(dataSet, baseEntropy, i)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    计算信息增益比&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param dataSet:数据集&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param baseEntropy:数据集中Y的信息熵&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param i: 特征维度i&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :return: 特征i对数据集的信息增益g(dataSet|X_i)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; calcInformationGain(dataSet, baseEntropy, i) / baseEntropy&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h1 id=&quot;决策树的生成&quot;&gt;&lt;a href=&quot;#决策树的生成&quot; class=&quot;headerlink&quot; title=&quot;决策树的生成&quot;&gt;&lt;/a&gt;决策树的生成&lt;/h1&gt;&lt;h2 id=&quot;ID3算法&quot;&gt;&lt;a href=&quot;#ID3算法&quot; class=&quot;headerlink&quot; title=&quot;ID3算法&quot;&gt;&lt;/a&gt;ID3算法&lt;/h2&gt;&lt;h3 id=&quot;算法描述&quot;&gt;&lt;a href=&quot;#算法描述&quot; class=&quot;headerlink&quot; title=&quot;算法描述&quot;&gt;&lt;/a&gt;算法描述&lt;/h3&gt;&lt;p&gt;从根节点开始，计算所有可能的特征的信息增益，选择信息增益最大的特征作为当前节点的特征，由特征的不同取值建立空白子节点，对空白子节点递归调用此方法，直到所有特征的信息增益小于阀值或者没有特征可选为止。&lt;/p&gt;
&lt;h3 id=&quot;Python实现&quot;&gt;&lt;a href=&quot;#Python实现&quot; class=&quot;headerlink&quot; title=&quot;Python实现&quot;&gt;&lt;/a&gt;Python实现&lt;/h3&gt;&lt;p&gt;ID3特征选择算法的Python实现：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;chooseBestFeatureToSplitByID3&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(dataSet)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    选择最好的数据集划分方式&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param dataSet:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :return:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    numFeatures = len(dataSet[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]) - &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;comment&quot;&gt;# 最后一列是分类&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    baseEntropy = calcShannonEnt(dataSet)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    bestInfoGain = &lt;span class=&quot;number&quot;&gt;0.0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    bestFeature = &lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; range(numFeatures):  &lt;span class=&quot;comment&quot;&gt;# 遍历所有维度特征&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        infoGain = calcInformationGain(dataSet, baseEntropy, i)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (infoGain &amp;gt; bestInfoGain):  &lt;span class=&quot;comment&quot;&gt;# 选择最大的信息增益&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            bestInfoGain = infoGain&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            bestFeature = i&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; bestFeature  &lt;span class=&quot;comment&quot;&gt;# 返回最佳特征对应的维度&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;完整调用：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# -*- coding:utf-8 -*-&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Filename: testTree.py&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Author：hankcs&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Date: 2014-04-19 下午9:19&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;###########中文支持################&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; sys&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; tree &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; *&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;reload(sys)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sys.setdefaultencoding(&lt;span class=&quot;string&quot;&gt;&#39;utf-8&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; pylab &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; *&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mpl.rcParams[&lt;span class=&quot;string&quot;&gt;&#39;font.sans-serif&#39;&lt;/span&gt;] = [&lt;span class=&quot;string&quot;&gt;&#39;SimHei&#39;&lt;/span&gt;]  &lt;span class=&quot;comment&quot;&gt;# 指定默认字体&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mpl.rcParams[&lt;span class=&quot;string&quot;&gt;&#39;axes.unicode_minus&#39;&lt;/span&gt;] = &lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;  &lt;span class=&quot;comment&quot;&gt;# 解决保存图像时负号&#39;-&#39;显示为方块的问题&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;##################################&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 测试决策树的构建&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;myDat, labels = createDataSet()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;myTree = createTree(myDat, labels)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 绘制决策树&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; treePlotter&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;treePlotter.createPlot(myTree)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;可视化&quot;&gt;&lt;a href=&quot;#可视化&quot; class=&quot;headerlink&quot; title=&quot;可视化&quot;&gt;&lt;/a&gt;可视化&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;66&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;68&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;69&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;70&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;71&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;72&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;73&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;74&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;75&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;76&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;77&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;78&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;79&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;80&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;81&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;82&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;83&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# -*- coding:utf-8 -*-&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Filename: treePlotter.py&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Author：hankcs&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Date: 2015/2/9 21:24&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 定义文本框和箭头格式&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;decisionNode = dict(boxstyle=&lt;span class=&quot;string&quot;&gt;&quot;round4&quot;&lt;/span&gt;, color=&lt;span class=&quot;string&quot;&gt;&#39;#3366FF&#39;&lt;/span&gt;)  &lt;span class=&quot;comment&quot;&gt;#定义判断结点形态&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;leafNode = dict(boxstyle=&lt;span class=&quot;string&quot;&gt;&quot;circle&quot;&lt;/span&gt;, color=&lt;span class=&quot;string&quot;&gt;&#39;#FF6633&#39;&lt;/span&gt;)  &lt;span class=&quot;comment&quot;&gt;#定义叶结点形态&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;arrow_args = dict(arrowstyle=&lt;span class=&quot;string&quot;&gt;&quot;&amp;lt;-&quot;&lt;/span&gt;, color=&lt;span class=&quot;string&quot;&gt;&#39;g&#39;&lt;/span&gt;)  &lt;span class=&quot;comment&quot;&gt;#定义箭头&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#绘制带箭头的注释&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;plotNode&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(nodeTxt, centerPt, parentPt, nodeType)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords=&lt;span class=&quot;string&quot;&gt;&#39;axes fraction&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                            xytext=centerPt, textcoords=&lt;span class=&quot;string&quot;&gt;&#39;axes fraction&#39;&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                            va=&lt;span class=&quot;string&quot;&gt;&quot;center&quot;&lt;/span&gt;, ha=&lt;span class=&quot;string&quot;&gt;&quot;center&quot;&lt;/span&gt;, bbox=nodeType, arrowprops=arrow_args)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#计算叶结点数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;getNumLeafs&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(myTree)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    numLeafs = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    firstStr = myTree.keys()[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    secondDict = myTree[firstStr]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; secondDict.keys():&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; type(secondDict[key]).__name__ == &lt;span class=&quot;string&quot;&gt;&#39;dict&#39;&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            numLeafs += getNumLeafs(secondDict[key])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            numLeafs += &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; numLeafs&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#计算树的层数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;getTreeDepth&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(myTree)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    maxDepth = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    firstStr = myTree.keys()[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    secondDict = myTree[firstStr]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; secondDict.keys():&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; type(secondDict[key]).__name__ == &lt;span class=&quot;string&quot;&gt;&#39;dict&#39;&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            thisDepth = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; + getTreeDepth(secondDict[key])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            thisDepth = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; thisDepth &amp;gt; maxDepth:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            maxDepth = thisDepth&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; maxDepth&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#在父子结点间填充文本信息&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;plotMidText&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(cntrPt, parentPt, txtString)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    xMid = (parentPt[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] - cntrPt[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]) / &lt;span class=&quot;number&quot;&gt;2.0&lt;/span&gt; + cntrPt[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    yMid = (parentPt[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] - cntrPt[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]) / &lt;span class=&quot;number&quot;&gt;2.0&lt;/span&gt; + cntrPt[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    createPlot.ax1.text(xMid, yMid, txtString, va=&lt;span class=&quot;string&quot;&gt;&quot;center&quot;&lt;/span&gt;, ha=&lt;span class=&quot;string&quot;&gt;&quot;center&quot;&lt;/span&gt;, rotation=&lt;span class=&quot;number&quot;&gt;30&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;plotTree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(myTree, parentPt, nodeTxt)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    numLeafs = getNumLeafs(myTree)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    depth = getTreeDepth(myTree)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    firstStr = myTree.keys()[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    cntrPt = (plotTree.xOff + (&lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt; + float(numLeafs)) / &lt;span class=&quot;number&quot;&gt;2.0&lt;/span&gt; / plotTree.totalW, plotTree.yOff)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plotMidText(cntrPt, parentPt, nodeTxt)  &lt;span class=&quot;comment&quot;&gt;#在父子结点间填充文本信息&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plotNode(firstStr, cntrPt, parentPt, decisionNode)  &lt;span class=&quot;comment&quot;&gt;#绘制带箭头的注释&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    secondDict = myTree[firstStr]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plotTree.yOff = plotTree.yOff - &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt; / plotTree.totalD&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; key &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; secondDict.keys():&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; type(secondDict[key]).__name__ == &lt;span class=&quot;string&quot;&gt;&#39;dict&#39;&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            plotTree(secondDict[key], cntrPt, str(key))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            plotTree.xOff = plotTree.xOff + &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt; / plotTree.totalW&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plotTree.yOff = plotTree.yOff + &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt; / plotTree.totalD&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;createPlot&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(inTree)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    fig = plt.figure(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, facecolor=&lt;span class=&quot;string&quot;&gt;&#39;white&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    fig.clf()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    axprops = dict(xticks=[], yticks=[])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    createPlot.ax1 = plt.subplot(&lt;span class=&quot;number&quot;&gt;111&lt;/span&gt;, frameon=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;, **axprops)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plotTree.totalW = float(getNumLeafs(inTree))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plotTree.totalD = float(getTreeDepth(inTree))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plotTree.xOff = &lt;span class=&quot;number&quot;&gt;-0.5&lt;/span&gt; / plotTree.totalW;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plotTree.yOff = &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plotTree(inTree, (&lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1.0&lt;/span&gt;), &lt;span class=&quot;string&quot;&gt;&#39;&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.show()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;C4-5生成算法&quot;&gt;&lt;a href=&quot;#C4-5生成算法&quot; class=&quot;headerlink&quot; title=&quot;C4.5生成算法&quot;&gt;&lt;/a&gt;C4.5生成算法&lt;/h2&gt;&lt;h3 id=&quot;算法描述-1&quot;&gt;&lt;a href=&quot;#算法描述-1&quot; class=&quot;headerlink&quot; title=&quot;算法描述&quot;&gt;&lt;/a&gt;算法描述&lt;/h3&gt;&lt;p&gt;输入： 训练数据集D，特征集A，阈值$\epsilon$&lt;/p&gt;
&lt;p&gt;输出：决策树T&lt;/p&gt;
&lt;p&gt;(1) 如果D中所有实例属于同一类$C_k$,则置T为单节点树，并将$C_k$作为该节点的类，返回T&lt;/p&gt;
&lt;p&gt;(2) 如果$A=\emptyset$,则置T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类，返回T&lt;/p&gt;
&lt;p&gt;(3) 否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征$A_g$&lt;/p&gt;
&lt;p&gt;(4) 如果$A_g$的信息增益比小于阈值$\epsilon$，则置T为单节点树，并将D中实例数最大的类$C_k$作为该节点的类，返回T&lt;/p&gt;
&lt;p&gt;(5) 否则，对$A_g$的每一个可能值$a_i$,依$A_g=a_i$将D分割为子集若干非空$D_i$,将$D_i$中实例数最大的类作为标记，构建子节点，由节点及其子节点构成树T，返回T&lt;/p&gt;
&lt;p&gt;(6) 对节点i，以$D_i$作为训练集，以$A-\{A_g\}$为特征集，递归调用(1)~(5)步，得到子树$T_i$,返回$T_i$&lt;/p&gt;
&lt;h3 id=&quot;python实现&quot;&gt;&lt;a href=&quot;#python实现&quot; class=&quot;headerlink&quot; title=&quot;python实现&quot;&gt;&lt;/a&gt;python实现&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;chooseBestFeatureToSplitByC45&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(dataSet)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    选择最好的数据集划分方式&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :param dataSet:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    :return:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    numFeatures = len(dataSet[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]) - &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;comment&quot;&gt;# 最后一列是分类&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    baseEntropy = calcShannonEnt(dataSet)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    bestInfoGainRate = &lt;span class=&quot;number&quot;&gt;0.0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    bestFeature = &lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; range(numFeatures):  &lt;span class=&quot;comment&quot;&gt;# 遍历所有维度特征&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        infoGainRate = calcInformationGainRate(dataSet, baseEntropy, i)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (infoGainRate &amp;gt; bestInfoGainRate):  &lt;span class=&quot;comment&quot;&gt;# 选择最大的信息增益&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            bestInfoGainRate = infoGainRate&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            bestFeature = i&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; bestFeature  &lt;span class=&quot;comment&quot;&gt;# 返回最佳特征对应的维度&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;调用方法只需加个参数：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;myTree = createTree(myDat, labels, chooseBestFeatureToSplitByC45)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h1 id=&quot;决策树的剪枝&quot;&gt;&lt;a href=&quot;#决策树的剪枝&quot; class=&quot;headerlink&quot; title=&quot;决策树的剪枝&quot;&gt;&lt;/a&gt;决策树的剪枝&lt;/h1&gt;&lt;p&gt;决策树很容易发生过拟合，过拟合的原因在于学习的时候过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法就是简化已生成的决策树，也就是剪枝。&lt;/p&gt;
&lt;p&gt;决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。&lt;/p&gt;
&lt;p&gt;设决策树T的叶节点有|T|个，t是某个叶节点，t有$N_t$个样本点，其中归入k类的样本点有$N_{tk}$个，$H_t(T)$为叶节点t上的经验熵，α≥0为参数，则损失函数可以定义为：&lt;/p&gt;
&lt;p&gt;$$C_{\alpha}(T)=\sum_{t=1}^{\vert T \vert} N_tH_t(T) + \alpha \vert T \vert$$&lt;/p&gt;
&lt;p&gt;其中经验熵Ht(T)为：&lt;/p&gt;
&lt;p&gt;$$H_t(T)=- \sum_k \frac{N_{ik}}{N_t} \log \frac{N_{tk}}{N_t}$$&lt;/p&gt;
&lt;p&gt;表示叶节点t所代表的类别的不确定性。损失函数对它求和表示所有被导向该叶节点的样本点所带来的不确定的和的和。&lt;/p&gt;
&lt;p&gt;在损失函数中，将右边第一项记作：&lt;/p&gt;
&lt;p&gt;$$C(T)=\sum_{t=1}^{\vert T \vert}N_tH_t(T)=-\sum_{t=1}^{\vert T \vert}\sum_{k=1}^K N_{tk} \log \frac{N_{tk}}{N_t}$$&lt;/p&gt;
&lt;p&gt;则损失函数可以简单记作：&lt;/p&gt;
&lt;p&gt;$$C_{\alpha}(T)=C(T) + \alpha \vert T \vert$$&lt;/p&gt;
&lt;p&gt;C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数α≥0控制两者之间的影响，α越大，模型越简单，α=0表示不考虑复杂度。&lt;/p&gt;
&lt;p&gt;剪枝，就是当α确定时，选择损失函数最小的模型。子树越大C(T)越小，但是α|T|越大，损失函数反映的是两者的平衡。&lt;/p&gt;
&lt;p&gt;决策树的生成过程只考虑了信息增益或信息增益比，只考虑更好地拟合训练数据，而剪枝过程则考虑了减小复杂度。前者是局部学习，后者是整体学习。&lt;/p&gt;
&lt;h2 id=&quot;树的剪枝算法&quot;&gt;&lt;a href=&quot;#树的剪枝算法&quot; class=&quot;headerlink&quot; title=&quot;树的剪枝算法&quot;&gt;&lt;/a&gt;树的剪枝算法&lt;/h2&gt;&lt;p&gt;从每个叶节点往上走，走了后如果损失函数减小了，则减掉叶节点，将父节点作为叶节点。如图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-08/决策树剪枝.jpg&quot; alt=&quot;树的剪枝算法&quot;&gt;&lt;/p&gt;
&lt;p&gt;说是这么说，实际上如果叶节点有多个，那么父节点变成叶节点后，新叶节点到底应该选择原来的叶节点中的哪一种类别呢？大概又是多数表决吧，原著并没有深入展开。&lt;/p&gt;
&lt;h1 id=&quot;CART算法&quot;&gt;&lt;a href=&quot;#CART算法&quot; class=&quot;headerlink&quot; title=&quot;CART算法&quot;&gt;&lt;/a&gt;CART算法&lt;/h1&gt;&lt;p&gt;分类与回归树（CART）模型同样由特征选取、树的生成和剪枝组成，既可以用于分类也可以用于回归。CART假设决策树是二叉树，内部节点特征的取值为是和否，对应一个实例的特征是否是这样的。决策树递归地二分每个特征，将输入空间划分为有限个单元。&lt;/p&gt;
&lt;h2 id=&quot;CART生成&quot;&gt;&lt;a href=&quot;#CART生成&quot; class=&quot;headerlink&quot; title=&quot;CART生成&quot;&gt;&lt;/a&gt;CART生成&lt;/h2&gt;&lt;p&gt;决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼系数最小化准则，进行特征选择，生成二叉树。&lt;/p&gt;
&lt;h3 id=&quot;回归树&quot;&gt;&lt;a href=&quot;#回归树&quot; class=&quot;headerlink&quot; title=&quot;回归树&quot;&gt;&lt;/a&gt;回归树&lt;/h3&gt;&lt;p&gt;回归树与分类树在数据集上的不同就是数据集的输出部分不是类别，而是连续变量。&lt;/p&gt;
&lt;p&gt;假设输入空间已经被分为M个单元输入空间单元$R_1,R_2,\cdots,R_M$，分别对应输出值$c_m$，于是回归树模型可以表示为：&lt;/p&gt;
&lt;p&gt;$$f(x)=\sum_{m=1}^Mc_mI(x \in R_m)$$&lt;/p&gt;
&lt;p&gt;回归树的预测误差：&lt;/p&gt;
&lt;p&gt;$$\sum_{x_x \in R_m}(y_i - f(x_i))^2$$&lt;/p&gt;
&lt;p&gt;那么输出值就是使上面误差最小的值，也就是均值：&lt;/p&gt;
&lt;p&gt;$$\hat c_m = ave(y_i \vert x_i \in R_m)$$&lt;/p&gt;
&lt;p&gt;难点在于怎么划分，一种启发式的方法（其实就是暴力搜索吧）：&lt;/p&gt;
&lt;p&gt;遍历所有输入变量，选择第j个变量和它的值s作为切分变量和切分点，将空间分为两个区域：&lt;/p&gt;
&lt;p&gt;$$R_1(j,s)=\{x \vert x^{(j)} \leq s\} 和R_2(j,s)=\{x \vert x^{(j)} &amp;gt; s\}$$&lt;/p&gt;
&lt;p&gt;然后计算两个区域的平方误差，求和，极小化这个和，具体的，就是：&lt;/p&gt;
&lt;p&gt;$$\min_{j,s} \left [ \min_{c_1} \sum_{x_i \in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)}(y_i-c_2)^2 \right ]$$&lt;/p&gt;
&lt;p&gt;当j最优化的时候，就可以将切分点最优化：&lt;/p&gt;
&lt;p&gt;$$\hat c_1 = ave()y_i | x_i \in R_1(j,s)) 和 \hat c_2 = ave()y_i | x_i \in R_2(j,s))$$&lt;/p&gt;
&lt;p&gt;递归调用此过程，这种回归树通常称为最小二乘回归树。&lt;/p&gt;
&lt;h3 id=&quot;最小二乘回归树生成算法&quot;&gt;&lt;a href=&quot;#最小二乘回归树生成算法&quot; class=&quot;headerlink&quot; title=&quot;最小二乘回归树生成算法&quot;&gt;&lt;/a&gt;最小二乘回归树生成算法&lt;/h3&gt;&lt;p&gt;输入：训练数据集D&lt;/p&gt;
&lt;p&gt;输出：回归树f(x)&lt;/p&gt;
&lt;p&gt;在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：&lt;/p&gt;
&lt;p&gt;(1) 选择最优切分变量j与切分点s，求解：&lt;/p&gt;
&lt;p&gt;$$\min_{j,s} \left [ \min_{c_1} \sum_{x_i \in R_1(j,s)}(y_i-c_1)^2 + \min_{c_2} \sum_{x_i \in R_2(j,s)}(y_i-c_2)^2 \right ]$$&lt;/p&gt;
&lt;p&gt;遍历变量j，对固定的切分变量j扫描切分点s,选择使上式达到最小值的对(j,s)&lt;/p&gt;
&lt;p&gt;(2) 用选定的对(j,s)划分区域并决定相应的输出值：&lt;/p&gt;
&lt;p&gt;$$R_1(j,s)=\{x \vert x^{(j)} \leq s\} 和R_2(j,s)=\{x \vert x^{(j)} &amp;gt; s\}$$&lt;/p&gt;
&lt;p&gt;$$\hat c_m = \frac{1}{N_m}\sum_{x_i \in R_m(j,s)} y_i, x \in R_m,m=1,2 $$&lt;/p&gt;
&lt;p&gt;(3) 继续对两个子区域调用步骤(1)和(2)，直到满足停止条件&lt;/p&gt;
&lt;p&gt;(4) 将输入空间划分为M个区域R_1,R_2,\cdots,R_M,生成决策树：&lt;/p&gt;
&lt;p&gt;$$f(x)=\sum_{m=1}^M \hat c_m I(x \in R_m)$$&lt;/p&gt;
&lt;h3 id=&quot;分类树&quot;&gt;&lt;a href=&quot;#分类树&quot; class=&quot;headerlink&quot; title=&quot;分类树&quot;&gt;&lt;/a&gt;分类树&lt;/h3&gt;&lt;p&gt;与回归树算法流程类似，只不过选择的是最优切分特征和最优切分点，并采用基尼指数衡量。基尼指数定义：&lt;/p&gt;
&lt;p&gt;$$Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$$&lt;/p&gt;
&lt;p&gt;对于给定数据集D，其基尼指数是：&lt;/p&gt;
&lt;p&gt;$$Gini(D)=1-\sum_{k=1}^K \left ( \frac{\vert C_k \vert}{\vert D \vert} \right) ^2$$&lt;/p&gt;
&lt;p&gt;Ck是属于第k类的样本子集，K是类的个数。Gini(D)反应的是D的不确定性（与熵类似），分区的目标就是降低不确定性。&lt;/p&gt;
&lt;p&gt;D根据特征A是否取某一个可能值a而分为D1和D2两部分：&lt;/p&gt;
&lt;p&gt;$$D_1=\{(x,y) \in D \vert A(x) = a\}, D_2 = D - D_1$$&lt;/p&gt;
&lt;p&gt;则在特征A的条件下，D的基尼指数是：&lt;/p&gt;
&lt;p&gt;$$Gini(D,A)=\frac{D_1}{D}Gini(D_1) + \frac{D_2}{D}Gini(D_2)$$&lt;/p&gt;
&lt;p&gt;有了上述知识储备，可以给出CART生成算法的伪码：&lt;/p&gt;
&lt;p&gt;设节点的当前数据集为D，对D中每一个特征A，对齐每个值a根据D中样本点是否满足A==a分为两部分，计算基尼指数。对所有基尼指数选择最小的，对应的特征和切分点作为最优特征和最优切分点，生成两个子节点，将对应的两个分区分配过去，然后对两个子节点递归。&lt;/p&gt;
&lt;h2 id=&quot;CART剪枝&quot;&gt;&lt;a href=&quot;#CART剪枝&quot; class=&quot;headerlink&quot; title=&quot;CART剪枝&quot;&gt;&lt;/a&gt;CART剪枝&lt;/h2&gt;&lt;p&gt;在上面介绍的损失函数中，当α固定时，一定存在使得损失函数最小的子树，记为复杂度=Tα，α偏大Tα就偏小。设对α递增的序列，对应的最优子树序列为Tn，子树序列第一棵包含第二棵，依次类推。&lt;/p&gt;
&lt;p&gt;从T0开始剪枝，对它内部的任意节点t，只有t这一个节点的子树的损失函数是：&lt;/p&gt;
&lt;p&gt;$$C_{\alpha}=C(t)+\alpha$$&lt;/p&gt;
&lt;p&gt;以t为根节点的子树的损失函数是：&lt;/p&gt;
&lt;p&gt;$$C_{\alpha}(T_t)=C(T_t)+\alpha \vert T \vert$$&lt;/p&gt;
&lt;p&gt;当α充分小，肯定有:&lt;/p&gt;
&lt;p&gt;$$C_{\alpha}(T_t)&amp;lt;C_{\alpha}(t)$$&lt;/p&gt;
&lt;p&gt;这个不等式的意思是复杂模型在复杂度影响力小的情况下损失函数更小。&lt;/p&gt;
&lt;p&gt;当α增大到某一点，这个不等式的符号会反过来。&lt;/p&gt;
&lt;p&gt;只要$\alpha = \frac{C(t)-C(T_t)}{\vert T_t \vert -1}$,损失函数值就相同，但是t更小啊，所以t更可取，于是把Tt剪枝掉。&lt;/p&gt;
&lt;p&gt;为此，对每一个t，计算&lt;/p&gt;
&lt;p&gt;$$g(t)=$\frac{C(t)-C(T_t)}{\vert T_t \vert -1}$$&lt;/p&gt;
&lt;p&gt;表示损失函数的减少程度，从T中剪枝掉g(t)最小的Tt，取新的α=g(t)，直到根节点。这样就得到了一个子树序列，对此序列，应用独立的验证数据集交叉验证，选取最优子树，剪枝完毕。&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;决策树(decision tree)是一种基本的分类与回归方法。决策树模型呈树形结构，其主要优点是模型具有可读性，分类速度快，学习时，利用训练数据根据损失函数最小化的原则建立决策树模型，分类速度快。决策树的学习通常包括3个步骤：特征选择、决策树的生成和决策树的修建。本节对应于统计学习方法第五章的内容，主要学习ID3、C4.5和CART算法。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="MachineLearning" scheme="http://keyunluo.github.io/categories/MachineLearning/"/>
    
    
      <category term="机器学习" scheme="http://keyunluo.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop源码学习(15)——文件系统(1)</title>
    <link href="http://keyunluo.github.io/2016/09/22/BigData/hadoop-filesystem-1.html"/>
    <id>http://keyunluo.github.io/2016/09/22/BigData/hadoop-filesystem-1.html</id>
    <published>2016-09-22T08:25:02.000Z</published>
    <updated>2017-07-06T08:08:06.872Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;为了提供对不同数据访问的一致接口，Hadoop借鉴了Linux虚拟文件系统的概念，引入了Hadoop抽象文件系统，并在Hadoop抽象文件系统上，提供了大量的具体文件系统的实现，满足构建于Hadoop应用之上的数据访问需求。本节学习Hadoop文件系统API。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;分布式文件系统的特性&quot;&gt;&lt;a href=&quot;#分布式文件系统的特性&quot; class=&quot;headerlink&quot; title=&quot;分布式文件系统的特性&quot;&gt;&lt;/a&gt;分布式文件系统的特性&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;访问透明性：用户不需要了解文件的分布性&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;位置透明性：客户程序可以使用单一的文件空间&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;移动透明性：文件被移动时，客户程序不需要改变&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;性能透明性，伸缩透明性，复制透明性，故障透明性，并发透明性&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;数据完整性、安全性和系统异构&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Hadoop文件系统类图&quot;&gt;&lt;a href=&quot;#Hadoop文件系统类图&quot; class=&quot;headerlink&quot; title=&quot;Hadoop文件系统类图&quot;&gt;&lt;/a&gt;Hadoop文件系统类图&lt;/h2&gt;&lt;p&gt;Hadoop提供了一个抽象的文件系统，该系统可以作为分布式系统实现也可是本地磁盘。HDFS是这个抽象文件系统的具体实现。抽象系统类org.apache.hadoop.fs.FileSystem的类图如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-08/FileSystem.png&quot; alt=&quot;FileSystem&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;API对应关系&quot;&gt;&lt;a href=&quot;#API对应关系&quot; class=&quot;headerlink&quot; title=&quot;API对应关系&quot;&gt;&lt;/a&gt;API对应关系&lt;/h2&gt;&lt;p&gt;和Linux与Java文件API类似，Hadoop抽象文件系统的方法可以分为两个部分：一部分用于处理文件和目录的相关事务，另一部分用于读写数据。下表总结了Hadoop抽象文件系统的文件操作与Java、Linux的对应关系&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hadoop抽象文件系统文件操作(部分)&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;HadoopFileSystem类&lt;/th&gt;
&lt;th&gt;Java操作&lt;/th&gt;
&lt;th&gt;Linux操作&lt;/th&gt;
&lt;th&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;URL.openStream, FileSystem.open, FileSystem.create, FileSystem.append&lt;/td&gt;
&lt;td&gt;URL.openStream&lt;/td&gt;
&lt;td&gt;open&lt;/td&gt;
&lt;td&gt;打开一个文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FSDataInputStream.read&lt;/td&gt;
&lt;td&gt;InputStream.read&lt;/td&gt;
&lt;td&gt;read&lt;/td&gt;
&lt;td&gt;读取文件中包含的数据&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FSDataOutputStream.write&lt;/td&gt;
&lt;td&gt;OutputStream.write&lt;/td&gt;
&lt;td&gt;write&lt;/td&gt;
&lt;td&gt;向文件中写数据&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FSDataOutputStream.close, FSDataInputStream.close&lt;/td&gt;
&lt;td&gt;InputStream.close,OutputStream.close&lt;/td&gt;
&lt;td&gt;close&lt;/td&gt;
&lt;td&gt;关闭一个文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FSDataInputStream.seek&lt;/td&gt;
&lt;td&gt;RandomAccessFile.seek&lt;/td&gt;
&lt;td&gt;lseek&lt;/td&gt;
&lt;td&gt;改变文件的读写位置&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FileSystem.getFileStatus, FileSystem.get*&lt;/td&gt;
&lt;td&gt;File.get*&lt;/td&gt;
&lt;td&gt;stat&lt;/td&gt;
&lt;td&gt;获取文件/目录的属性&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FileSystem.set*&lt;/td&gt;
&lt;td&gt;File.set*&lt;/td&gt;
&lt;td&gt;chmod等&lt;/td&gt;
&lt;td&gt;修改文件属性&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FileSystem.createNewFile&lt;/td&gt;
&lt;td&gt;File.createNewFile&lt;/td&gt;
&lt;td&gt;create&lt;/td&gt;
&lt;td&gt;创建一个文件&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FileSystem.delete&lt;/td&gt;
&lt;td&gt;File.delete&lt;/td&gt;
&lt;td&gt;remove/rmdir&lt;/td&gt;
&lt;td&gt;从文件系统中删除一个文件/文件夹&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FileSystem.rename&lt;/td&gt;
&lt;td&gt;File.renameTo&lt;/td&gt;
&lt;td&gt;rename&lt;/td&gt;
&lt;td&gt;更改文件/目录名&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FileSystem.mkdirs&lt;/td&gt;
&lt;td&gt;FileSystem.mkdir&lt;/td&gt;
&lt;td&gt;mkdir&lt;/td&gt;
&lt;td&gt;在给定目录下创建一个子目录&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FileSystem.listStatus&lt;/td&gt;
&lt;td&gt;File.list&lt;/td&gt;
&lt;td&gt;readdir&lt;/td&gt;
&lt;td&gt;读取一个目录下的项目&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FileSystem.setWorkingDirectory&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;getcwd/getwd&lt;/td&gt;
&lt;td&gt;返回当前工作目录&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;FileSystem.setWorkingDirectory&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;chdir&lt;/td&gt;
&lt;td&gt;更改当前工作目录&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;为了提供对不同数据访问的一致接口，Hadoop借鉴了Linux虚拟文件系统的概念，引入了Hadoop抽象文件系统，并在Hadoop抽象文件系统上，提供了大量的具体文件系统的实现，满足构建于Hadoop应用之上的数据访问需求。本节学习Hadoop文件系统API。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="BigData" scheme="http://keyunluo.github.io/categories/BigData/"/>
    
    
      <category term="Hadoop" scheme="http://keyunluo.github.io/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法——朴素贝叶斯法</title>
    <link href="http://keyunluo.github.io/2016/08/16/MachineLearning/statical-learning-4.html"/>
    <id>http://keyunluo.github.io/2016/08/16/MachineLearning/statical-learning-4.html</id>
    <published>2016-08-16T01:25:02.000Z</published>
    <updated>2017-07-06T08:08:06.950Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;朴素贝叶斯(native Bates)是基于贝叶斯定理与特征条件独立假设的分类法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率密度；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率的最大的输出y。朴素贝叶斯实现简单，学习和预测的效率都很高，是一种常用的方法。本节对应于统计学习方法第四章的内容。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;朴素贝叶斯法的学习与分类&quot;&gt;&lt;a href=&quot;#朴素贝叶斯法的学习与分类&quot; class=&quot;headerlink&quot; title=&quot;朴素贝叶斯法的学习与分类&quot;&gt;&lt;/a&gt;朴素贝叶斯法的学习与分类&lt;/h2&gt;&lt;h3 id=&quot;基本方法&quot;&gt;&lt;a href=&quot;#基本方法&quot; class=&quot;headerlink&quot; title=&quot;基本方法&quot;&gt;&lt;/a&gt;基本方法&lt;/h3&gt;&lt;p&gt;设输入空间$\mathcal{X} \subseteq R^n$为n维向量的集合，输出空间为类标记集合$\mathcal{Y} = \{c_1,c_2, \cdots ,c_K \}$.输入为特征向量$x \in \mathcal{X}$,输出为类标记$y \in \mathcal{Y},X$是定义在输入空间$\mathcal{X}$上的随机变量，$\mathcal{Y}$是定义在输出空间Y上的随机变量.P(X,Y)是X和Y的联合概率分布。训练数据集：&lt;/p&gt;
&lt;p&gt;$$T = \{(x_1,y_1),(x_2,y_2), \cdots , (x_N,y_N) \}$$&lt;/p&gt;
&lt;p&gt;由P(X,Y)独立同分布产生。朴素贝叶斯法通过训练数据集学习联合概率分布P(X,Y),具体地，学习以下先验概率密度及条件概率密度。先验概率密度：&lt;/p&gt;
&lt;p&gt;$$P(Y=c_k) , k= 1,2, \cdots ,K$$&lt;/p&gt;
&lt;p&gt;条件概率密度：&lt;/p&gt;
&lt;p&gt;$$P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)},\cdots , X^{(n)}=x^{(n)} |Y=c_k), k= 1,2, \cdots ,K$$&lt;/p&gt;
&lt;p&gt;于是学习到联合概率分布P(X,Y).&lt;/p&gt;
&lt;p&gt;朴素贝叶斯法对条件概率分布作了条件独立性的假设：&lt;/p&gt;
&lt;p&gt;$$P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)},\cdots , X^{(n)}=x^{(n)} |Y=c_k)=\prod_{j=1}^n P(X^{(j)}=x^{(j)} | Y = c_k)$$&lt;/p&gt;
&lt;p&gt;朴素贝叶斯法实际上学习到的是生成数据的机制，所以属于生成模型。条件独立性假设是说用于分类的特征在类确定的情况都是条件独立的，这一假设会使得朴素贝叶斯法变得简单，但有时会牺牲一定的分类准确率。&lt;/p&gt;
&lt;p&gt;朴素贝叶斯法时，对给定的输入x，通过学习到的模型计算后验概率分布$P(Y=c_k|X=x)$，将后验概率最大的类作为x的类输出，后验概率计算根据贝叶斯定理进行：&lt;/p&gt;
&lt;p&gt;$$P(Y=c_k|X=x)= \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_k P(X=x|Y=c_k)P(Y=c_k)}$$&lt;/p&gt;
&lt;p&gt;将条件独立性假设公式带入上式，得：&lt;/p&gt;
&lt;p&gt;$$P(Y=c_k|X=x)= \frac{P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_k P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}s|Y=c_k)} , k=1,2, \cdots ,K$$&lt;/p&gt;
&lt;p&gt;考虑到分母对所有的$c_k$都相同，朴素贝叶斯分类器可表示为：&lt;/p&gt;
&lt;p&gt;$$y =  arg \max_{c_k}P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)$$&lt;/p&gt;
&lt;h3 id=&quot;后验概率最大化的含义&quot;&gt;&lt;a href=&quot;#后验概率最大化的含义&quot; class=&quot;headerlink&quot; title=&quot;后验概率最大化的含义&quot;&gt;&lt;/a&gt;后验概率最大化的含义&lt;/h3&gt;&lt;p&gt;朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望风险最小化，假设选择0-1损失函数：&lt;/p&gt;
&lt;p&gt;$$
L(Y,f(X))=
\begin{cases}
0&amp;amp; Y=f(x)\\
1&amp;amp; Y \neq f(X)
\end{cases}
$$&lt;/p&gt;
&lt;p&gt;式中f(X)是分类决策函数，这时期望风险函数为：&lt;/p&gt;
&lt;p&gt;$$R_{exp}(f)=E[L(Y,f(X))]$$&lt;/p&gt;
&lt;p&gt;期望是对联合分布P(X,Y)取的，由此取期望：&lt;/p&gt;
&lt;p&gt;$$R_{exp}(f)=E_X\sum_{k=1}^{K}[L(c_k,f(X))]P(c_k|X)$$&lt;/p&gt;
&lt;p&gt;为了使期望风险最小化，只需对X=x逐个最小化，由此得到：&lt;/p&gt;
&lt;p&gt;$$f(x)= \arg \min_{y \in \mathcal{Y}} P(y=c_k|X=x)$$&lt;/p&gt;
&lt;h2 id=&quot;朴素贝叶斯法的参数估计&quot;&gt;&lt;a href=&quot;#朴素贝叶斯法的参数估计&quot; class=&quot;headerlink&quot; title=&quot;朴素贝叶斯法的参数估计&quot;&gt;&lt;/a&gt;朴素贝叶斯法的参数估计&lt;/h2&gt;&lt;h3 id=&quot;极大似然估计&quot;&gt;&lt;a href=&quot;#极大似然估计&quot; class=&quot;headerlink&quot; title=&quot;极大似然估计&quot;&gt;&lt;/a&gt;极大似然估计&lt;/h3&gt;&lt;p&gt;在朴素贝叶斯法中，学习意味着估计$P(Y=c_k)和P(X^{(j)}=x^{(j)}| Y=c_k)$,可以应用极大似然估计法估计相应的概率。先验概率$P(Y=c_k)$的极大似然估计是：&lt;/p&gt;
&lt;p&gt;$$P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N},k=1,2,\cdots ,K$$&lt;/p&gt;
&lt;p&gt;设第j个特征$x^{(j)}$可能取值的集合为$\{a_{j1},a_{j2},\cdots ,a_{jS_j}\}$,条件概率$P(X^{(j)}=a_{jl}|Y=c_k)$的极大似然估计是：&lt;/p&gt;
&lt;p&gt;$$P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}, j=1,2,\cdots ,n;l=1,2,\cdots ,S_j;k=1,2,\cdots ,K$$&lt;/p&gt;
&lt;p&gt;式中，$x_i^{(j)}$是第i个样本的第j个特征；$a_{jl}$是第j个特征可能取的第l个值；I为指示函数。&lt;/p&gt;
&lt;h3 id=&quot;学习与分类方法&quot;&gt;&lt;a href=&quot;#学习与分类方法&quot; class=&quot;headerlink&quot; title=&quot;学习与分类方法&quot;&gt;&lt;/a&gt;学习与分类方法&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;朴素贝叶斯算法&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;输入：训练数据集$T=\{(x_1,y_1),(x_2,y_2),\cdots ,(x_N,y_N)\}$,其中$x_i=(x_i^{(1)},\cdots , x_i^{(n)})^T,x_i^{(j)}$是第i个样本的第j个特征，$x_i^{(j)} \in \{a_{j1},a_{j2}, \cdots ,a_{jS_j}\},a_{jl}$是第j个特征可能取的第l个值，$j=1,2,\cdots ,n;l=1,2, \cdots ,S_j;y_i \in \{c_1,c_2,\cdots ,c_K\}$;实例x;
输出：实例x的分类&lt;/p&gt;
&lt;p&gt;(1) 计算先验概率及条件概率&lt;/p&gt;
&lt;p&gt;$$P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N},k=1,2,\cdots ,K$$&lt;/p&gt;
&lt;p&gt;$$P(X^{(j)}=a_{jl} | Y= c_k)=\frac{\sum_{i=1}^NY(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)},j=1,2,\cdots,n;l=1,2,\cdots,S_j;k=1,2,\cdots,K$$&lt;/p&gt;
&lt;p&gt;(2) 对于给定的实例$x=(x^{(1)},x^{(2)},\cdots ,x^{(n)})^T$,计算：&lt;/p&gt;
&lt;p&gt;$$P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}s|Y=c_k) , k=1,2, \cdots ,K$$&lt;/p&gt;
&lt;p&gt;(3) 确定实例x的分类&lt;/p&gt;
&lt;p&gt;$$y =  arg \max_{c_k}P(Y=c_k)\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)$$&lt;/p&gt;
&lt;h3 id=&quot;例子&quot;&gt;&lt;a href=&quot;#例子&quot; class=&quot;headerlink&quot; title=&quot;例子&quot;&gt;&lt;/a&gt;例子&lt;/h3&gt;&lt;p&gt;试由下表的训练数据学习一个朴素贝叶斯分类器并确定$x=(2,S)^T$的类标记y，表中$X^{(1)},X^{(2)}$为特征，取值的集合分别为$A_1=\{1,2,3\},A_2=\{S,M,L\}$,Y为类标记，$Y \in C =\{1,-1\}$&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;th&gt;2&lt;/th&gt;
&lt;th&gt;3&lt;/th&gt;
&lt;th&gt;4&lt;/th&gt;
&lt;th&gt;5&lt;/th&gt;
&lt;th&gt;6&lt;/th&gt;
&lt;th&gt;7&lt;/th&gt;
&lt;th&gt;8&lt;/th&gt;
&lt;th&gt;9&lt;/th&gt;
&lt;th&gt;10&lt;/th&gt;
&lt;th&gt;11&lt;/th&gt;
&lt;th&gt;12&lt;/th&gt;
&lt;th&gt;13&lt;/th&gt;
&lt;th&gt;14&lt;/th&gt;
&lt;th&gt;15&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;$X^{(1)}$&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$X^{(2)}$&lt;/td&gt;
&lt;td&gt;S&lt;/td&gt;
&lt;td&gt;M&lt;/td&gt;
&lt;td&gt;M&lt;/td&gt;
&lt;td&gt;S&lt;/td&gt;
&lt;td&gt;S&lt;/td&gt;
&lt;td&gt;S&lt;/td&gt;
&lt;td&gt;M&lt;/td&gt;
&lt;td&gt;M&lt;/td&gt;
&lt;td&gt;L&lt;/td&gt;
&lt;td&gt;L&lt;/td&gt;
&lt;td&gt;L&lt;/td&gt;
&lt;td&gt;M&lt;/td&gt;
&lt;td&gt;M&lt;/td&gt;
&lt;td&gt;L&lt;/td&gt;
&lt;td&gt;L&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;$Y$&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;-1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;根据朴素贝叶斯分类器，容易计算下列概率：&lt;/p&gt;
&lt;p&gt;$P(Y=1)=9/15,p(Y=-1)=6/15$&lt;/p&gt;
&lt;p&gt;$P(X^{(1)}=1|Y=1)=2/9,P(X^{(1)}=2|Y=1)=3/9,P(X^{(1)}=3|Y=1)=4/9$&lt;/p&gt;
&lt;p&gt;$P(X^{(2)}=S|Y=1)=1/9,P(X^{(2)}=M|Y=1)=4/9,P(X^{(2)}=L|Y=1)=4/9$&lt;/p&gt;
&lt;p&gt;$P(X^{(1)}=1|Y=-1)=3/6,P(X^{(1)}=2|Y=-1)=2/6,P(X^{(1)}=3|Y=-1)=1/6$&lt;/p&gt;
&lt;p&gt;$P(X^{(2)}=S|Y=-1)=3/6,P(X^{(2)}=M|Y=-1)=2/6,P(X^{(2)}=L|Y=-1)=1/6$&lt;/p&gt;
&lt;p&gt;于是，对于给定的$x=(2,S)^T$,计算：&lt;/p&gt;
&lt;p&gt;$P(Y=1)P(X^{(1)}=2|Y=1)P(X^{(2)}=S|Y=1)=9/15\cdot 3/9 \cdot 1/9=1/45$&lt;/p&gt;
&lt;p&gt;$P(Y=-1)P(X^{(1)}=2|Y=-1)P(X^{(2)}=S|Y=-1)=6/15\cdot 2/6 \cdot 3/6=1/15$&lt;/p&gt;
&lt;p&gt;故$y=-1$&lt;/p&gt;
&lt;h3 id=&quot;贝叶斯估计&quot;&gt;&lt;a href=&quot;#贝叶斯估计&quot; class=&quot;headerlink&quot; title=&quot;贝叶斯估计&quot;&gt;&lt;/a&gt;贝叶斯估计&lt;/h3&gt;&lt;p&gt;用极大似然估计可能会出现所要估计的概率的值为0的情况，这时会影响到后验概率的计算结果，使分类出现偏差，解决这一问题的方法是采用贝叶斯估计。具体地，条件概率的贝叶斯估计是&lt;/p&gt;
&lt;p&gt;$$P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)},y_i=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}$$&lt;/p&gt;
&lt;p&gt;式中$\lambda \geq 0$，等价于在随机变量各个取值的频数上赋予一个正数λ&amp;gt;0,当λ=0时就是极大似然估计。常取λ=1，这时称为拉普拉斯平滑(Laplace smoothing)。显然，对任何$l=1,2,\cdots ,S_j,k=1,2, \cdots ,K$，有&lt;/p&gt;
&lt;p&gt;$$P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)&amp;gt;0$$&lt;/p&gt;
&lt;p&gt;$$\sum_{l=1}^{S_j}P(X^{(j)}=a_{jl}|Y=c_k)=1$$&lt;/p&gt;
&lt;p&gt;表明上式确为一种概率分布，同样先验概率的贝叶斯估计是&lt;/p&gt;
&lt;p&gt;$$P_{\lambda}(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}$$&lt;/p&gt;
&lt;h3 id=&quot;综合：python实现简单情感极性分析器&quot;&gt;&lt;a href=&quot;#综合：python实现简单情感极性分析器&quot; class=&quot;headerlink&quot; title=&quot;综合：python实现简单情感极性分析器&quot;&gt;&lt;/a&gt;综合：python实现简单情感极性分析器&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;66&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;68&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;69&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;70&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;71&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;72&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;73&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;74&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;75&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;76&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;77&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;78&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;79&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;80&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;81&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;82&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;83&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;84&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;85&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;86&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;87&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;88&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;89&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;90&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;91&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;92&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;93&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;94&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;95&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;96&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;97&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;98&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;99&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;100&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;101&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;102&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;103&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;104&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;105&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;106&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;107&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;108&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# -*- coding:utf-8 -*-&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Filename: Bayes.py&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; math &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; log, exp&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;LaplaceEstimate&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(object)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    拉普拉斯平滑处理的贝叶斯估计&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.d = &amp;#123;&amp;#125;  &lt;span class=&quot;comment&quot;&gt;# [词-词频]的map&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.total = &lt;span class=&quot;number&quot;&gt;0.0&lt;/span&gt;  &lt;span class=&quot;comment&quot;&gt;# 全部词的词频&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.none = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;comment&quot;&gt;# 当一个词不存在的时候，它的词频（等于0+1）&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;exists&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, key)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; key &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; self.d&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;getsum&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; self.total&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, key)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; self.exists(key):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;, self.none&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;, self.d[key]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;getprob&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, key)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        估计先验概率&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        :param key: 词&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        :return: 概率&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; float(self.get(key)[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]) / self.total&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;samples&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;string&quot;&gt;&quot;&quot;&quot;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        获取全部样本&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        :return:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &quot;&quot;&quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; self.d.keys()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, key, value)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.total += value&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; self.exists(key):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.d[key] = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            self.total += &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.d[key] += value&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Bayes&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(object)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.d = &amp;#123;&amp;#125;  &lt;span class=&quot;comment&quot;&gt;# [标签, 概率] map&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.total = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;comment&quot;&gt;# 全部词频&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, data)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; d &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; data:  &lt;span class=&quot;comment&quot;&gt;# d是[[词链表], 标签]&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            c = d[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]  &lt;span class=&quot;comment&quot;&gt;# c是分类&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; c &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; self.d:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                self.d[c] = LaplaceEstimate()  &lt;span class=&quot;comment&quot;&gt;# d[c]是概率统计工具&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; word &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; d[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                self.d[c].add(word, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)  &lt;span class=&quot;comment&quot;&gt;# 统计词频&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.total = sum(map(&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; x: self.d[x].getsum(), self.d.keys()))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;classify&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, x)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        tmp = &amp;#123;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; c &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; self.d:  &lt;span class=&quot;comment&quot;&gt;# 分类&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            tmp[c] = log(self.d[c].getsum()) - log(self.total)  &lt;span class=&quot;comment&quot;&gt;# P(Y=ck)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; word &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; x:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                tmp[c] += log(self.d[c].getprob(word))          &lt;span class=&quot;comment&quot;&gt;# P(Xj=xj | Y=ck)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        ret, prob = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; c &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; self.d:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            now = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;try&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; otherc &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; self.d:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                    now += exp(tmp[otherc] - tmp[c])            &lt;span class=&quot;comment&quot;&gt;# 将对数还原为1/p&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                now = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt; / now&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;except&lt;/span&gt; OverflowError:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                now = &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; now &amp;gt; prob:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                ret, prob = c, now&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; (ret, prob)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Sentiment&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(object)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.classifier = Bayes()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;segment&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, sent)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        words = sent.split(&lt;span class=&quot;string&quot;&gt;&#39; &#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; words&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, neg_docs, pos_docs)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        data = []&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; sent &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; neg_docs:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            data.append([self.segment(sent), &lt;span class=&quot;string&quot;&gt;&#39;消极&#39;&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; sent &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; pos_docs:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            data.append([self.segment(sent), &lt;span class=&quot;string&quot;&gt;&#39;积极&#39;&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.classifier.train(data)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;classify&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, sent)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; self.classifier.classify(self.segment(sent))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;s = Sentiment()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;s.train([&lt;span class=&quot;string&quot;&gt;&#39;糟糕&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;好 差劲&#39;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&#39;坏&#39;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&#39;好 坏&#39;&lt;/span&gt;], [&lt;span class=&quot;string&quot;&gt;&#39;优秀&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;很 好&#39;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&#39;棒&#39;&lt;/span&gt;,&lt;span class=&quot;string&quot;&gt;&#39;好 人&#39;&lt;/span&gt;]) &lt;span class=&quot;comment&quot;&gt;# 空格分词&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(s.classify(&lt;span class=&quot;string&quot;&gt;&quot;好 棒&quot;&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;输出结果:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;(&lt;span class=&quot;string&quot;&gt;&#39;积极&#39;&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0.6451612903225805&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;朴素贝叶斯(native Bates)是基于贝叶斯定理与特征条件独立假设的分类法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率密度；然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率的最大的输出y。朴素贝叶斯实现简单，学习和预测的效率都很高，是一种常用的方法。本节对应于统计学习方法第四章的内容。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="MachineLearning" scheme="http://keyunluo.github.io/categories/MachineLearning/"/>
    
    
      <category term="机器学习" scheme="http://keyunluo.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>统计学习方法——K近邻算法</title>
    <link href="http://keyunluo.github.io/2016/08/15/MachineLearning/statical-learning-3.html"/>
    <id>http://keyunluo.github.io/2016/08/15/MachineLearning/statical-learning-3.html</id>
    <published>2016-08-15T07:25:02.000Z</published>
    <updated>2017-07-06T08:08:06.932Z</updated>
    
    <content type="html">&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;k近邻算法(k-nearest neighbor,k-NN)是一种基本的分类与回归算法，不具有显式的学习过程。k值选择、距离度量以及分类决策规则是k近邻算法的三个基本要素。本节对应于统计学习方法第三章的内容。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;h2 id=&quot;K近邻算法&quot;&gt;&lt;a href=&quot;#K近邻算法&quot; class=&quot;headerlink&quot; title=&quot;K近邻算法&quot;&gt;&lt;/a&gt;K近邻算法&lt;/h2&gt;&lt;h3 id=&quot;算法描述&quot;&gt;&lt;a href=&quot;#算法描述&quot; class=&quot;headerlink&quot; title=&quot;算法描述&quot;&gt;&lt;/a&gt;算法描述&lt;/h3&gt;&lt;p&gt;给定一个训练数据集，对新的输入实例，在训练数据集中找到跟它最近的k个实例，根据这k个实例的类判断它自己的类（一般采用多数表决的方法）。&lt;/p&gt;
&lt;h3 id=&quot;算法流程&quot;&gt;&lt;a href=&quot;#算法流程&quot; class=&quot;headerlink&quot; title=&quot;算法流程&quot;&gt;&lt;/a&gt;算法流程&lt;/h3&gt;&lt;p&gt;输入： 训练数据集 $T=\{(x_1,y_1),(x_2,y_2),\cdots ,(x_N,y_N)\}$, 其中，$x_i \in \mathcal{X} \subseteq R^n$ 为实例的特征向量，$y_i \in \mathcal{Y} = \{c_1,c_2,\cdots ,c_K \}$为实例的类别，$i = 1,2,\cdots ,N;$实例特征向量$x;$&lt;/p&gt;
&lt;p&gt;输出：实例x所属的类别y&lt;/p&gt;
&lt;p&gt;(1) 根据给定的距离度量，在训练集T中找出与x最邻近的k个点，涵盖这k个点的x的领域记为$N_k(x)$;&lt;/p&gt;
&lt;p&gt;(2) 在$N_k(x)$中根据分类决策规则(如多数表决)决定x的类别y:&lt;/p&gt;
&lt;p&gt;$$y = arg max_{c_j} \sum_{x_i \in N_k(x)} I(y_i = c_j),i=1,2, \cdots ,N;j = 1,2, \cdots ,K$$&lt;/p&gt;
&lt;p&gt;式中I为指示函数，即当$y_i=c_i$时I为1，否则I为0.&lt;/p&gt;
&lt;p&gt;k近邻算法的特殊情况是k=1的情形，称为最近邻算法。对于输入的实例点x，最近邻法将训练数据集中与x最邻近的类作为x的类。&lt;/p&gt;
&lt;h2 id=&quot;K近邻模型&quot;&gt;&lt;a href=&quot;#K近邻模型&quot; class=&quot;headerlink&quot; title=&quot;K近邻模型&quot;&gt;&lt;/a&gt;K近邻模型&lt;/h2&gt;&lt;p&gt;模型有3个要素——距离度量方法、k值的选择和分类决策规则。&lt;/p&gt;
&lt;h3 id=&quot;模型&quot;&gt;&lt;a href=&quot;#模型&quot; class=&quot;headerlink&quot; title=&quot;模型&quot;&gt;&lt;/a&gt;模型&lt;/h3&gt;&lt;p&gt;当3要素确定的时候，对任何实例（训练或输入），它所属的类都是确定的，相当于将特征空间分为一些子空间。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-08/k近邻模型.jpg&quot; alt=&quot;k近邻模型&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;距离度量&quot;&gt;&lt;a href=&quot;#距离度量&quot; class=&quot;headerlink&quot; title=&quot;距离度量&quot;&gt;&lt;/a&gt;距离度量&lt;/h3&gt;&lt;p&gt;对n维实数向量空间$R^n$，经常用$L_p$距离或曼哈顿$Minkowski$距离。&lt;/p&gt;
&lt;p&gt;设特征空间$\mathcal{X}$是n维实数向量空间$R^n,x_i,x_j \in \mathcal{X}, x_i = (x_i^{(1)},x_i^{(2)},\cdots , x_i^{(n)})^T,x_j = (x_j^{(1)},x_j^{(2)},\cdots , x_j^{(n)})^T,x_i,x_j的L_p距离定义为：$&lt;/p&gt;
&lt;p&gt;$$L_p(x_i,x_j)=\left( \sum_{l=1}^{n} \vert x_i^{(l)}-x_j^{(l)} \vert ^p \right)^{\frac{1}{p}}$$&lt;/p&gt;
&lt;p&gt;这里$p \geq 1$,当p=2时，称为欧氏距离(Euclidean distance)，即：&lt;/p&gt;
&lt;p&gt;$$L_2(x_i,x_j)=\left( \sum_{l=1}^{n} \vert x_i^{(l)}-x_j^{(l)} \vert ^2 \right ) ^{\frac{1}{2}}$$&lt;/p&gt;
&lt;p&gt;当p=1时，称为曼哈顿距离(Manhattan distance),即：&lt;/p&gt;
&lt;p&gt;$$L_1(x_i,x_j)=\sum_{l=1}^{n} \vert x_i^{(l)}-x_j^{(l)} \vert $$&lt;/p&gt;
&lt;p&gt;当$p=\infty$时，它是各个坐标距离的最大值,即：&lt;/p&gt;
&lt;p&gt;$$L_{\infty}(x_i,x_j)=\max \vert x_i^{(l)}-x_j^{(l)} \vert $$&lt;/p&gt;
&lt;h3 id=&quot;k值的选择&quot;&gt;&lt;a href=&quot;#k值的选择&quot; class=&quot;headerlink&quot; title=&quot;k值的选择&quot;&gt;&lt;/a&gt;k值的选择&lt;/h3&gt;&lt;p&gt;k较小，容易被噪声影响，发生过拟合。&lt;/p&gt;
&lt;p&gt;k较大，较远的训练实例也会对预测起作用，容易发生错误。&lt;/p&gt;
&lt;p&gt;在应用中，k值一般取一个比较小的数值，通常采用交叉验证法来选取最优的k值。&lt;/p&gt;
&lt;h3 id=&quot;分类决策规则&quot;&gt;&lt;a href=&quot;#分类决策规则&quot; class=&quot;headerlink&quot; title=&quot;分类决策规则&quot;&gt;&lt;/a&gt;分类决策规则&lt;/h3&gt;&lt;p&gt;使用0-1损失函数衡量，那么误分类率是：&lt;/p&gt;
&lt;p&gt;$$\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \not= c_j) = 1- \frac{1}{k}\sum_{x_i \in N_k(x)} I(y_i = c_j)$$&lt;/p&gt;
&lt;p&gt;$N_k$是近邻集合，要使左边最小，右边$\sum_{x_i \in N_k(x)} I(y_i = c_j)$必须最大，所以多数表决等价于经验风险最小化。&lt;/p&gt;
&lt;h2 id=&quot;kd树&quot;&gt;&lt;a href=&quot;#kd树&quot; class=&quot;headerlink&quot; title=&quot;kd树&quot;&gt;&lt;/a&gt;kd树&lt;/h2&gt;&lt;p&gt;算法核心在于怎么快速搜索k个近邻出来，朴素做法是线性扫描，不可取，这里介绍的方法是kd树。&lt;/p&gt;
&lt;h3 id=&quot;构造kd树&quot;&gt;&lt;a href=&quot;#构造kd树&quot; class=&quot;headerlink&quot; title=&quot;构造kd树&quot;&gt;&lt;/a&gt;构造kd树&lt;/h3&gt;&lt;p&gt;对数据集T中的子集S初始化S=T，取当前节点node=root取维数的序数i=0，对S递归执行：&lt;/p&gt;
&lt;p&gt;找出S的第i维的中位数对应的点，通过该点，且垂直于第i维坐标轴做一个超平面。该点加入node的子节点。该超平面将空间分为两个部分，对这两个部分分别重复此操作（S=S’，++i，node=current），直到不可再分。&lt;/p&gt;
&lt;h4 id=&quot;算法流程-1&quot;&gt;&lt;a href=&quot;#算法流程-1&quot; class=&quot;headerlink&quot; title=&quot;算法流程&quot;&gt;&lt;/a&gt;算法流程&lt;/h4&gt;&lt;p&gt;输入：k维空间数据集 $T=\{x_1,x_2,…,x_N\}$,其中$x_i=(x_i^{(1)},x_i^{(2)},\cdots ,x_i^{(k)})^T,i=1,2,\cdots ,N$&lt;/p&gt;
&lt;p&gt;输出： kd树&lt;/p&gt;
&lt;p&gt;(1) 开始：构造根节点，根节点对应于包含T的k维空间的超矩形区域：选择$x^{(1)}$为坐标轴，以T中所有实例的$x^{(1)}$坐标的中位数为切分点，将根节点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。&lt;/p&gt;
&lt;p&gt;由根节点生成深度为1的左右子节点：左子节点对应坐标$x^{(1)}$小于切分点的子区域，右子节点对应坐标$x^{(1)}$大于切分点的子区域。&lt;/p&gt;
&lt;p&gt;将落在切分超平面的实例点保存在根节点。&lt;/p&gt;
&lt;p&gt;(2) 重复：对深度为j的节点，选择$x^{(l)}$为切分的坐标轴，$l=j(mod k) +1$,以该节点的区域中所有实例的$x^{(l)}$坐标的中位数为切分点，将该节点对应的超矩形区域切分为两个子区域，切分由通过切分点并与坐标轴$x^{(l)}$垂直的超平面实现。&lt;/p&gt;
&lt;p&gt;由该节点生成深度为j+1的左右子节点：左子节点对应坐标$x^{(l)}$小于切分点的子区域，右子节点对应坐标$x^{(l)}$大于切分点的子区域。&lt;/p&gt;
&lt;p&gt;将落在切分超平面的实例点保存在根节点。&lt;/p&gt;
&lt;p&gt;(3) 直到两个子区域没有实例存在时停止，从而形成kd树的区域划分。&lt;/p&gt;
&lt;h4 id=&quot;例子&quot;&gt;&lt;a href=&quot;#例子&quot; class=&quot;headerlink&quot; title=&quot;例子&quot;&gt;&lt;/a&gt;例子&lt;/h4&gt;&lt;p&gt;给定一个二维空间的数据集：$T=\{(2,3)^T,(5,4)^T,(9,6)^T,(4,7)^T,(8,1)^T,(7,2)^T\}$,
构造一棵平衡kd树。&lt;/p&gt;
&lt;h4 id=&quot;python实现&quot;&gt;&lt;a href=&quot;#python实现&quot; class=&quot;headerlink&quot; title=&quot;python实现&quot;&gt;&lt;/a&gt;python实现&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;T = [[&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;]]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;node&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, point)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.left = &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.right = &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.point = point&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;median&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(lst)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    m = int(len(lst) / &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; lst[m], m&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;build_kdtree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(data, d)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    data = sorted(data, key=&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; x: x[d])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    p, m = median(data)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    tree = node(p)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;del&lt;/span&gt; data[m]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    print(data, p)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; m &amp;gt; &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;: tree.left = build_kdtree(data[:m], &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; d)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; len(data) &amp;gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;: tree.right = build_kdtree(data[m:], &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; d)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; tree&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;kd_tree = build_kdtree(T, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(kd_tree)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;可视化&quot;&gt;&lt;a href=&quot;#可视化&quot; class=&quot;headerlink&quot; title=&quot;可视化&quot;&gt;&lt;/a&gt;可视化&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;66&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;68&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;69&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;70&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;71&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;72&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;73&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;74&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;75&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;76&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;77&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;78&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;79&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;80&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;81&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;82&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;83&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;84&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;85&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;86&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;87&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;88&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;89&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;90&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;91&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;92&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;93&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;94&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;95&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;96&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;97&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;98&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;99&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;100&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;101&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;102&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;103&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;104&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;105&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# -*- coding:utf-8 -*-&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Filename: kdtree.py&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; copy&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; itertools&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; matplotlib &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; pyplot &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; plt&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; matplotlib.patches &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; Rectangle&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; matplotlib &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; animation&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;T = [[&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;]]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;draw_point&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(data)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    X, Y = [], []&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; p &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; data:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        X.append(p[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        Y.append(p[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.plot(X, Y, &lt;span class=&quot;string&quot;&gt;&#39;bo&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;draw_line&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(xy_list)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; xy &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; xy_list:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x, y = xy&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        plt.plot(x, y, &lt;span class=&quot;string&quot;&gt;&#39;g&#39;&lt;/span&gt;, lw=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;draw_square&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(square_list)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    currentAxis = plt.gca()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    colors = itertools.cycle([&lt;span class=&quot;string&quot;&gt;&quot;r&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;b&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;g&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;c&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;m&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;y&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;#EB70AA&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;#0099FF&#39;&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; square &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; square_list:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        currentAxis.add_patch(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            Rectangle((square[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;], square[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]), square[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] - square[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;], square[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] - square[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                      color=next(colors)))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;median&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(lst)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    m = int(len(lst) / &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; lst[m], m&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;history_quare = []&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;build_kdtree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(data, d, square)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    history_quare.append(square)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    data = sorted(data, key=&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; x: x[d])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    p, m = median(data)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;del&lt;/span&gt; data[m]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    print(data, p)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; m &amp;gt;= &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        sub_square = copy.deepcopy(square)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; d == &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            sub_square[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] = p[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            sub_square[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] = p[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        history_quare.append(sub_square)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; m &amp;gt; &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;: build_kdtree(data[:m], &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; d, sub_square)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; len(data) &amp;gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        sub_square = copy.deepcopy(square)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; d == &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            sub_square[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] = p[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            sub_square[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] = p[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        build_kdtree(data[m:], &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; d, sub_square)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;build_kdtree(T, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, [[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;]])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(history_quare)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# draw an animation to show how it works, the data comes from history&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# first set up the figure, the axis, and the plot element we want to animate&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;fig = plt.figure()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ax = plt.axes(xlim=(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;), ylim=(&lt;span class=&quot;number&quot;&gt;-2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;line, = ax.plot([], [], &lt;span class=&quot;string&quot;&gt;&#39;g&#39;&lt;/span&gt;, lw=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;label = ax.text([], [], &lt;span class=&quot;string&quot;&gt;&#39;&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# initialization function: plot the background of each frame&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.axis([&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.grid(&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.xlabel(&lt;span class=&quot;string&quot;&gt;&#39;x_1&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.ylabel(&lt;span class=&quot;string&quot;&gt;&#39;x_2&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    plt.title(&lt;span class=&quot;string&quot;&gt;&#39;构造KD树&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    draw_point(T)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;currentAxis = plt.gca()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;colors = itertools.cycle([&lt;span class=&quot;string&quot;&gt;&quot;#FF6633&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;g&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;#3366FF&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;c&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;m&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&quot;y&quot;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;#EB70AA&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;#0099FF&#39;&lt;/span&gt;, &lt;span class=&quot;string&quot;&gt;&#39;#66FFFF&#39;&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# animation function.  this is called sequentially&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;animate&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(i)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    square = history_quare[i]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    currentAxis.add_patch(&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        Rectangle((square[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;], square[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]), square[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] - square[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;], square[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] - square[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;],&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                  color=next(colors)))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# call the animator.  blit=true means only re-draw the parts that have changed.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;anim = animation.FuncAnimation(fig, animate, init_func=init, frames=len(history_quare), interval=&lt;span class=&quot;number&quot;&gt;1000&lt;/span&gt;, repeat=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                               blit=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;plt.show()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;anim.save(&lt;span class=&quot;string&quot;&gt;&#39;kdtree_build.gif&#39;&lt;/span&gt;, fps=&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, writer=&lt;span class=&quot;string&quot;&gt;&#39;imagemagick&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;img src=&quot;/resource/blog/2016-08/kdtree_build.gif&quot; alt=&quot;构建kd树&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;搜索kd树&quot;&gt;&lt;a href=&quot;#搜索kd树&quot; class=&quot;headerlink&quot; title=&quot;搜索kd树&quot;&gt;&lt;/a&gt;搜索kd树&lt;/h3&gt;&lt;h4 id=&quot;算法流程-2&quot;&gt;&lt;a href=&quot;#算法流程-2&quot; class=&quot;headerlink&quot; title=&quot;算法流程&quot;&gt;&lt;/a&gt;算法流程&lt;/h4&gt;&lt;p&gt;搜索跟二叉树一样，是一个递归的过程。先找到目标点的插入位置，然后往上走，逐步用自己到目标点的距离画个超球体，用超球体圈住的点来更新最近邻（或k最近邻）。&lt;/p&gt;
&lt;p&gt;输入：已构造的kd树，目标点x&lt;/p&gt;
&lt;p&gt;输出：x的最近邻&lt;/p&gt;
&lt;p&gt;(1) 在kd树中找到包含目标点x的叶节点：从根节点出发，递归的向下访问kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到左子节点，否则移动到右子节点。直到子节点为叶节点为止。&lt;/p&gt;
&lt;p&gt;(2) 以此叶节点为“当前最近点”&lt;/p&gt;
&lt;p&gt;(3) 递归的向上回退，在每个节点进行如下操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果该节点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”&lt;/li&gt;
&lt;li&gt;当前最近点一定存在于该节点一个子节点对应的区域，检查该子节点的父节点的另一子节点对应的区域是否有更近的点。具体的，检查另一子节点对应的区域是否以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。如果相交，可能在另一个子节点对应的区域内存在距离目标点更近的点，移动到另一个子节点，接着，递归地进行最近邻搜索。如果不相交，向上回退&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(4) 当回退到根节点时，搜索结束。最后的“当前最近点”即为x的最近邻点。&lt;/p&gt;
&lt;h4 id=&quot;python实现-1&quot;&gt;&lt;a href=&quot;#python实现-1&quot; class=&quot;headerlink&quot; title=&quot;python实现&quot;&gt;&lt;/a&gt;python实现&lt;/h4&gt;&lt;p&gt;以最近邻为例，实现如下（本实现由于测试数据简单，没有做超球体与超立体相交的逻辑）：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;66&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;68&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;69&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;70&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;71&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;72&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;73&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;74&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# -*- coding:utf-8 -*-&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# Filename: search_kdtree.py&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;T = [[&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;7&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;]]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;node&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, point)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.left = &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.right = &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.point = point&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.parent = &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;set_left&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, left)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; left == &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;: &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        left.parent = self&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.left = left&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;set_right&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, right)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; right == &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;: &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        right.parent = self&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.right = right&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;median&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(lst)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    m = int(len(lst) / &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; lst[m], m&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;build_kdtree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(data, d)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    data = sorted(data, key=&lt;span class=&quot;keyword&quot;&gt;lambda&lt;/span&gt; x: x[d])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    p, m = median(data)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    tree = node(p)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;del&lt;/span&gt; data[m]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; m &amp;gt; &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;: tree.set_left(build_kdtree(data[:m], &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; d))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; len(data) &amp;gt; &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;: tree.set_right(build_kdtree(data[m:], &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; d))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; tree&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;distance&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(a, b)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    print(a, b)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; ((a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] - b[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]) ** &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt; + (a[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] - b[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]) ** &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;) ** &lt;span class=&quot;number&quot;&gt;0.5&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;search_kdtree&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(tree, d, target)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; target[d] &amp;lt; tree.point[d]:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; tree.left != &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; search_kdtree(tree.left, &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; d, target)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; tree.right != &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; search_kdtree(tree.right, &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; d, target)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;update_best&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(t, best)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; t == &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;: &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        t = t.point&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        d = distance(t, target)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; d &amp;lt; best[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;]:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            best[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] = d&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;            best[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] = t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    best = [tree.point, &lt;span class=&quot;number&quot;&gt;100000.0&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;while&lt;/span&gt; (tree.parent != &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        update_best(tree.parent.left, best)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        update_best(tree.parent.right, best)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        tree = tree.parent&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; best[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;kd_tree = build_kdtree(T, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(search_kdtree(kd_tree, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, [&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;]))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;输出
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;number&quot;&gt;8&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] [&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;] [&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;] [&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;[&lt;span class=&quot;number&quot;&gt;9&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;]&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
</content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;本节内容：&lt;/strong&gt;k近邻算法(k-nearest neighbor,k-NN)是一种基本的分类与回归算法，不具有显式的学习过程。k值选择、距离度量以及分类决策规则是k近邻算法的三个基本要素。本节对应于统计学习方法第三章的内容。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="MachineLearning" scheme="http://keyunluo.github.io/categories/MachineLearning/"/>
    
    
      <category term="机器学习" scheme="http://keyunluo.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
